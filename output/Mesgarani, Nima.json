[{"PMID": "38798551", "Title": "Joint population coding and temporal coherence link an attended talker's voice and location features in naturalistic multi-talker scenes.", "Abstract": "Listeners readily extract multi-dimensional auditory objects such as a 'localized talker' from complex acoustic scenes with multiple talkers. Yet, the neural mechanisms underlying simultaneous encoding and linking of different sound features - for example, a talker's voice and location - are poorly understood. We analyzed invasive intracranial recordings in neurosurgical patients attending to a localized talker in real-life cocktail party scenarios. We found that sensitivity to an individual talker's voice and location features was distributed throughout auditory cortex and that neural sites exhibited a gradient from sensitivity to a single feature to joint sensitivity to both features. On a population level, cortical response patterns of both dual-feature sensitive sites but also single-feature sensitive sites revealed simultaneous encoding of an attended talker's voice and location features. However, for single-feature sensitive sites, the representation of the primary feature was more precise. Further, sites which selective tracked an attended speech stream concurrently encoded an attended talker's voice and location features, indicating that such sites combine selective tracking of an attended auditory object with encoding of the object's features. Finally, we found that attending a localized talker selectively enhanced temporal coherence between single-feature voice sensitive sites and single-feature location sensitive sites, providing an additional mechanism for linking voice and location in multi-talker scenes. These results demonstrate that a talker's voice and location features are linked during multi-dimensional object formation in naturalistic multi-talker scenes by joint population coding as well as by temporal coherence between neural sites.", "Keywords": [], "MeSH terms": [], "Authors": [{"First Name": "Kiki", "Last Name": "van der Heijden", "Affiliation": "N/A"}, {"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "N/A"}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "N/A"}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "N/A"}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "N/A"}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}], "Journal": "bioRxiv : the preprint server for biology", "PubDate": "2024May14"}, {"PMID": "38093008", "Title": "Large-scale single-neuron speech sound encoding across the depth of human cortex.", "Abstract": "Understanding the neural basis of speech perception requires that we study the human brain both at the scale of the fundamental computational unit of neurons and in\u00a0their organization across the depth of cortex. Here we used high-density Neuropixels arrays1-3 to record from 685\u2009neurons across cortical layers at nine sites in a high-level auditory region that is critical for speech, the superior temporal gyrus4,5, while participants listened to spoken sentences. Single neurons encoded a wide range of speech sound cues, including features of consonants and vowels, relative vocal pitch, onsets, amplitude envelope and sequence statistics. Neurons at each cross-laminar recording exhibited dominant tuning to a primary speech feature while also containing a substantial proportion of neurons that encoded other features contributing to heterogeneous selectivity. Spatially, neurons at similar cortical depths tended to encode similar speech features. Activity across all cortical layers was predictive of high-frequency field potentials (electrocorticography), providing a neuronal origin for macroelectrode recordings from the cortical surface. Together, these results establish single-neuron tuning across the cortical laminae as an important dimension of speech encoding in human superior temporal gyrus.", "Keywords": [], "MeSH terms": ["Humans", "Acoustic Stimulation", "Auditory Cortex", "Neurons", "Phonetics", "Speech", "Speech Perception", "Temporal Lobe", "Cues", "Electrodes"], "Authors": [{"First Name": "Matthew K", "Last Name": "Leonard", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, USA."}, {"First Name": "Laura", "Last Name": "Gwilliams", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, USA."}, {"First Name": "Kristin K", "Last Name": "Sellers", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, USA."}, {"First Name": "Jason E", "Last Name": "Chung", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, USA."}, {"First Name": "Duo", "Last Name": "Xu", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, USA."}, {"First Name": "Gavin", "Last Name": "Mischler", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA."}, {"First Name": "Marleen", "Last Name": "Welkenhuysen", "Affiliation": "IMEC, Leuven, Belgium."}, {"First Name": "Barundeb", "Last Name": "Dutta", "Affiliation": "IMEC, Leuven, Belgium."}, {"First Name": "Edward F", "Last Name": "Chang", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, San Francisco, CA, USA. edward.chang@ucsf.edu."}], "Journal": "Nature", "PubDate": "2024Feb"}, {"PMID": "38083559", "Title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation.", "Abstract": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.", "Keywords": [], "MeSH terms": ["Humans", "Speech Perception", "Speech", "Acoustic Stimulation", "Auditory Cortex", "Attention"], "Authors": [{"First Name": "Cong", "Last Name": "Han", "Affiliation": "N/A"}, {"First Name": "Vishal", "Last Name": "Choudhari", "Affiliation": "N/A"}, {"First Name": "Yinghao Aaron", "Last Name": "Li", "Affiliation": "N/A"}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}], "Journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "PubDate": "2023Jul"}, {"PMID": "37771949", "Title": "naplib-python: Neural acoustic data processing and analysis tools in python.", "Abstract": "Recently, the computational neuroscience community has pushed for more transparent and reproducible methods across the field. In the interest of unifying the domain of auditory neuroscience, naplib-python provides an intuitive and general data structure for handling all neural recordings and stimuli, as well as extensive preprocessing, feature extraction, and analysis tools which operate on that data structure. The package removes many of the complications associated with this domain, such as varying trial durations and multi-modal stimuli, and provides a general-purpose analysis framework that interfaces easily with existing toolboxes used in the field.", "Keywords": ["Auditory neuroscience", "EcoG", "Preprocessing", "Python", "iEEG"], "MeSH terms": [], "Authors": [{"First Name": "Gavin", "Last Name": "Mischler", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, NY, United States."}, {"First Name": "Vinay", "Last Name": "Raghavan", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, NY, United States."}, {"First Name": "Menoua", "Last Name": "Keshishian", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, NY, United States."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, NY, United States."}], "Journal": "Software impacts", "PubDate": "2023Sep"}, {"PMID": "37577180", "Title": "ONLINE BINAURAL SPEECH SEPARATION OF MOVING SPEAKERS WITH A WAVESPLIT NETWORK.", "Abstract": "Binaural speech separation in real-world scenarios often involves moving speakers. Most current speech separation methods use utterance-level permutation invariant training (u-PIT) for training. In inference time, however, the order of outputs can be inconsistent over time particularly in long-form speech separation. This situation which is referred to as the speaker swap problem is even more problematic when speakers constantly move in space and therefore poses a challenge for consistent placement of speakers in output channels. Here, we describe a real-time binaural speech separation model based on a Wavesplit network to mitigate the speaker swap problem for moving speaker separation. Our model computes a speaker embedding for each speaker at each time frame from the mixed audio, aggregates embeddings using online clustering, and uses cluster centroids as speaker profiles to track each speaker throughout the long duration. Experimental results on reverberant, long-form moving multitalker speech separation show that the proposed method is less prone to speaker swap and achieves comparable performance with u-PIT based models with ground truth tracking in both separation accuracy and preserving the interaural cues.", "Keywords": ["Binaural speech separation", "moving speakers", "real-time", "speaker representation"], "MeSH terms": [], "Authors": [{"First Name": "Cong", "Last Name": "Han", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}], "Journal": "Proceedings of the ... IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP (Conference)", "PubDate": "2023Jun"}, {"PMID": "37577179", "Title": "PHONEME-LEVEL BERT FOR ENHANCED PROSODY OF TEXT-TO-SPEECH WITH GRAPHEME PREDICTIONS.", "Abstract": "Large-scale pre-trained language models have been shown to be helpful in improving the naturalness of text-to-speech (TTS) models by enabling them to produce more naturalistic prosodic patterns. However, these models are usually word-level or sup-phoneme-level and jointly trained with phonemes, making them inefficient for the downstream TTS task where only phonemes are needed. In this work, we propose a phoneme-level BERT (PL-BERT) with a pretext task of predicting the corresponding graphemes along with the regular masked phoneme predictions. Subjective evaluations show that our phoneme-level BERT encoder has significantly improved the mean opinion scores (MOS) of rated naturalness of synthesized speech compared with the state-of-the-art (SOTA) StyleTTS baseline on out-of-distribution (OOD) texts.", "Keywords": ["BERT", "Pre-training", "Text-to-Speech", "Transfer learning"], "MeSH terms": [], "Authors": [{"First Name": "Yinghao", "Last Name": "Aaron Li", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}, {"First Name": "Cong", "Last Name": "Han", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}, {"First Name": "Xilin", "Last Name": "Jiang", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}], "Journal": "Proceedings of the ... IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP (Conference)", "PubDate": "2023Jun"}, {"PMID": "37577031", "Title": "STYLETTS-VC: ONE-SHOT VOICE CONVERSION BY KNOWLEDGE TRANSFER FROM STYLE-BASED TTS MODELS.", "Abstract": "One-shot voice conversion (VC) aims to convert speech from any source speaker to an arbitrary target speaker with only a few seconds of reference speech from the target speaker. This relies heavily on disentangling the speaker's identity and speech content, a task that still remains challenging. Here, we propose a novel approach to learning disentangled speech representation by transfer learning from style-based text-to-speech (TTS) models. With cycle consistent and adversarial training, the style-based TTS models can perform transcription-guided one-shot VC with high fidelity and similarity. By learning an additional mel-spectrogram encoder through a teacher-student knowledge transfer and novel data augmentation scheme, our approach results in disentangled speech representation without needing the input text. The subjective evaluation shows that our approach can significantly outperform the previous state-of-the-art one-shot voice conversion models in both naturalness and similarity.", "Keywords": ["Voice conversion", "disentangled representations", "text-to-speech", "transfer learning"], "MeSH terms": [], "Authors": [{"First Name": "Yinghao Aaron", "Last Name": "Li", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}, {"First Name": "Cong", "Last Name": "Han", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, USA."}], "Journal": "SLT ... : ... IEEE Workshop on Spoken Language Technology : proceedings. IEEE Workshop on Spoken Language Technology", "PubDate": "2023Jan"}, {"PMID": "37355730", "Title": "Automatic speaker diarization for natural conversation analysis in autism clinical trials.", "Abstract": "Challenges in social communication is one of the core symptom domains in autism spectrum disorder (ASD). Novel therapies are under development to help individuals with these challenges, however the ability to show a benefit is dependent on a sensitive and reliable measure of treatment effect. Currently, measuring these deficits requires the use of time-consuming and subjective techniques. Objective measures extracted from natural conversations could be more ecologically relevant, and administered more frequently-perhaps giving them added sensitivity to change. While several studies have used automated analysis methods to study autistic speech, they require manual transcriptions. In order to bypass this time-consuming process, an automated speaker diarization algorithm must first be applied. In this paper, we are testing whether a speaker diarization algorithm can be applied to natural conversations between autistic individuals and their conversational partner in a natural setting at home over the course of a clinical trial. We calculated the average duration that a participant would speak for within their turn. We found a significant correlation between this feature and the Vineland Adaptive Behaviour Scales (VABS) expressive communication score (r\u2009=\u20090.51, p\u2009=\u20097\u2009\u00d7\u200910-5). Our results show that natural conversations can be used to obtain measures of talkativeness, and that this measure can be derived automatically, thus showing the promise of objectively evaluating communication challenges in ASD.", "Keywords": [], "MeSH terms": ["Humans", "Autistic Disorder", "Autism Spectrum Disorder", "Communication", "Speech"], "Authors": [{"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland."}, {"First Name": "Guy", "Last Name": "Bogaarts", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland."}, {"First Name": "Philipp", "Last Name": "Schoenenberger", "Affiliation": "Neuroscience Early Development, Pharma Research & Early Development, Roche Innovation Center Basel, Basel, Switzerland."}, {"First Name": "Julian", "Last Name": "Tillmann", "Affiliation": "Neuroscience Early Development, Pharma Research & Early Development, Roche Innovation Center Basel, Basel, Switzerland."}, {"First Name": "David", "Last Name": "Slater", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, 10027, USA."}, {"First Name": "Eckhart", "Last Name": "Eule", "Affiliation": "Neuroscience Early Development, Pharma Research & Early Development, Roche Innovation Center Basel, Basel, Switzerland."}, {"First Name": "Timothy", "Last Name": "Kilchenmann", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland."}, {"First Name": "Lorraine", "Last Name": "Murtagh", "Affiliation": "Neuroscience & Rare Diseases, Pharma Research & Early Development, Roche Innovation Center Basel, Basel, Switzerland."}, {"First Name": "Joerg", "Last Name": "Hipp", "Affiliation": "Neuroscience & Rare Diseases, Pharma Research & Early Development, Roche Innovation Center Basel, Basel, Switzerland."}, {"First Name": "Michael", "Last Name": "Lindemann", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland."}, {"First Name": "Florian", "Last Name": "Lipsmeier", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland. florian.lipsmeier@roche.com."}, {"First Name": "Wei-Yi", "Last Name": "Cheng", "Affiliation": "Roche Innovation Center New York, Roche TCRC Inc., New York, USA."}, {"First Name": "David", "Last Name": "Nobbs", "Affiliation": "Roche Innovation Center BaselF. Hoffmann-La Roche Ltd., Basel, Switzerland."}, {"First Name": "Christopher", "Last Name": "Chatham", "Affiliation": "Biomarkers & Translational Technology, Neuroscience & Rare Diseases, Pharma Research & Early Development, Roche Innovation Center New York, New York, USA."}], "Journal": "Scientific reports", "PubDate": "2023Jun24"}, {"PMID": "37279203", "Title": "Distinct neural encoding of glimpsed and masked speech in multitalker situations.", "Abstract": "Humans can easily tune in to one talker in a multitalker environment while still picking up bits of background speech; however, it remains unclear how we perceive speech that is masked and to what degree non-target speech is processed. Some models suggest that perception can be achieved through glimpses, which are spectrotemporal regions where a talker has more energy than the background. Other models, however, require the recovery of the masked regions. To clarify this issue, we directly recorded from primary and non-primary auditory cortex (AC) in neurosurgical patients as they attended to one talker in multitalker speech and trained temporal response function models to predict high-gamma neural activity from glimpsed and masked stimulus features. We found that glimpsed speech is encoded at the level of phonetic features for target and non-target talkers, with enhanced encoding of target speech in non-primary AC. In contrast, encoding of masked phonetic features was found only for the target, with a greater response latency and distinct anatomical organization compared to glimpsed phonetic features. These findings suggest separate mechanisms for encoding glimpsed and masked speech and provide neural evidence for the glimpsing model of speech perception.", "Keywords": [], "MeSH terms": ["Humans", "Speech", "Acoustic Stimulation", "Phonetics", "Speech Perception", "Reaction Time"], "Authors": [{"First Name": "Vinay S", "Last Name": "Raghavan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York, United States of America."}, {"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York, United States of America."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "The Feinstein Institutes for Medical Research, Northwell Health, Manhasset, New York, United States of America."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "The Feinstein Institutes for Medical Research, Northwell Health, Manhasset, New York, United States of America."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York, United States of America."}], "Journal": "PLoS biology", "PubDate": "2023Jun"}, {"PMID": "37064534", "Title": "naplib-python: Neural Acoustic Data Processing and Analysis Tools in Python.", "Abstract": "Recently, the computational neuroscience community has pushed for more transparent and reproducible methods across the field. In the interest of unifying the domain of auditory neuroscience, naplib-python provides an intuitive and general data structure for handling all neural recordings and stimuli, as well as extensive preprocessing, feature extraction, and analysis tools which operate on that data structure. The package removes many of the complications associated with this domain, such as varying trial durations and multi-modal stimuli, and provides a general-purpose analysis framework that interfaces easily with existing toolboxes used in the field.", "Keywords": [], "MeSH terms": [], "Authors": [{"First Name": "Gavin", "Last Name": "Mischler", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States."}, {"First Name": "Vinay", "Last Name": "Raghavan", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States."}, {"First Name": "Menoua", "Last Name": "Keshishian", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States."}], "Journal": "ArXiv", "PubDate": "2023Apr04"}, {"PMID": "36864134", "Title": "Joint, distributed and hierarchically organized encoding of linguistic features in the human auditory cortex.", "Abstract": "The precise role of the human auditory cortex in representing speech sounds and transforming them to meaning is not yet fully understood. Here we used intracranial recordings from the auditory cortex of neurosurgical patients as they listened to natural speech. We found an explicit, temporally ordered and anatomically distributed neural encoding of multiple linguistic features, including phonetic, prelexical phonotactics, word frequency, and lexical-phonological and lexical-semantic information. Grouping neural sites on the basis of their encoded linguistic features revealed a hierarchical pattern, with distinct representations of prelexical and postlexical features distributed across various auditory areas. While sites with longer response latencies and greater distance from the primary auditory cortex encoded higher-level linguistic features, the encoding of lower-level features was preserved and not discarded. Our study reveals a cumulative mapping of sound to meaning and provides empirical evidence for validating neurolinguistic and psycholinguistic models of spoken word recognition that preserve the acoustic variations in speech.", "Keywords": [], "MeSH terms": ["Humans", "Auditory Cortex", "Speech Perception", "Auditory Perception", "Speech", "Phonetics"], "Authors": [{"First Name": "Menoua", "Last Name": "Keshishian", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "Serdar", "Last Name": "Akkol", "Affiliation": "Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, USA."}, {"First Name": "Jose", "Last Name": "Herrero", "Affiliation": "Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, USA."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Feinstein Institutes for Medical Research, Northwell Health, Manhasset, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA. nima@ee.columbia.edu."}], "Journal": "Nature human behaviour", "PubDate": "2023May"}, {"PMID": "36529203", "Title": "Deep neural networks effectively model neural adaptation to changing background noise and suggest nonlinear noise filtering methods in auditory cortex.", "Abstract": "The human auditory system displays a robust capacity to adapt to sudden changes in background noise, allowing for continuous speech comprehension despite changes in background environments. However, despite comprehensive studies characterizing this ability, the computations that underly this process are not well understood. The first step towards understanding a complex system is to propose a suitable model, but the classical and easily interpreted model for the auditory system, the spectro-temporal receptive field (STRF), cannot match the nonlinear neural dynamics involved in noise adaptation. Here, we utilize a deep neural network (DNN) to model neural adaptation to noise, illustrating its effectiveness at reproducing the complex dynamics at the levels of both individual electrodes and the cortical population. By closely inspecting the model's STRF-like computations over time, we find that the model alters both the gain and shape of its receptive field when adapting to a sudden noise change. We show that the DNN model's gain changes allow it to perform adaptive gain control, while the spectro-temporal change creates noise filtering by altering the inhibitory region of the model's receptive field. Further, we find that models of electrodes in nonprimary auditory cortex also exhibit noise filtering changes in their excitatory regions, suggesting differences in noise filtering mechanisms along the cortical hierarchy. These findings demonstrate the capability of deep neural networks to model complex neural adaptation and offer new hypotheses about the computations the auditory cortex performs to enable noise-robust speech perception in real-world, dynamic environments.", "Keywords": ["Adaptation", "Auditory neuroscience", "Deep neural networks", "Modeling"], "MeSH terms": ["Humans", "Auditory Cortex", "Acoustic Stimulation", "Auditory Perception", "Neurons", "Neural Networks, Computer"], "Authors": [{"First Name": "Gavin", "Last Name": "Mischler", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States; Department of Electrical Engineering, Columbia University, New York, United States."}, {"First Name": "Menoua", "Last Name": "Keshishian", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States; Department of Electrical Engineering, Columbia University, New York, United States."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, New York, United States."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, New York, United States."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior, Columbia University, New York, United States; Department of Electrical Engineering, Columbia University, New York, United States. Electronic address: nima@ee.columbia.edu."}], "Journal": "NeuroImage", "PubDate": "2023Feb01"}, {"PMID": "35973430", "Title": "Interaction of bottom-up and top-down neural mechanisms in spatial multi-talker speech perception.", "Abstract": "How the human auditory cortex represents spatially separated simultaneous talkers and how talkers' locations and voices modulate the neural representations of attended and unattended speech are unclear. Here, we measured the neural responses from electrodes implanted in neurosurgical patients as they performed single-talker and multi-talker speech perception tasks. We found that spatial separation between talkers caused a preferential encoding of the contralateral speech in Heschl's gyrus (HG), planum temporale (PT), and superior temporal gyrus (STG). Location and spectrotemporal features were encoded in different aspects of the neural response. Specifically, the talker's location changed the mean response level, whereas the talker's spectrotemporal features altered the variation of response around response's baseline. These components were differentially modulated by the attended talker's voice or location, which improved the population decoding of attended speech features. Attentional modulation due to the talker's voice only appeared in the auditory areas with longer latencies, but attentional modulation due to location was present throughout. Our results show that spatial multi-talker speech perception relies upon a separable pre-attentive neural representation, which could be further tuned by top-down attention to the location and voice of the talker.", "Keywords": ["attention", "auditory cortex", "sound localization in humans", "spatial multi-talker speech perception"], "MeSH terms": ["Auditory Cortex", "Humans", "Speech", "Speech Perception", "Temporal Lobe", "Voice"], "Authors": [{"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Department of Electrical Engineering, Columbia University, New York, NY 10027, USA."}, {"First Name": "Kiki", "Last Name": "van der Heijden", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Donders Institute for Brain Cognition and Behavior, Radboud University, Nijmegen, the Netherlands; Maastricht Centre for Systems Biology, Faculty of Science and Engineering, Maastricht University, Maastricht, the Netherlands."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "Hofstra Northwell School of Medicine, New York, NY 11549, USA; The Feinstein Institute for Medical Research, New York, NY 11030, USA."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Hofstra Northwell School of Medicine, New York, NY 11549, USA; The Feinstein Institute for Medical Research, New York, NY 11030, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Hofstra Northwell School of Medicine, New York, NY 11549, USA; The Feinstein Institute for Medical Research, New York, NY 11030, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Department of Electrical Engineering, Columbia University, New York, NY 10027, USA. Electronic address: nima@ee.columbia.edu."}], "Journal": "Current biology : CB", "PubDate": "2022Sep26"}, {"PMID": "35790403", "Title": "The Spatial Reach of Neuronal Coherence and Spike-Field Coupling across the Human Neocortex.", "Abstract": "Neuronal coherence is thought to be a fundamental mechanism of communication in the brain, where synchronized field potentials coordinate synaptic and spiking events to support plasticity and learning. Although the spread of field potentials has garnered great interest, little is known about the spatial reach of phase synchronization, or neuronal coherence. Functional connectivity between different brain regions is known to occur across long distances, but the locality of synchronization across the neocortex is understudied. Here we used simultaneous recordings from electrocorticography (ECoG) grids and high-density microelectrode arrays to estimate the spatial reach of neuronal coherence and spike-field coherence (SFC) across frontal, temporal, and occipital cortices during cognitive tasks in humans. We observed the strongest coherence within a 2-3 cm distance from the microelectrode arrays, potentially defining an effective range for local communication. This range was relatively consistent across brain regions, spectral frequencies, and cognitive tasks. The magnitude of coherence showed power law decay with increasing distance from the microelectrode arrays, where the highest coherence occurred between ECoG contacts, followed by coherence between ECoG and deep cortical local field potential (LFP), and then SFC (i.e., ECoG > LFP > SFC). The spectral frequency of coherence also affected its magnitude. Alpha coherence (8-14 Hz) was generally higher than other frequencies for signals nearest the microelectrode arrays, whereas delta coherence (1-3 Hz) was higher for signals that were farther away. Action potentials in all brain regions were most coherent with the phase of alpha oscillations, which suggests that alpha waves could play a larger, more spatially local role in spike timing than other frequencies. These findings provide a deeper understanding of the spatial and spectral dynamics of neuronal synchronization, further advancing knowledge about how activity propagates across the human brain.SIGNIFICANCE STATEMENT Coherence is theorized to facilitate information transfer across cerebral space by providing a convenient electrophysiological mechanism to modulate membrane potentials in spatiotemporally complex patterns. Our work uses a multiscale approach to evaluate the spatial reach of phase coherence and spike-field coherence during cognitive tasks in humans. Locally, coherence can reach up to 3 cm around a given area of neocortex. The spectral properties of coherence revealed that alpha phase-field and spike-field coherence were higher within ranges <2 cm, whereas lower-frequency delta coherence was higher for contacts farther away. Spatiotemporally shared information (i.e., coherence) across neocortex seems to reach farther than field potentials alone.", "Keywords": ["coherence", "neural oscillations", "spatial reach", "spike-field coherence"], "MeSH terms": ["Action Potentials", "Electrocorticography", "Humans", "Microelectrodes", "Neocortex", "Neurons"], "Authors": [{"First Name": "John C", "Last Name": "Myers", "Affiliation": "Department of Neurosurgery, Baylor College of Medicine, Houston, Texas 77030 john.myers@bcm.edu."}, {"First Name": "Elliot H", "Last Name": "Smith", "Affiliation": "Department of Neurosurgery, University of Utah, Salt Lake City, Utah 84132."}, {"First Name": "Marcin", "Last Name": "Leszczynski", "Affiliation": "Department of Psychiatry, Columbia University, New York, New York 10032."}, {"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027."}, {"First Name": "Mark J", "Last Name": "Yates", "Affiliation": "Department of Psychiatry, Columbia University, New York, New York 10032."}, {"First Name": "Guy", "Last Name": "McKhann", "Affiliation": "Department of Psychiatry, Columbia University, New York, New York 10032."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027."}, {"First Name": "Charles", "Last Name": "Schroeder", "Affiliation": "Department of Psychiatry, Columbia University, New York, New York 10032."}, {"First Name": "Catherine", "Last Name": "Schevon", "Affiliation": "Department of Neurology, Columbia University, New York, New York 10032."}, {"First Name": "Sameer A", "Last Name": "Sheth", "Affiliation": "Department of Neurosurgery, Baylor College of Medicine, Houston, Texas 77030."}], "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience", "PubDate": "2022Aug10"}, {"PMID": "35368278", "Title": "Editorial: Neural Tracking: Closing the Gap Between Neurophysiology and Translational Medicine.", "Abstract": "N/A", "Keywords": ["EEG", "MEG", "aging", "hearing impairment", "music perception", "neural entrainment", "neuromarker", "speech perception"], "MeSH terms": [], "Authors": [{"First Name": "Giovanni M", "Last Name": "Di Liberto", "Affiliation": "School of Computer Science and Statistics, Trinity College Dublin, Dublin, Ireland."}, {"First Name": "Jens", "Last Name": "Hjortkj\u00e6r", "Affiliation": "Hearing Systems Group, Department of Health Technology, Technical University of Denmark, Kongens Lyngby, Ireland."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Electrical Engineering Department, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, United States."}], "Journal": "Frontiers in neuroscience", "PubDate": "2022"}, {"PMID": "35347046", "Title": "Improved Speech Hearing in Noise with Invasive Electrical Brain Stimulation.", "Abstract": "Speech perception in noise is a challenging everyday task with which many listeners have difficulty. Here, we report a case in which electrical brain stimulation of implanted intracranial electrodes in the left planum temporale (PT) of a neurosurgical patient significantly and reliably improved subjective quality (up to 50%) and objective intelligibility (up to 97%) of speech in noise perception. Stimulation resulted in a selective enhancement of speech sounds compared with the background noises. The receptive fields of the PT sites whose stimulation improved speech perception were tuned to spectrally broad and rapidly changing sounds. Corticocortical evoked potential analysis revealed that the PT sites were located between the sites in Heschl's gyrus and the superior temporal gyrus. Moreover, the discriminability of speech from nonspeech sounds increased in population neural responses from Heschl's gyrus to the PT to the superior temporal gyrus sites. These findings causally implicate the PT in background noise suppression and may point to a novel potential neuroprosthetic solution to assist in the challenging task of speech perception in noise.SIGNIFICANCE STATEMENT Speech perception in noise remains a challenging task for many individuals. Here, we present a case in which the electrical brain stimulation of intracranially implanted electrodes in the planum temporale of a neurosurgical patient significantly improved both the subjective quality (up to 50%) and objective intelligibility (up to 97%) of speech perception in noise. Stimulation resulted in a selective enhancement of speech sounds compared with the background noises. Our local and network-level functional analyses placed the planum temporale sites in between the sites in the primary auditory areas in Heschl's gyrus and nonprimary auditory areas in the superior temporal gyrus. These findings causally implicate planum temporale in acoustic scene analysis and suggest potential neuroprosthetic applications to assist hearing in noise.", "Keywords": ["auditory cortex", "human electrophysiology", "speech preception"], "MeSH terms": ["Acoustic Stimulation", "Auditory Cortex", "Brain", "Brain Mapping", "Hearing", "Humans", "Magnetic Resonance Imaging", "Speech", "Speech Perception"], "Authors": [{"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York 10027."}, {"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York 10027."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Hofstra Northwell School of Medicine, New York, New York 11549."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "Hofstra Northwell School of Medicine, New York, New York 11549."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Hofstra Northwell School of Medicine, New York, New York 11549."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York 10027 nima@ee.columbia.edu."}], "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience", "PubDate": "2022Apr27"}, {"PMID": "35145280", "Title": "Multiscale temporal integration organizes hierarchical computation in human auditory cortex.", "Abstract": "To derive meaning from sound, the brain must integrate information across many timescales. What computations underlie multiscale integration in human auditory cortex? Evidence suggests that auditory cortex analyses sound using both generic acoustic representations (for example, spectrotemporal modulation tuning) and category-specific computations, but the timescales over which these putatively distinct computations integrate remain unclear. To answer this question, we developed a general method to estimate sensory integration windows-the time window when stimuli alter the neural response-and applied our method to intracranial recordings from neurosurgical patients. We show that human auditory cortex integrates hierarchically across diverse timescales spanning from ~50 to 400\u2009ms. Moreover, we find that neural populations with short and long integration windows exhibit distinct functional properties: short-integration electrodes (less than ~200\u2009ms) show prominent spectrotemporal modulation selectivity, while long-integration electrodes (greater than ~200\u2009ms) show prominent category selectivity. These findings reveal how multiscale integration organizes auditory computation in the human brain.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Auditory Cortex", "Auditory Perception", "Brain", "Brain Mapping", "Humans"], "Authors": [{"First Name": "Sam V", "Last Name": "Norman-Haignere", "Affiliation": "Zuckerman Mind, Brain, Behavior Institute, Columbia University, New York, NY, USA. samuel_norman-haignere@urmc.rochester.edu."}, {"First Name": "Laura K", "Last Name": "Long", "Affiliation": "Zuckerman Mind, Brain, Behavior Institute, Columbia University, New York, NY, USA."}, {"First Name": "Orrin", "Last Name": "Devinsky", "Affiliation": "Department of Neurology, NYU Langone Medical Center, New York, NY, USA."}, {"First Name": "Werner", "Last Name": "Doyle", "Affiliation": "Comprehensive Epilepsy Center, NYU Langone Medical Center, New York, NY, USA."}, {"First Name": "Ifeoma", "Last Name": "Irobunda", "Affiliation": "Department of Neurology, Columbia University Irving Medical Center, New York, NY, USA."}, {"First Name": "Edward M", "Last Name": "Merricks", "Affiliation": "Department of Neurology, Columbia University Irving Medical Center, New York, NY, USA."}, {"First Name": "Neil A", "Last Name": "Feldstein", "Affiliation": "Department of Neurological Surgery, Columbia University Irving Medical Center, New York, NY, USA."}, {"First Name": "Guy M", "Last Name": "McKhann", "Affiliation": "Department of Neurological Surgery, Columbia University Irving Medical Center, New York, NY, USA."}, {"First Name": "Catherine A", "Last Name": "Schevon", "Affiliation": "Department of Neurology, Columbia University Irving Medical Center, New York, NY, USA."}, {"First Name": "Adeen", "Last Name": "Flinker", "Affiliation": "Department of Neurology, NYU Langone Medical Center, New York, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Zuckerman Mind, Brain, Behavior Institute, Columbia University, New York, NY, USA. nima@ee.columbia.edu."}], "Journal": "Nature human behaviour", "PubDate": "2022Mar"}, {"PMID": "38737583", "Title": "Understanding Adaptive, Multiscale Temporal Integration In Deep Speech Recognition Systems.", "Abstract": "Natural signals such as speech are hierarchically structured across many different timescales, spanning tens (e.g., phonemes) to hundreds (e.g., words) of milliseconds, each of which is highly variable and context-dependent. While deep neural networks (DNNs) excel at recognizing complex patterns from natural signals, relatively little is known about how DNNs flexibly integrate across multiple timescales. Here, we show how a recently developed method for studying temporal integration in biological neural systems - the temporal context invariance (TCI) paradigm - can be used to understand temporal integration in DNNs. The method is simple: we measure responses to a large number of stimulus segments presented in two different contexts and estimate the smallest segment duration needed to achieve a context invariant response. We applied our method to understand how the popular DeepSpeech2 model learns to integrate across time in speech. We find that nearly all of the model units, even in recurrent layers, have a compact integration window within which stimuli substantially alter the response and outside of which stimuli have little effect. We show that training causes these integration windows to shrink at early layers and expand at higher layers, creating a hierarchy of integration windows across the network. Moreover, by measuring integration windows for time-stretched/compressed speech, we reveal a transition point, midway through the trained network, where integration windows become yoked to the duration of stimulus structures (e.g., phonemes or words) rather than absolute time. Similar phenomena were observed in a purely recurrent and purely convolutional network although structure-yoked integration was more prominent in the recurrent network. These findings suggest that deep speech recognition systems use a common motif to encode the hierarchical structure of speech: integrating across short, time-yoked windows at early layers and long, structure-yoked windows at later layers. Our method provides a straightforward and general-purpose toolkit for understanding temporal integration in black-box machine learning models.", "Keywords": [], "MeSH terms": [], "Authors": [{"First Name": "Menoua", "Last Name": "Keshishian", "Affiliation": "Department of Electrical Engineering, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027."}, {"First Name": "Sam V", "Last Name": "Norman-Haignere", "Affiliation": "Department of Electrical Engineering, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027."}], "Journal": "Advances in neural information processing systems", "PubDate": "2021Dec"}, {"PMID": "33789135", "Title": "Functional characterization of human Heschl's gyrus in response to natural speech.", "Abstract": "Heschl's gyrus (HG) is a brain area that includes the primary auditory cortex in humans. Due to the limitations in obtaining direct neural measurements from this region during naturalistic speech listening, the functional organization and the role of HG in speech perception remain uncertain. Here, we used intracranial EEG to directly record neural activity in HG in eight neurosurgical patients as they listened to continuous speech stories. We studied the spatial distribution of acoustic tuning and the organization of linguistic feature encoding. We found a main gradient of change from posteromedial to anterolateral parts of HG. We also observed a decrease in frequency and temporal modulation tuning and an increase in phonemic representation, speaker normalization, speech sensitivity, and response latency. We did not observe a difference between the two brain hemispheres. These findings reveal a functional role for HG in processing and transforming simple to complex acoustic features and inform neurophysiological models of speech processing in the human auditory cortex.", "Keywords": ["Auditory field maps", "Cortical mapping", "Heschl\u2019s gyrus", "Human auditory cortex", "Tonotopy", "iEEG"], "MeSH terms": ["Adult", "Auditory Cortex", "Brain Mapping", "Electrocorticography", "Epilepsy", "Female", "Humans", "Male", "Middle Aged", "Neurosurgical Procedures", "Speech Perception"], "Authors": [{"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Mortimer B. Zuckerman Brain Behavior Institute, Columbia University, New York, NY, United States; Department of Electrical Engineering, Columbia University, New York, NY, United States. Electronic address: bahar.kh@columbia.edu."}, {"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "Mortimer B. Zuckerman Brain Behavior Institute, Columbia University, New York, NY, United States; Department of Electrical Engineering, Columbia University, New York, NY, United States. Electronic address: pmp2138@columbia.edu."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, United States; The Feinstein Institutes for Medical Research, Manhasset, NY, United States. Electronic address: jherreroru@northwell.edu."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, United States; The Feinstein Institutes for Medical Research, Manhasset, NY, United States. Electronic address: Sbickel@northwell.edu."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, United States; The Feinstein Institutes for Medical Research, Manhasset, NY, United States. Electronic address: amehta@northwell.edu."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Brain Behavior Institute, Columbia University, New York, NY, United States; Department of Electrical Engineering, Columbia University, New York, NY, United States. Electronic address: nima@ee.columbia.edu."}], "Journal": "NeuroImage", "PubDate": "2021Jul15"}, {"PMID": "33506209", "Title": "Learning Speech Production and Perception through Sensorimotor Interactions.", "Abstract": "Action and perception are closely linked in many behaviors necessitating a close coordination between sensory and motor neural processes so as to achieve a well-integrated smoothly evolving task performance. To investigate the detailed nature of these sensorimotor interactions, and their role in learning and executing the skilled motor task of speaking, we analyzed ECoG recordings of responses in the high-\u03b3 band (70-150\u00a0Hz) in human subjects while they listened to, spoke, or silently articulated speech. We found elaborate spectrotemporally modulated neural activity projecting in both \"forward\" (motor-to-sensory) and \"inverse\" directions between the higher-auditory and motor cortical regions engaged during speaking. Furthermore, mathematical simulations demonstrate a key role for the forward projection in \"learning\" to control the vocal tract, beyond its commonly postulated predictive role during execution. These results therefore offer a broader view of the functional role of the ubiquitous forward projection as an important ingredient in learning, rather than just control, of skilled sensorimotor tasks.", "Keywords": ["auditory cortex", "human ECoG", "mirror network", "sensorimotor interactions", "speech perception", "speech production", "vocal articulation"], "MeSH terms": [], "Authors": [{"First Name": "Shihab", "Last Name": "Shamma", "Affiliation": "Department of Electrical and Computer Engineering, Institute for Systems Research, University of Maryland, College Park, MD 20742, USA."}, {"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY 10027, USA."}, {"First Name": "Shoutik", "Last Name": "Mukherjee", "Affiliation": "Department of Electrical and Computer Engineering, Institute for Systems Research, University of Maryland, College Park, MD 20742, USA."}, {"First Name": "Guilhem", "Last Name": "Marion", "Affiliation": "Laboratoire des Syst\u00e8mes Perceptifs, Department des Etudes Cognitive, \u00c9cole Normale Sup\u00e9rieure, PSL University, 75005 Paris, France."}, {"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY 10027, USA."}, {"First Name": "Cong", "Last Name": "Han", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY 10027, USA."}, {"First Name": "Jose", "Last Name": "Herrero", "Affiliation": "Neurosurgery, Hofstra Northwell School of Medicine, Manhasset, NY, USA."}, {"First Name": "Stephan", "Last Name": "Bickel", "Affiliation": "Neurosurgery, Hofstra Northwell School of Medicine, Manhasset, NY, USA."}, {"First Name": "Ashesh", "Last Name": "Mehta", "Affiliation": "Neurosurgery, Hofstra Northwell School of Medicine, Manhasset, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY 10027, USA."}], "Journal": "Cerebral cortex communications", "PubDate": "2021"}, {"PMID": "33346131", "Title": "Neural representation of linguistic feature hierarchy reflects second-language proficiency.", "Abstract": "Acquiring a new language requires individuals to simultaneously and gradually learn linguistic attributes on multiple levels. Here, we investigated how this learning process changes the neural encoding of natural speech by assessing the encoding of the linguistic feature hierarchy in second-language listeners. Electroencephalography (EEG) signals were recorded from native Mandarin speakers with varied English proficiency and from native English speakers while they listened to audio-stories in English. We measured the temporal response functions (TRFs) for acoustic, phonemic, phonotactic, and semantic features in individual participants and found a main effect of proficiency on linguistic encoding. This effect of second-language proficiency was particularly prominent on the neural encoding of phonemes, showing stronger encoding of \"new\" phonemic contrasts (i.e., English contrasts that do not exist in Mandarin) with increasing proficiency. Overall, we found that the nonnative listeners with higher proficiency levels had a linguistic feature representation more similar to that of native listeners, which enabled the accurate decoding of language proficiency. This result advances our understanding of the cortical processing of linguistic information in second-language learners and provides an objective measure of language proficiency.", "Keywords": [], "MeSH terms": ["Adolescent", "Adult", "Brain", "Comprehension", "Electroencephalography", "Female", "Humans", "Language", "Male", "Middle Aged", "Multilingualism", "Phonetics", "Speech Perception", "Young Adult"], "Authors": [{"First Name": "Giovanni M Di", "Last Name": "Liberto", "Affiliation": "Laboratoire des syst\u00e8mes perceptifs, D\u00e9partement d'\u00e9tudes cognitives, \u00c9cole normale sup\u00e9rieure, PSL University, CNRS, 75005 Paris, France. Electronic address: diliberg@tcd.ie."}, {"First Name": "Jingping", "Last Name": "Nie", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, United States."}, {"First Name": "Jeremy", "Last Name": "Yeaton", "Affiliation": "Laboratoire des syst\u00e8mes perceptifs, D\u00e9partement d'\u00e9tudes cognitives, \u00c9cole normale sup\u00e9rieure, PSL University, CNRS, 75005 Paris, France; Laboratoire de Psychologie Cognitive, UMR 7290, CNRS, France. Aix-Marseille Universit\u00e9, France."}, {"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, United States."}, {"First Name": "Shihab A", "Last Name": "Shamma", "Affiliation": "Laboratoire des syst\u00e8mes perceptifs, D\u00e9partement d'\u00e9tudes cognitives, \u00c9cole normale sup\u00e9rieure, PSL University, CNRS, 75005 Paris, France; Institute for Systems Research, Electrical and Computer Engineering, University of Maryland, College Park, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, United States. Electronic address: nima@ee.columbia.edu."}], "Journal": "NeuroImage", "PubDate": "2021Feb15"}, {"PMID": "33023923", "Title": "Crossmodal Phase Reset and Evoked Responses Provide Complementary Mechanisms for the Influence of Visual Speech in Auditory Cortex.", "Abstract": "Natural conversation is multisensory: when we can see the speaker's face, visual speech cues improve our comprehension. The neuronal mechanisms underlying this phenomenon remain unclear. The two main alternatives are visually mediated phase modulation of neuronal oscillations (excitability fluctuations) in auditory neurons and visual input-evoked responses in auditory neurons. Investigating this question using naturalistic audiovisual speech with intracranial recordings in humans of both sexes, we find evidence for both mechanisms. Remarkably, auditory cortical neurons track the temporal dynamics of purely visual speech using the phase of their slow oscillations and phase-related modulations in broadband high-frequency activity. Consistent with known perceptual enhancement effects, the visual phase reset amplifies the cortical representation of concomitant auditory speech. In contrast to this, and in line with earlier reports, visual input reduces the amplitude of evoked responses to concomitant auditory input. We interpret the combination of improved phase tracking and reduced response amplitude as evidence for more efficient and reliable stimulus processing in the presence of congruent auditory and visual speech inputs.SIGNIFICANCE STATEMENT Watching the speaker can facilitate our understanding of what is being said. The mechanisms responsible for this influence of visual cues on the processing of speech remain incompletely understood. We studied these mechanisms by recording the electrical activity of the human brain through electrodes implanted surgically inside the brain. We found that visual inputs can operate by directly activating auditory cortical areas, and also indirectly by modulating the strength of cortical responses to auditory input. Our results help to understand the mechanisms by which the brain merges auditory and visual speech into a unitary perception.", "Keywords": ["audiovisual speech", "broadband high-frequency activity", "crossmodal stimuli", "intracranial electroencephalography", "neuronal oscillations", "phase\u2013amplitude coupling"], "MeSH terms": ["Adult", "Auditory Cortex", "Drug Resistant Epilepsy", "Electrocorticography", "Evoked Potentials", "Evoked Potentials, Auditory", "Evoked Potentials, Visual", "Female", "Humans", "Middle Aged", "Neurons", "Nonverbal Communication", "Photic Stimulation", "Young Adult"], "Authors": [{"First Name": "Pierre", "Last Name": "M\u00e9gevand", "Affiliation": "Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Hempstead, New York 11549."}, {"First Name": "Manuel R", "Last Name": "Mercier", "Affiliation": "Department of Neurology, Montefiore Medical Center, Bronx, New York 10467."}, {"First Name": "David M", "Last Name": "Groppe", "Affiliation": "Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Hempstead, New York 11549."}, {"First Name": "Elana", "Last Name": "Zion Golumbic", "Affiliation": "The Gonda Brain Research Center, Bar Ilan University, Ramat Gan 5290002, Israel."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027."}, {"First Name": "Michael S", "Last Name": "Beauchamp", "Affiliation": "Department of Neurosurgery, Baylor College of Medicine, Houston, Texas 77030."}, {"First Name": "Charles E", "Last Name": "Schroeder", "Affiliation": "Nathan S. Kline Institute, Orangeburg, New York 10962 cs2388@columbia.edu amehta@northwell.edu."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Department of Neurosurgery, Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, Hempstead, New York 11549 cs2388@columbia.edu amehta@northwell.edu."}], "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience", "PubDate": "2020Oct28"}, {"PMID": "32828921", "Title": "Brain-informed speech separation (BISS) for enhancement of target speaker in multitalker speech perception.", "Abstract": "Hearing-impaired people often struggle to follow the speech stream of an individual talker in noisy environments. Recent studies show that the brain tracks attended speech and that the attended talker can be decoded from neural data on a single-trial level. This raises the possibility of \"neuro-steered\" hearing devices in which the brain-decoded intention of a hearing-impaired listener is used to enhance the voice of the attended speaker from a speech separation front-end. So far, methods that use this paradigm have focused on optimizing the brain decoding and the acoustic speech separation independently. In this work, we propose a novel framework called brain-informed speech separation (BISS)1 in which the information about the attended speech, as decoded from the subject's brain, is directly used to perform speech separation in the front-end. We present a deep learning model that uses neural data to extract the clean audio signal that a listener is attending to from a multi-talker speech mixture. We show that the framework can be applied successfully to the decoded output from either invasive intracranial electroencephalography (iEEG) or non-invasive electroencephalography (EEG) recordings from hearing-impaired subjects. It also results in improved speech separation, even in scenes with background noise. The generalization capability of the system renders it a perfect candidate for neuro-steered hearing-assistive devices.", "Keywords": ["Cognitive control", "Deep learning", "EEG", "Hearing aid", "Neuro-steered", "Speech separation"], "MeSH terms": ["Acoustic Stimulation", "Adult", "Algorithms", "Brain", "Deep Learning", "Electroencephalography", "Hearing Loss", "Humans", "Middle Aged", "Signal Processing, Computer-Assisted", "Speech Acoustics", "Speech Perception"], "Authors": [{"First Name": "Enea", "Last Name": "Ceolini", "Affiliation": "University of Z\u00fcrich and ETH Z\u00fcrich, Institute of Neuroinformatics, Switzerland. Electronic address: enea.ceolini@ini.uzh.ch."}, {"First Name": "Jens", "Last Name": "Hjortkj\u00e6r", "Affiliation": "Department of Health Technology, Danmarks Tekniske Universitet DTU, Kongens Lyngby, Denmark; Danish Research Centre for Magnetic Resonance, Copenhagen University Hospital Hvidovre, Hvidovre, Denmark."}, {"First Name": "Daniel D E", "Last Name": "Wong", "Affiliation": "Laboratoire des Syst\u00e8mes Perceptifs, CNRS, UMR 8248, Paris, France; D\u00e9partement d'\u00c9tudes Cognitives, \u00c9cole Normale Sup\u00e9rieure, PSL Research University, Paris, France."}, {"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA."}, {"First Name": "Vinay S", "Last Name": "Raghavan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA."}, {"First Name": "Jose", "Last Name": "Herrero", "Affiliation": "Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA."}, {"First Name": "Shih-Chii", "Last Name": "Liu", "Affiliation": "University of Z\u00fcrich and ETH Z\u00fcrich, Institute of Neuroinformatics, Switzerland."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA; Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA. Electronic address: nima@ee.columbia.edu."}], "Journal": "NeuroImage", "PubDate": "2020Dec"}, {"PMID": "32589140", "Title": "Estimating and interpreting nonlinear receptive field of sensory neural responses with deep neural network models.", "Abstract": "Our understanding of nonlinear stimulus transformations by neural circuits is hindered by the lack of comprehensive yet interpretable computational modeling frameworks. Here, we propose a data-driven approach based on deep neural networks to directly model arbitrarily nonlinear stimulus-response mappings. Reformulating the exact function of a trained neural network as a collection of stimulus-dependent linear functions enables a locally linear receptive field interpretation of the neural network. Predicting the neural responses recorded invasively from the auditory cortex of neurosurgical patients as they listened to speech, this approach significantly improves the prediction accuracy of auditory cortical responses, particularly in nonprimary areas. Moreover, interpreting the functions learned by neural networks uncovered three distinct types of nonlinear transformations of speech that varied considerably from primary to nonprimary auditory regions. The ability of this framework to capture arbitrary stimulus-response mappings while maintaining model interpretability leads to a better understanding of cortical processing of sensory signals.", "Keywords": ["computational modeling", "human", "human auditory cortex", "neuroscience", "sensory processing", "speech"], "MeSH terms": ["Acoustic Stimulation", "Auditory Cortex", "Auditory Perception", "Electrocorticography", "Humans", "Models, Neurological", "Neural Networks, Computer", "Nonlinear Dynamics", "Sensory Receptor Cells", "Speech"], "Authors": [{"First Name": "Menoua", "Last Name": "Keshishian", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, United States."}, {"First Name": "Hassan", "Last Name": "Akbari", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, United States."}, {"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, United States."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Feinstein Institute for Medical Research, Manhasset, United States."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Feinstein Institute for Medical Research, Manhasset, United States."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, United States."}], "Journal": "eLife", "PubDate": "2020Jun26"}, {"PMID": "32122465", "Title": "Cortical encoding of melodic expectations in human temporal cortex.", "Abstract": "Humans engagement in music rests on underlying elements such as the listeners' cultural background and interest in music. These factors modulate how listeners anticipate musical events, a process inducing instantaneous neural responses as the music confronts these expectations. Measuring such neural correlates would represent a direct window into high-level brain processing. Here we recorded cortical signals as participants listened to Bach melodies. We assessed the relative contributions of acoustic versus melodic components of the music to the neural signal. Melodic features included information on pitch progressions and their tempo, which were extracted from a predictive model of musical structure based on Markov chains. We related the music to brain activity with temporal response functions demonstrating, for the first time, distinct cortical encoding of pitch and note-onset expectations during naturalistic music listening. This encoding was most pronounced at response latencies up to 350 ms, and in both planum temporale and Heschl's gyrus.", "Keywords": ["cortical signals", "expectations", "human", "markov model", "music", "neuroscience", "pitch", "sensory"], "MeSH terms": ["Acoustic Stimulation", "Auditory Perception", "Electroencephalography", "Evoked Potentials, Auditory", "Humans", "Music", "Reaction Time", "Temporal Lobe"], "Authors": [{"First Name": "Giovanni M", "Last Name": "Di Liberto", "Affiliation": "Laboratoire des syst\u00e8mes perceptifs, D\u00e9partement d'\u00e9tudes cognitives, \u00c9cole normale sup\u00e9rieure, PSL University, CNRS, 75005 Paris, France."}, {"First Name": "Claire", "Last Name": "Pelofi", "Affiliation": "Department of Psychology, New York University, New York, United States."}, {"First Name": "Roberta", "Last Name": "Bianco", "Affiliation": "UCL Ear Institute, London, United Kingdom."}, {"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, United States."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Department of Neurosurgery, Zucker School of Medicine at Hofstra/Northwell, Manhasset, United States."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Department of Neurosurgery, Zucker School of Medicine at Hofstra/Northwell, Manhasset, United States."}, {"First Name": "Alain", "Last Name": "de Cheveign\u00e9", "Affiliation": "Laboratoire des syst\u00e8mes perceptifs, D\u00e9partement d'\u00e9tudes cognitives, \u00c9cole normale sup\u00e9rieure, PSL University, CNRS, 75005 Paris, France."}, {"First Name": "Shihab", "Last Name": "Shamma", "Affiliation": "Laboratoire des syst\u00e8mes perceptifs, D\u00e9partement d'\u00e9tudes cognitives, \u00c9cole normale sup\u00e9rieure, PSL University, CNRS, 75005 Paris, France."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, United States."}], "Journal": "eLife", "PubDate": "2020Mar03"}, {"PMID": "31893764", "Title": "General properties of auditory spectro-temporal receptive fields.", "Abstract": "The analyses of more than 2000 spectro-temporal receptive fields (STRFs) obtained from the ferret primary auditory cortex revealed their dominant encoding properties along time and frequency domains. Results showed that the peak responses of the STRFs were roughly aligned along the time axis, and enhanced low modulation components of the signal around 3\u2009Hz. On the contrary, the peaks of the STRF population along the frequency axis varied and were distributed widely. Further analyses revealed that there were some general properties along the frequency axis around the best frequency of each STRF. The STRFs enhanced modulation frequencies around 0.25 cycles/octave. These findings are consistent with techniques that have been reported in the literature to result in improvements in speech recognition performance, highlighting the significant potential of biological insights into the design of better machines.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Action Potentials", "Animals", "Auditory Cortex", "Auditory Perception", "Evoked Potentials, Auditory", "Ferrets", "Models, Neurological", "Neurons"], "Authors": [{"First Name": "Nagaraj R", "Last Name": "Mahajan", "Affiliation": "Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, Maryland 21218, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027, USAnagaraj.mahajan@jhu.edu, nima@ee.columbia.edu, hynek@jhu.edu."}, {"First Name": "Hynek", "Last Name": "Hermansky", "Affiliation": "Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, Maryland 21218, USA."}], "Journal": "The Journal of the Acoustical Society of America", "PubDate": "2019Dec"}, {"PMID": "31648900", "Title": "Hierarchical Encoding of Attended Auditory Objects in Multi-talker Speech Perception.", "Abstract": "Humans can easily focus on one speaker in a multi-talker acoustic environment, but how different areas of the human auditory cortex (AC) represent the acoustic components of mixed speech is unknown. We obtained invasive recordings from the primary and nonprimary AC in neurosurgical patients as they listened to multi-talker speech. We found that neural sites in the primary AC responded to individual speakers in the mixture and were relatively unchanged by attention. In contrast, neural sites in the nonprimary AC were less discerning of individual speakers but selectively represented the attended speaker. Moreover, the encoding of the attended speaker in the nonprimary AC was invariant to the degree of acoustic overlap with the unattended speaker. Finally, this emergent representation of attended speech in the nonprimary AC was linearly predictable from the primary AC responses. Our results reveal the neural computations underlying the hierarchical formation of auditory objects in human AC during multi-talker speech perception.", "Keywords": ["Heschl\u2019s gyrus", "auditory object", "cocktail party", "encoding", "hierarchical", "human auditory cortex", "multi-talker", "speech perception", "superior temporal gyrus"], "MeSH terms": ["Attention", "Auditory Cortex", "Humans", "Speech Perception"], "Authors": [{"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "Jose", "Last Name": "Herrero", "Affiliation": "Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA."}, {"First Name": "Elliot", "Last Name": "Smith", "Affiliation": "Department of Neurological Surgery, The Neurological Institute, New York, NY, USA; Department of Neurosurgery, University of Utah, Salt Lake City, UT, USA."}, {"First Name": "Catherine", "Last Name": "Schevon", "Affiliation": "Department of Neurological Surgery, The Neurological Institute, New York, NY, USA."}, {"First Name": "Guy M", "Last Name": "McKhann", "Affiliation": "Department of Neurological Surgery, The Neurological Institute, New York, NY, USA."}, {"First Name": "Sameer A", "Last Name": "Sheth", "Affiliation": "Department of Neurological Surgery, The Neurological Institute, New York, NY, USA; Department of Neurosurgery, Baylor College of Medicine, Houston, TX, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA. Electronic address: nima@ee.columbia.edu."}], "Journal": "Neuron", "PubDate": "2019Dec18"}, {"PMID": "31485462", "Title": "Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation.", "Abstract": "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency of the entire system. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a much shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications. This study therefore represents a major step toward the realization of speech separation systems for real-world speech processing technologies.", "Keywords": ["Source separation", "deep learning", "real-time", "single-channel", "time-domain"], "MeSH terms": [], "Authors": [{"First Name": "Yi", "Last Name": "Luo", "Affiliation": "N/A"}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}], "Journal": "IEEE/ACM transactions on audio, speech, and language processing", "PubDate": "2019Aug"}, {"PMID": "31395905", "Title": "Comparison of Two-Talker Attention Decoding from EEG with Nonlinear Neural Networks and Linear Methods.", "Abstract": "Auditory attention decoding (AAD) through a brain-computer interface has had a flowering of developments since it was first introduced by Mesgarani and Chang (2012) using electrocorticograph recordings. AAD has been pursued for its potential application to hearing-aid design in which an attention-guided algorithm selects, from multiple competing acoustic sources, which should be enhanced for the listener and which should be suppressed. Traditionally, researchers have separated the AAD problem into two stages: reconstruction of a representation of the attended audio from neural signals, followed by determining the similarity between the candidate audio streams and the reconstruction. Here, we compare the traditional two-stage approach with a novel neural-network architecture that subsumes the explicit similarity step. We compare this new architecture against linear and non-linear (neural-network) baselines using both wet and dry electroencephalogram (EEG) systems. Our results indicate that the new architecture outperforms the baseline linear stimulus-reconstruction method, improving decoding accuracy from 66% to 81% using wet EEG and from 59% to 87% for dry EEG. Also of note was the finding that the dry EEG system can deliver comparable or even better results than the wet, despite the latter having one third as many EEG channels as the former. The 11-subject, wet-electrode AAD dataset for two competing, co-located talkers, the 11-subject, dry-electrode AAD dataset, and our software are available for further validation, experimentation, and modification.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Algorithms", "Attention", "Auditory Cortex", "Brain-Computer Interfaces", "Electrocorticography", "Electroencephalography", "Hearing Aids", "Humans", "Linear Models", "Neural Networks, Computer", "Noise", "Nonlinear Dynamics", "Speech Perception"], "Authors": [{"First Name": "Gregory", "Last Name": "Ciccarelli", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA."}, {"First Name": "Michael", "Last Name": "Nolan", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA."}, {"First Name": "Joseph", "Last Name": "Perricone", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA."}, {"First Name": "Paul T", "Last Name": "Calamia", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA."}, {"First Name": "Stephanie", "Last Name": "Haro", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA."}, {"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "Thomas F", "Last Name": "Quatieri", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA."}, {"First Name": "Christopher J", "Last Name": "Smalt", "Affiliation": "Bioengineering Systems and Technologies Group, MIT Lincoln Laboratory, Lexington, MA, USA. christopher.smalt@ll.mit.edu."}], "Journal": "Scientific reports", "PubDate": "2019Aug08"}, {"PMID": "31175304", "Title": "Adaptation of the human auditory cortex to changing background noise.", "Abstract": "Speech communication in real-world environments requires adaptation to changing acoustic conditions. How the human auditory cortex adapts as a new noise source appears in or disappears from the acoustic scene remain unclear. Here, we directly measured neural activity in the auditory cortex of six human subjects as they listened to speech with abruptly changing background noises. We report rapid and selective suppression of acoustic features of noise in the neural responses. This suppression results in enhanced representation and perception of speech acoustic features. The degree of adaptation to different background noises varies across neural sites and is predictable from the tuning properties and speech specificity of the sites. Moreover, adaptation to background noise is unaffected by the attentional focus of the listener. The convergence of these neural and perceptual effects reveals the intrinsic dynamic mechanisms that enable a listener to filter out irrelevant sound sources in a changing acoustic scene.", "Keywords": [], "MeSH terms": ["Adaptation, Physiological", "Adult", "Attention", "Auditory Cortex", "Drug Resistant Epilepsy", "Electrocorticography", "Female", "Humans", "Male", "Noise", "Speech Acoustics", "Speech Perception"], "Authors": [{"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, 10027, USA."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, 11549, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, 11549, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, 10027, USA. nima@ee.columbia.edu."}], "Journal": "Nature communications", "PubDate": "2019Jun07"}, {"PMID": "31106271", "Title": "Speaker-independent auditory attention decoding without access to clean speech sources.", "Abstract": "Speech perception in crowded environments is challenging for hearing-impaired listeners. Assistive hearing devices cannot lower interfering speakers without knowing which speaker the listener is focusing on. One possible solution is auditory attention decoding in which the brainwaves of listeners are compared with sound sources to determine the attended source, which can then be amplified to facilitate hearing. In realistic situations, however, only mixed audio is available. We utilize a novel speech separation algorithm to automatically separate speakers in mixed audio, with no need for the speakers to have prior training. Our results show that auditory attention decoding with automatically separated speakers is as accurate and fast as using clean speech sounds. The proposed method significantly improves the subjective and objective quality of the attended speaker. Our study addresses a major obstacle in actualization of auditory attention decoding that can assist hearing-impaired listeners and reduce listening effort for normal-hearing subjects.", "Keywords": [], "MeSH terms": ["Acoustics", "Algorithms", "Attention", "Auditory Cortex", "Auditory Perception", "Behavior", "Brain-Computer Interfaces", "Electrodes", "Female", "Hearing", "Humans", "Language", "Male", "Neural Networks, Computer", "Pattern Recognition, Automated", "Speech", "Speech Perception"], "Authors": [{"First Name": "Cong", "Last Name": "Han", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "Yi", "Last Name": "Luo", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}, {"First Name": "Jose", "Last Name": "Herrero", "Affiliation": "Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Department of Neurosurgery, Hofstra-Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, USA."}], "Journal": "Science advances", "PubDate": "2019May"}, {"PMID": "30696881", "Title": "Towards reconstructing intelligible speech from the human auditory cortex.", "Abstract": "Auditory stimulus reconstruction is a technique that finds the best approximation of the acoustic stimulus from the population of evoked neural activity. Reconstructing speech from the human auditory cortex creates the possibility of a speech neuroprosthetic to establish a direct communication with the brain and has been shown to be possible in both overt and covert conditions. However, the low quality of the reconstructed speech has severely limited the utility of this method for brain-computer interface (BCI) applications. To advance the state-of-the-art in speech neuroprosthesis, we combined the recent advances in deep learning with the latest innovations in speech synthesis technologies to reconstruct closed-set intelligible speech from the human auditory cortex. We investigated the dependence of reconstruction accuracy on linear and nonlinear (deep neural network) regression methods and the acoustic representation that is used as the target of reconstruction, including auditory spectrogram and speech synthesis parameters. In addition, we compared the reconstruction accuracy from low and high neural frequency ranges. Our results show that a deep neural network model that directly estimates the parameters of a speech synthesizer from all neural frequencies achieves the highest subjective and objective scores on a digit recognition task, improving the intelligibility by 65% over the baseline method which used linear regression to reconstruct the auditory spectrogram. These results demonstrate the efficacy of deep learning and speech synthesis algorithms for designing the next generation of speech BCI systems, which not only can restore communications for paralyzed patients but also have the potential to transform human-computer interaction technologies.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Algorithms", "Auditory Cortex", "Brain Mapping", "Deep Learning", "Evoked Potentials, Auditory", "Humans", "Neural Networks, Computer", "Neural Prostheses", "Speech", "Speech Intelligibility", "Speech Perception"], "Authors": [{"First Name": "Hassan", "Last Name": "Akbari", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, United States."}, {"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, United States."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, United States."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Hofstra Northwell School of Medicine, Manhasset, NY, United States."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, United States. nima@ee.columbia.edu."}], "Journal": "Scientific reports", "PubDate": "2019Jan29"}, {"PMID": "30134167", "Title": "Joint Representation of Spatial and Phonetic Features in the Human Core Auditory Cortex.", "Abstract": "The human auditory cortex simultaneously processes speech and determines the location of a speaker in space. Neuroimaging studies in humans have implicated core auditory areas in processing the spectrotemporal and the spatial content of sound; however, how these features are represented together is unclear. We recorded directly from human subjects implanted bilaterally with depth electrodes in core auditory areas as they listened to speech from different directions. We found local and joint selectivity to spatial and spectrotemporal speech features, where the spatial and spectrotemporal features are organized independently of each other. This representation enables successful decoding of both spatial and phonetic information. Furthermore, we found that the location of the speaker does not change the spectrotemporal tuning of the electrodes but, rather, modulates their mean response level. Our findings contribute to defining the functional organization of responses in the human auditory cortex, with implications for more accurate neurophysiological models of speech processing.", "Keywords": ["auditory cortex", "binaural sound", "electrocorticography", "sound localization", "speech"], "MeSH terms": ["Acoustic Stimulation", "Auditory Cortex", "Auditory Perception", "Humans", "Phonetics"], "Authors": [{"First Name": "Prachi", "Last Name": "Patel", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Department of Electrical Engineering, Columbia University, New York, NY 10027, USA."}, {"First Name": "Laura K", "Last Name": "Long", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Doctoral Program in Neurobiology and Behavior, Columbia University, New York, NY 10027, USA."}, {"First Name": "Jose L", "Last Name": "Herrero", "Affiliation": "Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, New York City, NY 11549, USA; The Feinstein Institute for Medical Research, New York City, NY 11030, USA."}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "Donald and Barbara Zucker School of Medicine at Hofstra/Northwell, New York City, NY 11549, USA; The Feinstein Institute for Medical Research, New York City, NY 11030, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Department of Electrical Engineering, Columbia University, New York, NY 10027, USA. Electronic address: nima@ee.columbia.edu."}], "Journal": "Cell reports", "PubDate": "2018Aug21"}, {"PMID": "29430213", "Title": "NAPLIB: AN OPEN SOURCE TOOLBOX FOR REAL-TIME AND OFFLINE NEURAL ACOUSTIC PROCESSING.", "Abstract": "In this paper, we introduce the Neural Acoustic Processing Library (NAPLib), a toolbox containing novel processing methods for real-time and offline analysis of neural activity in response to speech. Our method divides the speech signal and resultant neural activity into segmental units (e.g., phonemes), allowing for fast and efficient computations that can be implemented in real-time. NAPLib contains a suite of tools that characterize various properties of the neural representation of speech, which can be used for functionality such as characterizing electrode tuning properties, brain mapping and brain computer interfaces. The library is general and applicable to both invasive and non-invasive recordings, including electroencephalography (EEG), electrocorticography (ECoG) and magnetoecnephalography (MEG). In this work, we describe the structure of NAPLib, as well as demonstrating its use in both EEG and ECoG. We believe NAPLib provides a valuable tool to both clinicians and researchers who are interested in the representation of speech in the brain.", "Keywords": ["ECoG", "EEG", "auditory neuroscience", "brain mapping", "real-time processing"], "MeSH terms": [], "Authors": [{"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, USA."}, {"First Name": "Tasha", "Last Name": "Nagamine", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, USA."}, {"First Name": "Ashesh", "Last Name": "Mehta", "Affiliation": "Department of Neurosurgery, Hofstra Northwell School of Medicine and Feinstein Institute for Medical Research, Manhasset, New York, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, USA."}], "Journal": "Proceedings of the ... IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP (Conference)", "PubDate": "2017Mar"}, {"PMID": "29430212", "Title": "DEEP ATTRACTOR NETWORK FOR SINGLE-MICROPHONE SPEAKER SEPARATION.", "Abstract": "Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49% improvement over the previous state-of-the-art methods.", "Keywords": ["Source separation", "attractor network", "deep clustering", "multi-talker"], "MeSH terms": [], "Authors": [{"First Name": "Zhuo", "Last Name": "Chen", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}, {"First Name": "Yi", "Last Name": "Luo", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}], "Journal": "Proceedings of the ... IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP (Conference)", "PubDate": "2017Mar"}, {"PMID": "29398973", "Title": "DEEP CLUSTERING AND CONVENTIONAL NETWORKS FOR MUSIC SEPARATION: STRONGER TOGETHER.", "Abstract": "Deep clustering is the first method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation, presumably because its more flexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination significantly outperforms either of its components.", "Keywords": ["Deep clustering", "Deep learning", "Music separation", "Singing voice separation"], "MeSH terms": [], "Authors": [{"First Name": "Yi", "Last Name": "Luo", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}, {"First Name": "Zhuo", "Last Name": "Chen", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}, {"First Name": "John R", "Last Name": "Hershey", "Affiliation": "Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA."}, {"First Name": "Jonathan", "Last Name": "Le Roux", "Affiliation": "Mitsubishi Electric Research Laboratories (MERL), Cambridge, MA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY."}], "Journal": "Proceedings of the ... IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP (Conference)", "PubDate": "2017Mar"}, {"PMID": "29397190", "Title": "Neural correlates of sine-wave speech intelligibility in human frontal and temporal cortex.", "Abstract": "Auditory speech comprehension is the result of neural computations that occur in a broad network that includes the temporal lobe auditory cortex and the left inferior frontal cortex. It remains unclear how representations in this network differentially contribute to speech comprehension. Here, we recorded high-density direct cortical activity during a sine-wave speech (SWS) listening task to examine detailed neural speech representations when the exact same acoustic input is comprehended versus not comprehended. Listeners heard SWS sentences (pre-exposure), followed by clear versions of the same sentences, which revealed the content of the sounds (exposure), and then the same SWS sentences again (post-exposure). Across all three task phases, high-gamma neural activity in the superior temporal gyrus was similar, distinguishing different words based on bottom-up acoustic features. In contrast, frontal regions showed a more pronounced and sudden increase in activity only when the input was comprehended, which corresponded with stronger representational separability among spatiotemporal activity patterns evoked by different words. We observed this effect only in participants who were not able to comprehend the stimuli during the pre-exposure phase, indicating a relationship between frontal high-gamma activity and speech understanding. Together, these results demonstrate that both frontal and temporal cortical networks are involved in spoken language understanding, and that under certain listening conditions, frontal regions are involved in discriminating speech sounds.", "Keywords": ["Auditory", "Comprehension", "Electrocorticography", "Language", "Perception", "Sine-wave speech", "Speech"], "MeSH terms": ["Adult", "Connectome", "Female", "Frontal Lobe", "Humans", "Male", "Speech Intelligibility", "Speech Perception", "Temporal Lobe"], "Authors": [{"First Name": "Sattar", "Last Name": "Khoshkhoo", "Affiliation": "School of Medicine, University of California, San Francisco, 505 Parnassus Ave., San Francisco, CA 94143, United States."}, {"First Name": "Matthew K", "Last Name": "Leonard", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, 505 Parnassus Ave., San Francisco, CA 94143, United States; Center for Integrative Neuroscience, University of California, San Francisco, 675 Nelson Rising Ln., Room 535, San Francisco, CA 94158, United States; Weill Institute for Neurosciences, University of California, San Francisco, 675 Nelson Rising Ln., Room 535, San Francisco, CA 94158, United States."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, Mudd Building, Room 1339, 500 W 120th St., New York, NY 10027, United States."}, {"First Name": "Edward F", "Last Name": "Chang", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, 505 Parnassus Ave., San Francisco, CA 94143, United States; Center for Integrative Neuroscience, University of California, San Francisco, 675 Nelson Rising Ln., Room 535, San Francisco, CA 94158, United States; Weill Institute for Neurosciences, University of California, San Francisco, 675 Nelson Rising Ln., Room 535, San Francisco, CA 94158, United States. Electronic address: Edward.Chang@ucsf.edu."}], "Journal": "Brain and language", "PubDate": "2018Dec"}, {"PMID": "29060199", "Title": "Neural decoding of attentional selection in multi-speaker environments without access to separated sources.", "Abstract": "People who suffer from hearing impairments can find it difficult to follow a conversation in a multi-speaker environment. Modern hearing aids can suppress background noise; however, there is little that can be done to help a user attend to a single conversation without knowing which speaker is being attended to. Cognitively controlled hearing aids that use auditory attention decoding (AAD) methods are the next step in offering help. A number of challenges exist, including the lack of access to the clean sound sources in the environment with which to compare with the neural signals. We propose a novel framework that combines single-channel speech separation algorithms with AAD. We present an end-to-end system that 1) receives a single audio channel containing a mixture of speakers that is heard by a listener along with the listener's neural signals, 2) automatically separates the individual speakers in the mixture, 3) determines the attended speaker, and 4) amplifies the attended speaker's voice to assist the listener. Using invasive electrophysiology recordings, our system is able to decode the attention of a subject and detect switches in attention using only the mixed audio. We also identified the regions of the auditory cortex that contribute to AAD. Our quality assessment of the modified audio demonstrates a significant improvement in both subjective and objective speech quality measures. Our novel framework for AAD bridges the gap between the most recent advancements in speech processing technologies and speech prosthesis research and moves us closer to the development of cognitively controlled hearing aids.", "Keywords": [], "MeSH terms": ["Attention", "Auditory Cortex", "Hearing Aids", "Noise", "Speech Perception"], "Authors": [{"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "N/A"}, {"First Name": "N/A", "Last Name": "Zhuo Chen", "Affiliation": "N/A"}, {"First Name": "Sameer A", "Last Name": "Sheth", "Affiliation": "N/A"}, {"First Name": "Guy", "Last Name": "McKhann", "Affiliation": "N/A"}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "N/A"}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}], "Journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "PubDate": "2017Jul"}, {"PMID": "28776506", "Title": "Neural decoding of attentional selection in multi-speaker environments without access to clean sources.", "Abstract": "People who suffer from hearing impairments can find it difficult to follow a conversation in a multi-speaker environment. Current hearing aids can suppress background noise; however, there is little that can be done to help a user attend to a single conversation amongst many without knowing which speaker the user is attending to. Cognitively controlled hearing aids that use auditory attention decoding (AAD) methods are the next step in offering help. Translating the successes in AAD research to real-world applications poses a number of challenges, including the lack of access to the clean sound sources in the environment with which to compare with the neural signals. We propose a novel framework that combines single-channel speech separation algorithms with AAD.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Auditory Cortex", "Auditory Perception", "Electrodes, Implanted", "Electroencephalography", "Female", "Hearing Aids", "Humans", "Male", "Nerve Net", "Speech Perception"], "Authors": [{"First Name": "James", "Last Name": "O'Sullivan", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, United States of America. Mortimer B Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, United States of America."}, {"First Name": "Zhuo", "Last Name": "Chen", "Affiliation": "N/A"}, {"First Name": "Jose", "Last Name": "Herrero", "Affiliation": "N/A"}, {"First Name": "Guy M", "Last Name": "McKhann", "Affiliation": "N/A"}, {"First Name": "Sameer A", "Last Name": "Sheth", "Affiliation": "N/A"}, {"First Name": "Ashesh D", "Last Name": "Mehta", "Affiliation": "N/A"}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}], "Journal": "Journal of neural engineering", "PubDate": "2017Oct"}, {"PMID": "31549099", "Title": "Understanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition.", "Abstract": "Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform. These methods are used to discern and quantify properties of feedforward neural networks trained to map acoustic features to phoneme labels. We show a selective and nonlinear warping of the feature space, achieved by forming prototypical functions to account for the possible variation of each class. This study provides a joint framework where the properties of node activations and the functions implemented by the network can be linked together.", "Keywords": [], "MeSH terms": [], "Authors": [{"First Name": "Tasha", "Last Name": "Nagamine", "Affiliation": "Columbia University, New York, NY, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Columbia University, New York, NY, USA."}], "Journal": "Proceedings of machine learning research", "PubDate": "2017Aug"}, {"PMID": "28286424", "Title": "SYNAPTIC DEPRESSION IN DEEP NEURAL NETWORKS FOR SPEECH PROCESSING.", "Abstract": "A characteristic property of biological neurons is their ability to dynamically change the synaptic efficacy in response to variable input conditions. This mechanism, known as synaptic depression, significantly contributes to the formation of normalized representation of speech features. Synaptic depression also contributes to the robust performance of biological systems. In this paper, we describe how synaptic depression can be modeled and incorporated into deep neural network architectures to improve their generalization ability. We observed that when synaptic depression is added to the hidden layers of a neural network, it reduces the effect of changing background activity in the node activations. In addition, we show that when synaptic depression is included in a deep neural network trained for phoneme classification, the performance of the network improves under noisy conditions not included in the training phase. Our results suggest that more complete neuron models may further reduce the gap between the biological performance and artificial computing, resulting in networks that better generalize to novel signal conditions.", "Keywords": ["deep learning", "neural network", "phoneme recognition", "synaptic depression"], "MeSH terms": [], "Authors": [{"First Name": "Wenhao", "Last Name": "Zhang", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, 10027."}, {"First Name": "Hanyu", "Last Name": "Li", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, 10027."}, {"First Name": "Minda", "Last Name": "Yang", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, 10027."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, NY, 10027."}], "Journal": "Proceedings of the ... IEEE International Conference on Acoustics, Speech, and Signal Processing. ICASSP (Conference)", "PubDate": "2016Mar"}, {"PMID": "28268946", "Title": "Designing a hands-on brain computer interface laboratory course.", "Abstract": "Devices and systems that interact with the brain have become a growing field of research and development in recent years. Engineering students are well positioned to contribute to both hardware development and signal analysis techniques in this field. However, this area has been left out of most engineering curricula. We developed an electroencephalography (EEG) based brain computer interface (BCI) laboratory course to educate students through hands-on experiments. The course is offered jointly by the Biomedical Engineering, Electrical Engineering, and Computer Science Departments of Columbia University in the City of New York and is open to senior undergraduate and graduate students. The course provides an effective introduction to the experimental design, neuroscience concepts, data analysis techniques, and technical skills required in the field of BCI.", "Keywords": [], "MeSH terms": ["Biomedical Engineering", "Brain-Computer Interfaces", "Curriculum", "Goals", "Humans", "Laboratories", "Students", "Surveys and Questionnaires", "Universities"], "Authors": [{"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "N/A"}, {"First Name": "Laura Kathleen", "Last Name": "Long", "Affiliation": "N/A"}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}], "Journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "PubDate": "2016Aug"}, {"PMID": "28119400", "Title": "Dynamic Encoding of Acoustic Features in Neural Responses to Continuous Speech.", "Abstract": "Humans are unique in their ability to communicate using spoken language. However, it remains unclear how the speech signal is transformed and represented in the brain at different stages of the auditory pathway. In this study, we characterized electroencephalography responses to continuous speech by obtaining the time-locked responses to phoneme instances (phoneme-related potential). We showed that responses to different phoneme categories are organized by phonetic features. We found that each instance of a phoneme in continuous speech produces multiple distinguishable neural responses occurring as early as 50 ms and as late as 400 ms after the phoneme onset. Comparing the patterns of phoneme similarity in the neural responses and the acoustic signals confirms a repetitive appearance of acoustic distinctions of phonemes in the neural data. Analysis of the phonetic and speaker information in neural activations revealed that different time intervals jointly encode the acoustic similarity of both phonetic and speaker categories. These findings provide evidence for a dynamic neural transformation of low-level speech features as they propagate along the auditory pathway, and form an empirical framework to study the representational changes in learning, attention, and speech disorders.SIGNIFICANCE STATEMENT We characterized the properties of evoked neural responses to phoneme instances in continuous speech. We show that each instance of a phoneme in continuous speech produces several observable neural responses at different times occurring as early as 50 ms and as late as 400 ms after the phoneme onset. Each temporal event explicitly encodes the acoustic similarity of phonemes, and linguistic and nonlinguistic information are best represented at different time intervals. Finally, we show a joint encoding of phonetic and speaker information, where the neural representation of speakers is dependent on phoneme category. These findings provide compelling new evidence for dynamic processing of speech sounds in the auditory pathway.", "Keywords": ["EEG", "event-related potential", "phonemes", "speech"], "MeSH terms": ["Acoustic Stimulation", "Acoustics", "Brain Mapping", "Electroencephalography", "Evoked Potentials, Auditory", "Female", "Humans", "Language", "Male", "Phonetics", "Reaction Time", "Speech", "Speech Perception", "Statistics as Topic", "Time Factors"], "Authors": [{"First Name": "Bahar", "Last Name": "Khalighinejad", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027."}, {"First Name": "Guilherme", "Last Name": "Cruzatto da Silva", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027 nima@ee.columbia.edu."}], "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience", "PubDate": "2017Feb22"}, {"PMID": "27927954", "Title": "Predictive Ensemble Decoding of Acoustical Features Explains Context-Dependent Receptive Fields.", "Abstract": "A primary goal of auditory neuroscience is to identify the sound features extracted and represented by auditory neurons. Linear encoding models, which describe neural responses as a function of the stimulus, have been primarily used for this purpose. Here, we provide theoretical arguments and experimental evidence in support of an alternative approach, based on decoding the stimulus from the neural response. We used a Bayesian normative approach to predict the responses of neurons detecting relevant auditory features, despite ambiguities and noise. We compared the model predictions to recordings from the primary auditory cortex of ferrets and found that: (1) the decoding filters of auditory neurons resemble the filters learned from the statistics of speech sounds; (2) the decoding model captures the dynamics of responses better than a linear encoding model of similar complexity; and (3) the decoding model accounts for the accuracy with which the stimulus is represented in neural activity, whereas linear encoding model performs very poorly. Most importantly, our model predicts that neuronal responses are fundamentally shaped by \"explaining away,\" a divisive competition between alternative interpretations of the auditory scene.", "Keywords": ["Bayesian", "auditory cortex", "decoding", "encoding", "explaining away", "predictive coding"], "MeSH terms": ["Acoustic Stimulation", "Algorithms", "Animals", "Auditory Cortex", "Auditory Perception", "Bayes Theorem", "Female", "Ferrets", "Male", "Models, Neurological", "Nerve Net", "Noise", "Phonetics", "Sensory Receptor Cells"], "Authors": [{"First Name": "Izzet B", "Last Name": "Yildiz", "Affiliation": "Group for Neural Theory, Laboratoire de Neurosciences Cognitives, D\u00e9partement d'Etudes Cognitives, Ecole Normale Sup\u00e9rieure, 75005 Paris, France, and izzet.burak.yildiz@gmail.com."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, New York, New York 10027."}, {"First Name": "Sophie", "Last Name": "Deneve", "Affiliation": "Group for Neural Theory, Laboratoire de Neurosciences Cognitives, D\u00e9partement d'Etudes Cognitives, Ecole Normale Sup\u00e9rieure, 75005 Paris, France, and."}], "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience", "PubDate": "2016Dec07"}, {"PMID": "27484713", "Title": "Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity.", "Abstract": "The superior temporal gyrus (STG) and neighboring brain regions play a key role in human language processing. Previous studies have attempted to reconstruct speech information from brain activity in the STG, but few of them incorporate the probabilistic framework and engineering methodology used in modern speech recognition systems. In this work, we describe the initial efforts toward the design of a neural speech recognition (NSR) system that performs continuous phoneme recognition on English stimuli with arbitrary vocabulary sizes using the high gamma band power of local field potentials in the STG and neighboring cortical areas obtained via electrocorticography.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Algorithms", "Auditory Cortex", "Cerebral Cortex", "Computer Simulation", "Discriminant Analysis", "Electrocorticography", "Electrodes, Implanted", "Female", "Gamma Rhythm", "Humans", "Likelihood Functions", "Male", "Markov Chains", "Reproducibility of Results", "Sex Characteristics", "Speech Recognition Software", "Temporal Lobe"], "Authors": [{"First Name": "David A", "Last Name": "Moses", "Affiliation": "Department of Neurological Surgery, UC San Francisco, CA, USA. Center for Integrative Neuroscience, UC San Francisco, CA, USA. Graduate Program in Bioengineering, UC Berkeley-UC San Francisco, CA, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}, {"First Name": "Matthew K", "Last Name": "Leonard", "Affiliation": "N/A"}, {"First Name": "Edward F", "Last Name": "Chang", "Affiliation": "N/A"}], "Journal": "Journal of neural engineering", "PubDate": "2016Oct"}, {"PMID": "29359204", "Title": "Analyzing Distributional Learning of Phonemic Categories in Unsupervised Deep Neural Networks.", "Abstract": "Infants' speech perception adapts to the phonemic categories of their native language, a process assumed to be driven by the distributional properties of speech. This study investigates whether deep neural networks (DNNs), the current state-of-the-art in distributional feature learning, are capable of learning phoneme-like representations of speech in an unsupervised manner. We trained DNNs with unlabeled and labeled speech and analyzed the activations of each layer with respect to the phones in the input segments. The analyses reveal that the emergence of phonemic invariance in DNNs is dependent on the availability of phonemic labeling of the input during the training. No increased phonemic selectivity of the hidden layers was observed in the purely unsupervised networks despite successful learning of low-dimensional representations for speech. This suggests that additional learning constraints or more sophisticated models are needed to account for the emergence of phone-like categories in distributional learning operating on natural speech.", "Keywords": ["categorical perception", "connectionism", "distributional learning", "language acquisition", "phonemic categories", "speech perception", "statistical learning"], "MeSH terms": [], "Authors": [{"First Name": "Okko", "Last Name": "R\u00e4s\u00e4nen", "Affiliation": "Department of Signal Processing and Acoustics, Aalto University, PO Box 13000, Aalto 00076, Finland."}, {"First Name": "Tasha", "Last Name": "Nagamine", "Affiliation": "Department of Electrical Engineering, Columbia University, 500 W. 120 St., New York, NY 10027 USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University, 500 W. 120 St., New York, NY 10027 USA."}], "Journal": "CogSci ... Annual Conference of the Cognitive Science Society. Cognitive Science Society (U.S.). Conference", "PubDate": "2016Aug"}, {"PMID": "26865624", "Title": "Human Superior Temporal Gyrus Organization of Spectrotemporal Modulation Tuning Derived from Speech Stimuli.", "Abstract": "The human superior temporal gyrus (STG) is critical for speech perception, yet the organization of spectrotemporal processing of speech within the STG is not well understood. Here, to characterize the spatial organization of spectrotemporal processing of speech across human STG, we use high-density cortical surface field potential recordings while participants listened to natural continuous speech. While synthetic broad-band stimuli did not yield sustained activation of the STG, spectrotemporal receptive fields could be reconstructed from vigorous responses to speech stimuli. We find that the human STG displays a robust anterior-posterior spatial distribution of spectrotemporal tuning in which the posterior STG is tuned for temporally fast varying speech sounds that have relatively constant energy across the frequency axis (low spectral modulation) while the anterior STG is tuned for temporally slow varying speech sounds that have a high degree of spectral variation across the frequency axis (high spectral modulation). This work illustrates organization of spectrotemporal processing in the human STG, and illuminates processing of ethologically relevant speech signals in a region of the brain specialized for speech perception.", "Keywords": ["functional organization", "human STG", "human superior temporal gyrus", "modulation tuning", "modulotopic", "spectrotemporal processing"], "MeSH terms": ["Acoustic Stimulation", "Adult", "Algorithms", "Brain Mapping", "Energy Metabolism", "Evoked Potentials", "Female", "Humans", "Male", "Phonetics", "Speech Perception", "Temporal Lobe"], "Authors": [{"First Name": "Patrick W", "Last Name": "Hullett", "Affiliation": "University of California Berkeley and San Francisco Joint Graduate Group in Bioengineering, Center for Integrative Neuroscience, Department of Otolaryngology-Head and Neck Surgery, and."}, {"First Name": "Liberty S", "Last Name": "Hamilton", "Affiliation": "Center for Integrative Neuroscience, Department of Neurological Surgery, School of Medicine, University of California, San Francisco, San Francisco, California 94158."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Center for Integrative Neuroscience, Department of Neurological Surgery, School of Medicine, University of California, San Francisco, San Francisco, California 94158."}, {"First Name": "Christoph E", "Last Name": "Schreiner", "Affiliation": "University of California Berkeley and San Francisco Joint Graduate Group in Bioengineering, Center for Integrative Neuroscience, Department of Otolaryngology-Head and Neck Surgery, and."}, {"First Name": "Edward F", "Last Name": "Chang", "Affiliation": "University of California Berkeley and San Francisco Joint Graduate Group in Bioengineering, Center for Integrative Neuroscience, Department of Neurological Surgery, School of Medicine, University of California, San Francisco, San Francisco, California 94158 changed@neurosurg.ucsf.edu."}], "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience", "PubDate": "2016Feb10"}, {"PMID": "26528113", "Title": "Reconstruction of audio waveforms from spike trains of artificial cochlea models.", "Abstract": "Spiking cochlea models describe the analog processing and spike generation process within the biological cochlea. Reconstructing the audio input from the artificial cochlea spikes is therefore useful for understanding the fidelity of the information preserved in the spikes. The reconstruction process is challenging particularly for spikes from the mixed signal (analog/digital) integrated circuit (IC) cochleas because of multiple non-linearities in the model and the additional variance caused by random transistor mismatch. This work proposes an offline method for reconstructing the audio input from spike responses of both a particular spike-based hardware model called the AEREAR2 cochlea and an equivalent software cochlea model. This method was previously used to reconstruct the auditory stimulus based on the peri-stimulus histogram of spike responses recorded in the ferret auditory cortex. The reconstructed audio from the hardware cochlea is evaluated against an analogous software model using objective measures of speech quality and intelligibility; and further tested in a word recognition task. The reconstructed audio under low signal-to-noise (SNR) conditions (SNR < -5 dB) gives a better classification performance than the original SNR input in this word recognition task.", "Keywords": ["analog reconstruction", "cochlea model", "cochlea spikes", "digit recognition", "silicon cochlea", "spike reconstruction"], "MeSH terms": [], "Authors": [{"First Name": "Anja T", "Last Name": "Zai", "Affiliation": "Institute of Neuroinformatics, University of Zurich and ETH Zurich Zurich, Switzerland."}, {"First Name": "Saurabh", "Last Name": "Bhargava", "Affiliation": "Institute of Neuroinformatics, University of Zurich and ETH Zurich Zurich, Switzerland."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Electrical Engineering, Columbia University New York, NY, USA."}, {"First Name": "Shih-Chii", "Last Name": "Liu", "Affiliation": "Institute of Neuroinformatics, University of Zurich and ETH Zurich Zurich, Switzerland."}], "Journal": "Frontiers in neuroscience", "PubDate": "2015"}, {"PMID": "24753585", "Title": "Mechanisms of noise robust representation of speech in primary auditory cortex.", "Abstract": "Humans and animals can reliably perceive behaviorally relevant sounds in noisy and reverberant environments, yet the neural mechanisms behind this phenomenon are largely unknown. To understand how neural circuits represent degraded auditory stimuli with additive and reverberant distortions, we compared single-neuron responses in ferret primary auditory cortex to speech and vocalizations in four conditions: clean, additive white and pink (1/f) noise, and reverberation. Despite substantial distortion, responses of neurons to the vocalization signal remained stable, maintaining the same statistical distribution in all conditions. Stimulus spectrograms reconstructed from population responses to the distorted stimuli resembled more the original clean than the distorted signals. To explore mechanisms contributing to this robustness, we simulated neural responses using several spectrotemporal receptive field models that incorporated either a static nonlinearity or subtractive synaptic depression and multiplicative gain normalization. The static model failed to suppress the distortions. A dynamic model incorporating feed-forward synaptic depression could account for the reduction of additive noise, but only the combined model with feedback gain normalization was able to predict the effects across both additive and reverberant conditions. Thus, both mechanisms can contribute to the abilities of humans and animals to extract relevant sounds in diverse noisy environments.", "Keywords": ["cortical", "hearing", "phonemes", "population code"], "MeSH terms": ["Acoustic Stimulation", "Animals", "Auditory Cortex", "Female", "Ferrets", "Humans", "Models, Neurological", "Neurons", "Noise", "Nonlinear Dynamics", "Speech Perception", "Vocalization, Animal"], "Authors": [{"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Institute for Systems Research, University of Maryland, College Park, MD 20742."}, {"First Name": "Stephen V", "Last Name": "David", "Affiliation": "N/A"}, {"First Name": "Jonathan B", "Last Name": "Fritz", "Affiliation": "N/A"}, {"First Name": "Shihab A", "Last Name": "Shamma", "Affiliation": "N/A"}], "Journal": "Proceedings of the National Academy of Sciences of the United States of America", "PubDate": "2014May06"}, {"PMID": "24606276", "Title": "Auditory gist: recognition of very short sounds from timbre cues.", "Abstract": "Sounds such as the voice or musical instruments can be recognized on the basis of timbre alone. Here, sound recognition was investigated with severely reduced timbre cues. Short snippets of naturally recorded sounds were extracted from a large corpus. Listeners were asked to report a target category (e.g., sung voices) among other sounds (e.g., musical instruments). All sound categories covered the same pitch range, so the task had to be solved on timbre cues alone. The minimum duration for which performance was above chance was found to be short, on the order of a few milliseconds, with the best performance for voice targets. Performance was independent of pitch and was maintained when stimuli contained less than a full waveform cycle. Recognition was not generally better when the sound snippets were time-aligned with the sound onset compared to when they were extracted with a random starting time. Finally, performance did not depend on feedback or training, suggesting that the cues used by listeners in the artificial gating task were similar to those relevant for longer, more familiar sounds. The results show that timbre cues for sound recognition are available at a variety of time scales, including very short ones.", "Keywords": [], "MeSH terms": ["Acoustic Stimulation", "Adult", "Analysis of Variance", "Audiometry", "Cues", "Discrimination, Psychological", "Feedback, Psychological", "Female", "Humans", "Male", "Music", "Pitch Discrimination", "Recognition, Psychology", "Sensory Gating", "Signal Detection, Psychological", "Singing", "Sound Spectrography", "Time Factors", "Voice Quality", "Young Adult"], "Authors": [{"First Name": "Clara", "Last Name": "Suied", "Affiliation": "Institut de Recherche Biom\u00e9dicale des Arm\u00e9es, D\u00e9partement Action et Cognition en Situation Op\u00e9rationnelle, 91223 Br\u00e9tigny sur Orge, France."}, {"First Name": "Trevor R", "Last Name": "Agus", "Affiliation": "Sonic Arts Research Centre, School of Creative Arts, 1 Cloreen Park, Queen's University Belfast, Belfast, BT7 1NN, United Kingdom."}, {"First Name": "Simon J", "Last Name": "Thorpe", "Affiliation": "Centre de Recherche Cerveau et Cognition, UMR 5549, CNRS and Universit\u00e9 Paul Sabatier, Toulouse, France."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Departments of Neurological Surgery and Physiology, UCSF Center for Integrative Neuroscience, University of California, San Francisco, California 94143."}, {"First Name": "Daniel", "Last Name": "Pressnitzer", "Affiliation": "Laboratoire des Syst\u00e8mes Perceptifs, UMR 8248, CNRS and \u00c9cole normale sup\u00e9rieure, 29 rue d'Ulm, 75005 Paris, France."}], "Journal": "The Journal of the Acoustical Society of America", "PubDate": "2014Mar"}, {"PMID": "24482117", "Title": "Phonetic feature encoding in human superior temporal gyrus.", "Abstract": "During speech perception, linguistic elements such as consonants and vowels are extracted from a complex acoustic speech signal. The superior temporal gyrus (STG) participates in high-order auditory processing of speech, but how it encodes phonetic information is poorly understood. We used high-density direct cortical surface recordings in humans while they listened to natural, continuous speech to reveal the STG representation of the entire English phonetic inventory. At single electrodes, we found response selectivity to distinct phonetic features. Encoding of acoustic properties was mediated by a distributed population response. Phonetic features could be directly related to tuning for spectrotemporal acoustic cues, some of which were encoded in a nonlinear fashion or by integration of multiple cues. These findings demonstrate the acoustic-phonetic representation of speech in human STG.", "Keywords": [], "MeSH terms": ["Auditory Cortex", "Female", "Humans", "Magnetic Resonance Imaging", "Male", "Phonetics", "Speech Acoustics", "Speech Perception"], "Authors": [{"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Neurological Surgery, Department of Physiology, and Center for Integrative Neuroscience, University of California, San Francisco, CA 94143, USA."}, {"First Name": "Connie", "Last Name": "Cheung", "Affiliation": "N/A"}, {"First Name": "Keith", "Last Name": "Johnson", "Affiliation": "N/A"}, {"First Name": "Edward F", "Last Name": "Chang", "Affiliation": "N/A"}], "Journal": "Science (New York, N.Y.)", "PubDate": "2014Feb28"}, {"PMID": "24429136", "Title": "Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG.", "Abstract": "How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial (\u224860 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at \u223c200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain-computer interfaces.", "Keywords": ["BCI", "EEG", "attention", "cocktail party", "speech", "stimulus-reconstruction"], "MeSH terms": ["Acoustic Stimulation", "Adult", "Attention", "Brain", "Electroencephalography", "Female", "Humans", "Male", "Neuropsychological Tests", "Signal Processing, Computer-Assisted", "Speech Perception", "Time Factors"], "Authors": [{"First Name": "James A", "Last Name": "O'Sullivan", "Affiliation": "School of Engineering, Trinity Centre for Bioengineering and Trinity College Institute of Neuroscience, Trinity College Dublin, Dublin 2, Ireland."}, {"First Name": "Alan J", "Last Name": "Power", "Affiliation": "School of Engineering, Trinity Centre for Bioengineering and Trinity College Institute of Neuroscience, Trinity College Dublin, Dublin 2, Ireland Department of Psychology, Centre for Neuroscience in Education, University of Cambridge, Cambridge, UK."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "Department of Neurological Surgery Department of Physiology, UCSF Center for Integrative Neuroscience, University of California, San Francisco, CA 94143, USA."}, {"First Name": "Siddharth", "Last Name": "Rajaram", "Affiliation": "The Center for Computational Neuroscience and Neural Technology, Boston University, Boston, MA 02215, USA."}, {"First Name": "John J", "Last Name": "Foxe", "Affiliation": "The Sheryl and Daniel R. Tishman Cognitive Neurophysiology Laboratory, Children's Evaluation and Rehabilitation Center, Departments of Pediatrics and Neuroscience, Albert Einstein College of Medicine, Bronx, NY 10461, USA."}, {"First Name": "Barbara G", "Last Name": "Shinn-Cunningham", "Affiliation": "The Center for Computational Neuroscience and Neural Technology, Boston University, Boston, MA 02215, USA."}, {"First Name": "Malcolm", "Last Name": "Slaney", "Affiliation": "Microsoft Research, Mountain View, CA 94043, USA."}, {"First Name": "Shihab A", "Last Name": "Shamma", "Affiliation": "Institute for Systems Research, University of Maryland, College Park, MD 20742, USA."}, {"First Name": "Edmund C", "Last Name": "Lalor", "Affiliation": "School of Engineering, Trinity Centre for Bioengineering and Trinity College Institute of Neuroscience, Trinity College Dublin, Dublin 2, Ireland."}], "Journal": "Cerebral cortex (New York, N.Y. : 1991)", "PubDate": "2015Jul"}, {"PMID": "23426266", "Title": "Functional organization of human sensorimotor cortex for speech articulation.", "Abstract": "Speaking is one of the most complex actions that we perform, but nearly all of us learn to do it effortlessly. Production of fluent speech requires the precise, coordinated movement of multiple articulators (for example, the lips, jaw, tongue and larynx) over rapid time scales. Here we used high-resolution, multi-electrode cortical recordings during the production of consonant-vowel syllables to determine the organization of speech sensorimotor cortex in humans. We found speech-articulator representations that are arranged somatotopically on ventral pre- and post-central gyri, and that partially overlap at individual electrodes. These representations were coordinated temporally as sequences during syllable production. Spatial patterns of cortical activity showed an emergent, population-level representation, which was organized by phonetic features. Over tens of milliseconds, the spatial patterns transitioned between distinct representations for different consonants and vowels. These results reveal the dynamic organization of speech sensorimotor cortex during the generation of multi-articulator movements that underlies our ability to speak.", "Keywords": [], "MeSH terms": ["Cerebral Cortex", "Electromagnetic Phenomena", "Feedback, Sensory", "Humans", "Phonetics", "Principal Component Analysis", "Speech", "Time Factors"], "Authors": [{"First Name": "Kristofer E", "Last Name": "Bouchard", "Affiliation": "Department of Neurological Surgery, University of California, San Francisco, 505 Parnassus Avenue, San Francisco, California 94143, USA."}, {"First Name": "Nima", "Last Name": "Mesgarani", "Affiliation": "N/A"}, {"First Name": "Keith", "Last Name": "Johnson", "Affiliation": "N/A"}, {"First Name": "Edward F", "Last Name": "Chang", "Affiliation": "N/A"}], "Journal": "Nature", "PubDate": "2013Mar21"}]