[
    {
        "PMID": "38918605",
        "Title": "Lightning Pose: improved animal pose estimation via semi-supervised learning, Bayesian ensembling and cloud-native open-source tools.",
        "Abstract": "Contemporary pose estimation methods enable precise measurements of behavior via supervised deep learning with hand-labeled video frames. Although effective in many cases, the supervised approach requires extensive labeling and often produces outputs that are unreliable for downstream analyses. Here, we introduce 'Lightning Pose', an efficient pose estimation package with three algorithmic contributions. First, in addition to training on a few labeled video frames, we use many unlabeled videos and penalize the network whenever its predictions violate motion continuity, multiple-view geometry and posture plausibility (semi-supervised learning). Second, we introduce a network architecture that resolves occlusions by predicting pose on any given frame using surrounding unlabeled frames. Third, we refine the pose predictions post hoc by combining ensembling and Kalman smoothing. Together, these components render pose trajectories more accurate and scientifically usable. We released a cloud application that allows users to label data, train networks and process new videos directly from the browser.",
        "Keywords": [],
        "MeSH terms": [
            "Bayes Theorem",
            "Animals",
            "Video Recording",
            "Algorithms",
            "Supervised Machine Learning",
            "Cloud Computing",
            "Software",
            "Posture",
            "Deep Learning",
            "Image Processing, Computer-Assisted",
            "Behavior, Animal"
        ],
        "Authors": [
            {
                "First Name": "Dan",
                "Last Name": "Biderman",
                "Affiliation": "Columbia University, New York, NY, USA. db3236@cumc.columbia.edu."
            },
            {
                "First Name": "Matthew R",
                "Last Name": "Whiteway",
                "Affiliation": "Columbia University, New York, NY, USA. m.whiteway@columbia.edu."
            },
            {
                "First Name": "Cole",
                "Last Name": "Hurwitz",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Nicholas",
                "Last Name": "Greenspan",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Robert S",
                "Last Name": "Lee",
                "Affiliation": "Lightning.ai, New York, NY, USA."
            },
            {
                "First Name": "Ankit",
                "Last Name": "Vishnubhotla",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Richard",
                "Last Name": "Warren",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Federico",
                "Last Name": "Pedraja",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Dillon",
                "Last Name": "Noone",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Michael M",
                "Last Name": "Schartner",
                "Affiliation": "Champalimaud Centre for the Unknown, Lisbon, Portugal."
            },
            {
                "First Name": "Julia M",
                "Last Name": "Huntenburg",
                "Affiliation": "Max Planck Institute for Biological Cybernetics, TÃ¼bingen, Germany."
            },
            {
                "First Name": "Anup",
                "Last Name": "Khanal",
                "Affiliation": "University of California, Los Angeles, Los Angeles, CA, USA."
            },
            {
                "First Name": "Guido T",
                "Last Name": "Meijer",
                "Affiliation": "Champalimaud Centre for the Unknown, Lisbon, Portugal."
            },
            {
                "First Name": "Jean-Paul",
                "Last Name": "Noel",
                "Affiliation": "New York University, New York, NY, USA."
            },
            {
                "First Name": "Alejandro",
                "Last Name": "Pan-Vazquez",
                "Affiliation": "Princeton University, Princeton, NJ, USA."
            },
            {
                "First Name": "Karolina Z",
                "Last Name": "Socha",
                "Affiliation": "University College London, London, UK."
            },
            {
                "First Name": "Anne E",
                "Last Name": "Urai",
                "Affiliation": "Leiden University, Leiden, the Netherlands."
            },
            {
                "First Name": "",
                "Last Name": "",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Cunningham",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Nathaniel B",
                "Last Name": "Sawtell",
                "Affiliation": "Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Columbia University, New York, NY, USA."
            }
        ],
        "Journal": "Nature methods",
        "PubDate": "2024"
    },
    {
        "PMID": "38709828",
        "Title": "Removing direct photocurrent artifacts in optogenetic connectivity mapping data via constrained matrix factorization.",
        "Abstract": "Monosynaptic connectivity mapping is crucial for building circuit-level models of neural computation. Two-photon optogenetic stimulation, when combined with whole-cell recording, enables large-scale mapping of physiological circuit parameters. In this experimental setup, recorded postsynaptic currents are used to infer the presence and strength of connections. For many cell types, nearby connections are those we expect to be strongest. However, when the postsynaptic cell expresses opsin, optical excitation of nearby cells can induce direct photocurrents in the postsynaptic cell. These photocurrent artifacts contaminate synaptic currents, making it difficult or impossible to probe connectivity for nearby cells. To overcome this problem, we developed a computational tool, Photocurrent Removal with Constraints (PhoRC). Our method is based on a constrained matrix factorization model which leverages the fact that photocurrent kinetics are less variable than those of synaptic currents. We demonstrate on real and simulated data that PhoRC consistently removes photocurrents while preserving synaptic currents, despite variations in photocurrent kinetics across datasets. Our method allows the discovery of synaptic connections which would have been otherwise obscured by photocurrent artifacts, and may thus reveal a more complete picture of synaptic connectivity. PhoRC runs faster than real time and is available as open source software.",
        "Keywords": [],
        "MeSH terms": [
            "Optogenetics",
            "Artifacts",
            "Animals",
            "Models, Neurological",
            "Computational Biology",
            "Synapses",
            "Mice",
            "Neurons",
            "Software",
            "Computer Simulation",
            "Algorithms",
            "Patch-Clamp Techniques",
            "Humans"
        ],
        "Authors": [
            {
                "First Name": "Benjamin",
                "Last Name": "Antin",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Masato",
                "Last Name": "Sadahiro",
                "Affiliation": "Department of Molecular and Cell Biology, University of California, Berkeley, Berkeley, California, United States of America."
            },
            {
                "First Name": "Marta",
                "Last Name": "Gajowa",
                "Affiliation": "Department of Molecular and Cell Biology, University of California, Berkeley, Berkeley, California, United States of America."
            },
            {
                "First Name": "Marcus A",
                "Last Name": "Triplett",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Hillel",
                "Last Name": "Adesnik",
                "Affiliation": "Department of Molecular and Cell Biology, University of California, Berkeley, Berkeley, California, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2024"
    },
    {
        "PMID": "37790422",
        "Title": "Bypassing spike sorting: Density-based decoding using spike localization from dense multielectrode probes.",
        "Abstract": "Neural decoding and its applications to brain computer interfaces (BCI) are essential for understanding the association between neural activity and behavior. A prerequisite for many decoding approaches is ",
        "Keywords": [],
        "MeSH terms": [],
        "Authors": [
            {
                "First Name": "Yizi",
                "Last Name": "Zhang",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Tianxiao",
                "Last Name": "He",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Julien",
                "Last Name": "Boussard",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Charlie",
                "Last Name": "Windolf",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Olivier",
                "Last Name": "Winter",
                "Affiliation": "The International Brain Laboratory."
            },
            {
                "First Name": "Eric",
                "Last Name": "Trautmann",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Noam",
                "Last Name": "Roth",
                "Affiliation": "University of Washington."
            },
            {
                "First Name": "Hailey",
                "Last Name": "Barrell",
                "Affiliation": "University of Washington."
            },
            {
                "First Name": "Mark",
                "Last Name": "Churchland",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Nicholas A",
                "Last Name": "Steinmetz",
                "Affiliation": "University of Washington."
            },
            {
                "First Name": "",
                "Last Name": "",
                "Affiliation": ""
            },
            {
                "First Name": "Erdem",
                "Last Name": "Varol",
                "Affiliation": "New York University."
            },
            {
                "First Name": "Cole",
                "Last Name": "Hurwitz",
                "Affiliation": "Columbia University."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Columbia University."
            }
        ],
        "Journal": "bioRxiv : the preprint server for biology",
        "PubDate": "2023"
    },
    {
        "PMID": "37745388",
        "Title": "maskNMF: A denoise-sparsen-detect approach for extracting neural signals from dense imaging data.",
        "Abstract": "A number of calcium imaging methods have been developed to monitor the activity of large populations of neurons. One particularly promising approach, Bessel imaging, captures neural activity from a volume by projecting within the imaged volume onto a single imaging plane, therefore effectively mixing signals and increasing the number of neurons imaged per pixel. These signals must then be computationally demixed to recover the desired neural activity. Unfortunately, currently-available demixing methods can perform poorly in the regime of high imaging density (i.e., many neurons per pixel). In this work we introduce a new pipeline (maskNMF) for demixing dense calcium imaging data. The main idea is to first denoise and temporally sparsen the observed video; this enhances signal strength and reduces spatial overlap significantly. Next we detect neurons in the sparsened video using a neural network trained on a library of neural shapes. These shapes are derived from segmented electron microscopy images input into a Bessel imaging model; therefore no manual selection of \"good\" neural shapes from the functional data is required here. After cells are detected, we use a constrained non-negative matrix factorization approach to demix the activity, using the detected cells' shapes to initialize the factorization. We test the resulting pipeline on both simulated and real datasets and find that it is able to achieve accurate demixing on denser data than was previously feasible, therefore enabling faithful imaging of larger neural populations. The method also provides good results on more \"standard\" two-photon imaging data. Finally, because much of the pipeline operates on a significantly compressed version of the raw data and is highly parallelizable, the algorithm is fast, processing large datasets faster than real time.",
        "Keywords": [
            "Bessel imaging",
            "calcium imaging",
            "demixing",
            "open-source software"
        ],
        "MeSH terms": [],
        "Authors": [
            {
                "First Name": "Amol",
                "Last Name": "Pasarkar",
                "Affiliation": "Center for Theoretical Neuroscience and Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Ian",
                "Last Name": "Kinsella",
                "Affiliation": "Center for Theoretical Neuroscience and Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Pengcheng",
                "Last Name": "Zhou",
                "Affiliation": "Shenzhen Institute of Advanced Technology, Shenzhen, 518055, China."
            },
            {
                "First Name": "Melissa",
                "Last Name": "Wu",
                "Affiliation": "Department of Biomedical Engineering, Duke University, Durham, NC 27708."
            },
            {
                "First Name": "Daisong",
                "Last Name": "Pan",
                "Affiliation": "Department of Physics, University of California, Berkeley, California 94720, USA."
            },
            {
                "First Name": "Jiang Lan",
                "Last Name": "Fan",
                "Affiliation": "Joint Bioengineering Graduate Program, University of California, Berkeley, CA 94720."
            },
            {
                "First Name": "Zhen",
                "Last Name": "Wang",
                "Affiliation": "Department of Electrical and Computer Engineering, UCLA, Los Angeles, CA, 90095, USA."
            },
            {
                "First Name": "Lamiae",
                "Last Name": "Abdeladim",
                "Affiliation": "Department of Molecular and Cell Biology, University of California, Berkeley, Berkeley, CA 94720, USA."
            },
            {
                "First Name": "Darcy S",
                "Last Name": "Peterka",
                "Affiliation": "Center for Theoretical Neuroscience and Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Hillel",
                "Last Name": "Adesnik",
                "Affiliation": "Department of Molecular and Cell Biology, University of California, Berkeley, Berkeley, CA 94720, USA."
            },
            {
                "First Name": "Na",
                "Last Name": "Ji",
                "Affiliation": "Department of Physics, University of California, Berkeley, California 94720, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Center for Theoretical Neuroscience and Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA."
            }
        ],
        "Journal": "bioRxiv : the preprint server for biology",
        "PubDate": "2023"
    },
    {
        "PMID": "37292661",
        "Title": "Bayesian target optimisation for high-precision holographic optogenetics.",
        "Abstract": "Two-photon optogenetics has transformed our ability to probe the structure and function of neural circuits. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a novel computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from ",
        "Keywords": [],
        "MeSH terms": [],
        "Authors": [
            {
                "First Name": "Marcus A",
                "Last Name": "Triplett",
                "Affiliation": "Department of Statistics, Columbia University."
            },
            {
                "First Name": "Marta",
                "Last Name": "Gajowa",
                "Affiliation": "Department of Molecular and Cell Biology, UC Berkeley."
            },
            {
                "First Name": "Hillel",
                "Last Name": "Adesnik",
                "Affiliation": "Department of Molecular and Cell Biology, UC Berkeley."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Columbia University."
            }
        ],
        "Journal": "bioRxiv : the preprint server for biology",
        "PubDate": "2023"
    },
    {
        "PMID": "37162966",
        "Title": "Lightning Pose: improved animal pose estimation via semi-supervised learning, Bayesian ensembling, and cloud-native open-source tools.",
        "Abstract": "Contemporary pose estimation methods enable precise measurements of behavior via supervised deep learning with hand-labeled video frames. Although effective in many cases, the supervised approach requires extensive labeling and often produces outputs that are unreliable for downstream analyses. Here, we introduce \"Lightning Pose,\" an efficient pose estimation package with three algorithmic contributions. First, in addition to training on a few labeled video frames, we use many unlabeled videos and penalize the network whenever its predictions violate motion continuity, multiple-view geometry, and posture plausibility (semi-supervised learning). Second, we introduce a network architecture that resolves occlusions by predicting pose on any given frame using surrounding unlabeled frames. Third, we refine the pose predictions post-hoc by combining ensembling and Kalman smoothing. Together, these components render pose trajectories more accurate and scientifically usable. We release a cloud application that allows users to label data, train networks, and predict new videos directly from the browser.",
        "Keywords": [],
        "MeSH terms": [],
        "Authors": [
            {
                "First Name": "Dan",
                "Last Name": "Biderman",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Matthew R",
                "Last Name": "Whiteway",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Cole",
                "Last Name": "Hurwitz",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Nicholas",
                "Last Name": "Greenspan",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Robert S",
                "Last Name": "Lee",
                "Affiliation": "Work done while at Lightning.ai, New York, USA."
            },
            {
                "First Name": "Ankit",
                "Last Name": "Vishnubhotla",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Richard",
                "Last Name": "Warren",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Federico",
                "Last Name": "Pedraja",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Dillon",
                "Last Name": "Noone",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Michael",
                "Last Name": "Schartner",
                "Affiliation": "Champalimaud Centre for the Unknown, Lisbon, Portugal."
            },
            {
                "First Name": "Julia M",
                "Last Name": "Huntenburg",
                "Affiliation": "Max Planck Institute for Biological Cybernetics, TÃ¼bingen, Germany."
            },
            {
                "First Name": "Anup",
                "Last Name": "Khanal",
                "Affiliation": "University of California Los Angeles, Los Angeles, USA."
            },
            {
                "First Name": "Guido T",
                "Last Name": "Meijer",
                "Affiliation": "Champalimaud Centre for the Unknown, Lisbon, Portugal."
            },
            {
                "First Name": "Jean-Paul",
                "Last Name": "Noel",
                "Affiliation": "New York University, New York, USA."
            },
            {
                "First Name": "Alejandro",
                "Last Name": "Pan-Vazquez",
                "Affiliation": "Princeton University, Princeton, USA."
            },
            {
                "First Name": "Karolina Z",
                "Last Name": "Socha",
                "Affiliation": "University College London, London, United Kingdom."
            },
            {
                "First Name": "Anne E",
                "Last Name": "Urai",
                "Affiliation": "Leiden University, Leiden, The Netherlands."
            },
            {
                "First Name": "",
                "Last Name": "",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Cunningham",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Nathaniel B",
                "Last Name": "Sawtell",
                "Affiliation": "Columbia University, New York, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Columbia University, New York, USA."
            }
        ],
        "Journal": "bioRxiv : the preprint server for biology",
        "PubDate": "2024"
    },
    {
        "PMID": "35395020",
        "Title": "Blind demixing methods for recovering dense neuronal morphology from barcode imaging data.",
        "Abstract": "Cellular barcoding methods offer the exciting possibility of 'infinite-pseudocolor' anatomical reconstruction-i.e., assigning each neuron its own random unique barcoded 'pseudocolor,' and then using these pseudocolors to trace the microanatomy of each neuron. Here we use simulations, based on densely-reconstructed electron microscopy microanatomy, with signal structure matched to real barcoding data, to quantify the feasibility of this procedure. We develop a new blind demixing approach to recover the barcodes that label each neuron, and validate this method on real data with known barcodes. We also develop a neural network which uses the recovered barcodes to reconstruct the neuronal morphology from the observed fluorescence imaging data, 'connecting the dots' between discontiguous barcode amplicon signals. We find that accurate recovery should be feasible, provided that the barcode signal density is sufficiently high. This study suggests the possibility of mapping the morphology and projection pattern of many individual neurons simultaneously, at high resolution and at large scale, via conventional light microscopy.",
        "Keywords": [],
        "MeSH terms": [
            "DNA Barcoding, Taxonomic",
            "Neurons",
            "Optical Imaging"
        ],
        "Authors": [
            {
                "First Name": "Shuonan",
                "Last Name": "Chen",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Jackson",
                "Last Name": "Loper",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Pengcheng",
                "Last Name": "Zhou",
                "Affiliation": "Faculty of Life and Health Sciences, Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2022"
    },
    {
        "PMID": "34550974",
        "Title": "Partitioning variability in animal behavioral videos using semi-supervised variational autoencoders.",
        "Abstract": "Recent neuroscience studies demonstrate that a deeper understanding of brain function requires a deeper understanding of behavior. Detailed behavioral measurements are now often collected using video cameras, resulting in an increased need for computer vision algorithms that extract useful information from video data. Here we introduce a new video analysis tool that combines the output of supervised pose estimation algorithms (e.g. DeepLabCut) with unsupervised dimensionality reduction methods to produce interpretable, low-dimensional representations of behavioral videos that extract more information than pose estimates alone. We demonstrate this tool by extracting interpretable behavioral features from videos of three different head-fixed mouse preparations, as well as a freely moving mouse in an open field arena, and show how these interpretable features can facilitate downstream behavioral and neural analyses. We also show how the behavioral features produced by our model improve the precision and interpretation of these downstream analyses compared to using the outputs of either fully supervised or fully unsupervised methods alone.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Artificial Intelligence",
            "Behavior, Animal",
            "Computational Biology",
            "Computer Simulation",
            "Markov Chains",
            "Mice",
            "Models, Statistical",
            "Neural Networks, Computer",
            "Supervised Machine Learning",
            "Unsupervised Machine Learning",
            "Video Recording"
        ],
        "Authors": [
            {
                "First Name": "Matthew R",
                "Last Name": "Whiteway",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Dan",
                "Last Name": "Biderman",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Yoni",
                "Last Name": "Friedman",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Mario",
                "Last Name": "Dipoppa",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "E Kelly",
                "Last Name": "Buchanan",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Anqi",
                "Last Name": "Wu",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "John",
                "Last Name": "Zhou",
                "Affiliation": "Department of Computer Science, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "NiccolÃ²",
                "Last Name": "Bonacchi",
                "Affiliation": "Champalimaud Centre for the Unknown, Lisbon, Portugal."
            },
            {
                "First Name": "Nathaniel J",
                "Last Name": "Miska",
                "Affiliation": "Sainsbury-Wellcome Centre for Neural Circuits and Behavior, University College London, London, United Kingdom."
            },
            {
                "First Name": "Jean-Paul",
                "Last Name": "Noel",
                "Affiliation": "Center for Neural Science, New York University, New York, New York, United States of America."
            },
            {
                "First Name": "Erica",
                "Last Name": "Rodriguez",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Michael",
                "Last Name": "Schartner",
                "Affiliation": "Champalimaud Centre for the Unknown, Lisbon, Portugal."
            },
            {
                "First Name": "Karolina",
                "Last Name": "Socha",
                "Affiliation": "Institute of Ophthalmology, University College London, London, United Kingdom."
            },
            {
                "First Name": "Anne E",
                "Last Name": "Urai",
                "Affiliation": "Cognitive Psychology Unit, Leiden University, Leiden, The Netherlands."
            },
            {
                "First Name": "C Daniel",
                "Last Name": "Salzman",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "",
                "Last Name": "",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Cunningham",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2021"
    },
    {
        "PMID": "34411268",
        "Title": "Nonlinear Decoding of Natural Images From Large-Scale Primate Retinal Ganglion Recordings.",
        "Abstract": "Decoding sensory stimuli from neural activity can provide insight into how the nervous system might interpret the physical environment, and facilitates the development of brain-machine interfaces. Nevertheless, the neural decoding problem remains a significant open challenge. Here, we present an efficient nonlinear decoding approach for inferring natural scene stimuli from the spiking activities of retinal ganglion cells (RGCs). Our approach uses neural networks to improve on existing decoders in both accuracy and scalability. Trained and validated on real retinal spike data from more than 1000 simultaneously recorded macaque RGC units, the decoder demonstrates the necessity of nonlinear computations for accurate decoding of the fine structures of visual stimuli. Specifically, high-pass spatial features of natural images can only be decoded using nonlinear techniques, while low-pass features can be extracted equally well by linear and nonlinear methods. Together, these results advance the state of the art in decoding natural stimuli from large populations of neurons.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Brain-Computer Interfaces",
            "Macaca",
            "Neural Networks, Computer",
            "Retina",
            "Retinal Ganglion Cells"
        ],
        "Authors": [
            {
                "First Name": "Young Joon",
                "Last Name": "Kim",
                "Affiliation": "Columbia University, New York, NY 10027, U.S.A. yjkimnada@gmail.com."
            },
            {
                "First Name": "Nora",
                "Last Name": "Brackbill",
                "Affiliation": "Stanford University, Stanford, CA 94305, U.S.A. nbrack@stanford.edu."
            },
            {
                "First Name": "Eleanor",
                "Last Name": "Batty",
                "Affiliation": "Columbia University, New York, NY 10027, U.S.A. erb2180@columbia.edu."
            },
            {
                "First Name": "JinHyung",
                "Last Name": "Lee",
                "Affiliation": "Columbia University, New York, NY 10027, U.S.A. jl4303@columbia.edu."
            },
            {
                "First Name": "Catalin",
                "Last Name": "Mitelut",
                "Affiliation": "Columbia University, New York, NY 10027, U.S.A. mitelutco@gmail.com."
            },
            {
                "First Name": "William",
                "Last Name": "Tong",
                "Affiliation": "Columbia University, New York, NY 10027, U.S.A. wlt2115@columbia.edu."
            },
            {
                "First Name": "E J",
                "Last Name": "Chichilnisky",
                "Affiliation": "Stanford University, Stanford, CA U.S.A. ej@stanford.edu."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Columbia University, New York, NY 10027, U.S.A. liam@stat.columbia.edu."
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2021"
    },
    {
        "PMID": "33684106",
        "Title": "BARcode DEmixing through Non-negative Spatial Regression (BarDensr).",
        "Abstract": "Modern spatial transcriptomics methods can target thousands of different types of RNA transcripts in a single slice of tissue. Many biological applications demand a high spatial density of transcripts relative to the imaging resolution, leading to partial mixing of transcript rolonies in many voxels; unfortunately, current analysis methods do not perform robustly in this highly-mixed setting. Here we develop a new analysis approach, BARcode DEmixing through Non-negative Spatial Regression (BarDensr): we start with a generative model of the physical process that leads to the observed image data and then apply sparse convex optimization methods to estimate the underlying (demixed) rolony densities. We apply BarDensr to simulated and real data and find that it achieves state of the art signal recovery, particularly in densely-labeled regions or data with low spatial resolution. Finally, BarDensr is fast and parallelizable. We provide open-source code as well as an implementation for the 'NeuroCAAS' cloud platform.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Computer Simulation",
            "Spatial Regression",
            "Transcriptome"
        ],
        "Authors": [
            {
                "First Name": "Shuonan",
                "Last Name": "Chen",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Jackson",
                "Last Name": "Loper",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Xiaoyin",
                "Last Name": "Chen",
                "Affiliation": "Cold Spring Harbor Laboratory, Cold Spring Harbor, New York, United States of America."
            },
            {
                "First Name": "Alex",
                "Last Name": "Vaughan",
                "Affiliation": "Cold Spring Harbor Laboratory, Cold Spring Harbor, New York, United States of America."
            },
            {
                "First Name": "Anthony M",
                "Last Name": "Zador",
                "Affiliation": "Cold Spring Harbor Laboratory, Cold Spring Harbor, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2021"
    },
    {
        "PMID": "32282806",
        "Title": "Localized semi-nonnegative matrix factorization (LocaNMF) of widefield calcium imaging data.",
        "Abstract": "Widefield calcium imaging enables recording of large-scale neural activity across the mouse dorsal cortex. In order to examine the relationship of these neural signals to the resulting behavior, it is critical to demix the recordings into meaningful spatial and temporal components that can be mapped onto well-defined brain regions. However, no current tools satisfactorily extract the activity of the different brain regions in individual mice in a data-driven manner, while taking into account mouse-specific and preparation-specific differences. Here, we introduce Localized semi-Nonnegative Matrix Factorization (LocaNMF), a method that efficiently decomposes widefield video data and allows us to directly compare activity across multiple mice by outputting mouse-specific localized functional regions that are significantly more interpretable than more traditional decomposition techniques. Moreover, it provides a natural subspace to directly compare correlation maps and neural dynamics across different behaviors, mice, and experimental conditions, and enables identification of task- and movement-related brain regions.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Brain Mapping",
            "Calcium",
            "Image Processing, Computer-Assisted",
            "Mice",
            "Prefrontal Cortex"
        ],
        "Authors": [
            {
                "First Name": "Shreya",
                "Last Name": "Saxena",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Ian",
                "Last Name": "Kinsella",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Simon",
                "Last Name": "Musall",
                "Affiliation": "Cold Spring Harbor Laboratory, Cold Spring Harbor, New York, United States of America."
            },
            {
                "First Name": "Sharon H",
                "Last Name": "Kim",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Jozsef",
                "Last Name": "Meszaros",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "David N",
                "Last Name": "Thibodeaux",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Carla",
                "Last Name": "Kim",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "John",
                "Last Name": "Cunningham",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Elizabeth M C",
                "Last Name": "Hillman",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Anne",
                "Last Name": "Churchland",
                "Affiliation": "Cold Spring Harbor Laboratory, Cold Spring Harbor, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Mortimer B. Zuckerman Mind Brain Behavior Institute, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2020"
    },
    {
        "PMID": "30784583",
        "Title": "Reinforcement Learning Recruits Somata and Apical Dendrites across Layers of Primary Sensory Cortex.",
        "Abstract": "The mammalian brain can form associations between behaviorally relevant stimuli in an animal's environment. While such learning is thought to primarily involve high-order association cortex, even primary sensory areas receive long-range connections carrying information that could contribute to high-level representations. Here, we imaged layer 1 apical dendrites in the barrel cortex of mice performing a whisker-based operant behavior. In addition to sensory-motor events, calcium signals in apical dendrites of layers 2/3 and 5 neurons and in layer 2/3 somata track the delivery of rewards, both choice related and randomly administered. Reward-related tuft-wide dendritic spikes emerge gradually with training and are task specific. Learning recruits cells whose intrinsic activity coincides with the time of reinforcement. Layer 4 largely lacked reward-related signals, suggesting a source other than the primary thalamus. Our results demonstrate that a sensory cortex can acquire a set of associations outside its immediate sensory modality and linked to salient behavioral events.",
        "Keywords": [
            "GCaMP",
            "apical dendrites",
            "barrel cortex",
            "detection",
            "reward",
            "two-photon",
            "vibrissa",
            "voltage-gated calcium"
        ],
        "MeSH terms": [
            "Animals",
            "Calcium Signaling",
            "Dendrites",
            "Female",
            "Male",
            "Mice",
            "Mice, Inbred C57BL",
            "Reinforcement, Psychology",
            "Sensory Receptor Cells",
            "Somatosensory Cortex",
            "Vibrissae"
        ],
        "Authors": [
            {
                "First Name": "Clay O",
                "Last Name": "Lacefield",
                "Affiliation": "Department of Neuroscience, Mortimer Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Kavli Institute for Brain Science, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Eftychios A",
                "Last Name": "Pnevmatikakis",
                "Affiliation": "Department of Statistics, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Neuroscience, Mortimer Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Kavli Institute for Brain Science, Columbia University, New York, NY 10027, USA; Department of Statistics, Columbia University, New York, NY 10027, USA; Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Randy M",
                "Last Name": "Bruno",
                "Affiliation": "Department of Neuroscience, Mortimer Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10027, USA; Kavli Institute for Brain Science, Columbia University, New York, NY 10027, USA. Electronic address: randybruno@columbia.edu."
            }
        ],
        "Journal": "Cell reports",
        "PubDate": "2019"
    },
    {
        "PMID": "29469809",
        "Title": "Efficient and accurate extraction of in vivo calcium signals from microendoscopic video data.",
        "Abstract": "In vivo calcium imaging through microendoscopic lenses enables imaging of previously inaccessible neuronal populations deep within the brains of freely moving animals. However, it is computationally challenging to extract single-neuronal activity from microendoscopic data, because of the very large background fluctuations and high spatial overlaps intrinsic to this recording modality. Here, we describe a new constrained matrix factorization approach to accurately separate the background and then demix and denoise the neuronal signals of interest. We compared the proposed method against previous independent components analysis and constrained nonnegative matrix factorization approaches. On both simulated and experimental data recorded from mice, our method substantially improved the quality of extracted cellular signals and detected more well-isolated neural signals, especially in noisy data regimes. These advances can in turn significantly enhance the statistical power of downstream analyses, and ultimately improve scientific conclusions derived from microendoscopic data.",
        "Keywords": [
            "calcium imaging",
            "microendoscope",
            "mouse",
            "neuroscience",
            "source extraction"
        ],
        "MeSH terms": [
            "Animals",
            "Brain",
            "Calcium Signaling",
            "Endoscopy",
            "Image Processing, Computer-Assisted",
            "Mice",
            "Neurons",
            "Video Recording"
        ],
        "Authors": [
            {
                "First Name": "Pengcheng",
                "Last Name": "Zhou",
                "Affiliation": "Center for the Neural Basis of Cognition, Carnegie Mellon University, Pittsburgh, United States."
            },
            {
                "First Name": "Shanna L",
                "Last Name": "Resendez",
                "Affiliation": "Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, United States."
            },
            {
                "First Name": "Jose",
                "Last Name": "Rodriguez-Romaguera",
                "Affiliation": "Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, United States."
            },
            {
                "First Name": "Jessica C",
                "Last Name": "Jimenez",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, United States."
            },
            {
                "First Name": "Shay Q",
                "Last Name": "Neufeld",
                "Affiliation": "Department of Neurobiology, Harvard Medical School, Howard Hughes Medical Institute, Boston, United States."
            },
            {
                "First Name": "Andrea",
                "Last Name": "Giovannucci",
                "Affiliation": "Center for Computational Biology, Flatiron Institute, Simons Foundation, New York, United States."
            },
            {
                "First Name": "Johannes",
                "Last Name": "Friedrich",
                "Affiliation": "Center for Computational Biology, Flatiron Institute, Simons Foundation, New York, United States."
            },
            {
                "First Name": "Eftychios A",
                "Last Name": "Pnevmatikakis",
                "Affiliation": "Center for Computational Biology, Flatiron Institute, Simons Foundation, New York, United States."
            },
            {
                "First Name": "Garret D",
                "Last Name": "Stuber",
                "Affiliation": "Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, United States."
            },
            {
                "First Name": "Rene",
                "Last Name": "Hen",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, United States."
            },
            {
                "First Name": "Mazen A",
                "Last Name": "Kheirbek",
                "Affiliation": "Weill Institute for Neurosciences, University of California, San Francisco, San Francisco, United States."
            },
            {
                "First Name": "Bernardo L",
                "Last Name": "Sabatini",
                "Affiliation": "Department of Neurobiology, Harvard Medical School, Howard Hughes Medical Institute, Boston, United States."
            },
            {
                "First Name": "Robert E",
                "Last Name": "Kass",
                "Affiliation": "Center for the Neural Basis of Cognition, Carnegie Mellon University, Pittsburgh, United States."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Columbia University, New York, United States."
            }
        ],
        "Journal": "eLife",
        "PubDate": "2018"
    },
    {
        "PMID": "29131818",
        "Title": "Electrical stimulus artifact cancellation and neural spike detection on large multi-electrode arrays.",
        "Abstract": "Simultaneous electrical stimulation and recording using multi-electrode arrays can provide a valuable technique for studying circuit connectivity and engineering neural interfaces. However, interpreting these measurements is challenging because the spike sorting process (identifying and segregating action potentials arising from different neurons) is greatly complicated by electrical stimulation artifacts across the array, which can exhibit complex and nonlinear waveforms, and overlap temporarily with evoked spikes. Here we develop a scalable algorithm based on a structured Gaussian Process model to estimate the artifact and identify evoked spikes. The effectiveness of our methods is demonstrated in both real and simulated 512-electrode recordings in the peripheral primate retina with single-electrode and several types of multi-electrode stimulation. We establish small error rates in the identification of evoked spikes, with a computational complexity that is compatible with real-time data analysis. This technology may be helpful in the design of future high-resolution sensory prostheses based on tailored stimulation (e.g., retinal prostheses), and for closed-loop neural stimulation at a much larger scale than currently possible.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Artifacts",
            "Electric Stimulation",
            "Electrodes",
            "Humans",
            "Models, Statistical",
            "Primates",
            "Retinal Neurons",
            "Signal-To-Noise Ratio"
        ],
        "Authors": [
            {
                "First Name": "Gonzalo E",
                "Last Name": "Mena",
                "Affiliation": "Statistics Department, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Lauren E",
                "Last Name": "Grosberg",
                "Affiliation": "Department of Neurosurgery and Hansen Experimental Physics Laboratory, Stanford University, Stanford, California, United States of America."
            },
            {
                "First Name": "Sasidhar",
                "Last Name": "Madugula",
                "Affiliation": "Department of Neurosurgery and Hansen Experimental Physics Laboratory, Stanford University, Stanford, California, United States of America."
            },
            {
                "First Name": "PaweÅ",
                "Last Name": "Hottowy",
                "Affiliation": "Physics and Applied Computer Science, AGH University of Science and Technology, Krakow, Poland."
            },
            {
                "First Name": "Alan",
                "Last Name": "Litke",
                "Affiliation": "Santa Cruz Institute for Particle Physics, University of California, Santa Cruz, Santa Cruz, California, United States of America."
            },
            {
                "First Name": "John",
                "Last Name": "Cunningham",
                "Affiliation": "Statistics Department, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "E J",
                "Last Name": "Chichilnisky",
                "Affiliation": "Department of Neurosurgery and Hansen Experimental Physics Laboratory, Stanford University, Stanford, California, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Statistics Department, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2017"
    },
    {
        "PMID": "28771570",
        "Title": "Multi-scale approaches for high-speed imaging and analysis of large neural populations.",
        "Abstract": "Progress in modern neuroscience critically depends on our ability to observe the activity of large neuronal populations with cellular spatial and high temporal resolution. However, two bottlenecks constrain efforts towards fast imaging of large populations. First, the resulting large video data is challenging to analyze. Second, there is an explicit tradeoff between imaging speed, signal-to-noise, and field of view: with current recording technology we cannot image very large neuronal populations with simultaneously high spatial and temporal resolution. Here we describe multi-scale approaches for alleviating both of these bottlenecks. First, we show that spatial and temporal decimation techniques based on simple local averaging provide order-of-magnitude speedups in spatiotemporally demixing calcium video data into estimates of single-cell neural activity. Second, once the shapes of individual neurons have been identified at fine scale (e.g., after an initial phase of conventional imaging with standard temporal and spatial resolution), we find that the spatial/temporal resolution tradeoff shifts dramatically: after demixing we can accurately recover denoised fluorescence traces and deconvolved neural activity of each individual neuron from coarse scale data that has been spatially decimated by an order of magnitude. This offers a cheap method for compressing this large video data, and also implies that it is possible to either speed up imaging significantly, or to \"zoom out\" by a corresponding factor to image order-of-magnitude larger neuronal populations with minimal loss in accuracy or temporal resolution.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Brain",
            "Computational Biology",
            "Image Processing, Computer-Assisted",
            "Mice",
            "Neurons",
            "Neurophysiology",
            "Zebrafish"
        ],
        "Authors": [
            {
                "First Name": "Johannes",
                "Last Name": "Friedrich",
                "Affiliation": "Department of Statistics, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Weijian",
                "Last Name": "Yang",
                "Affiliation": "NeuroTechnology Center, Department of Biological Sciences, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Daniel",
                "Last Name": "Soudry",
                "Affiliation": "Department of Statistics, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Yu",
                "Last Name": "Mu",
                "Affiliation": "Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, Virginia, United States of America."
            },
            {
                "First Name": "Misha B",
                "Last Name": "Ahrens",
                "Affiliation": "Janelia Research Campus, Howard Hughes Medical Institute, Ashburn, Virginia, United States of America."
            },
            {
                "First Name": "Rafael",
                "Last Name": "Yuste",
                "Affiliation": "NeuroTechnology Center, Department of Biological Sciences, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Darcy S",
                "Last Name": "Peterka",
                "Affiliation": "NeuroTechnology Center, Department of Biological Sciences, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2017"
    },
    {
        "PMID": "28291787",
        "Title": "Fast online deconvolution of calcium imaging data.",
        "Abstract": "Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations, but extracting the activity of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse non-negative deconvolution problem. Importantly, the algorithm 3progresses through each time series sequentially from beginning to end, thus enabling real-time online estimation of neural activity during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed: more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Unlike these approaches, our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. A minor modification can further improve the quality of activity inference by imposing a constraint on the minimum spike size. The algorithm enables real-time simultaneous deconvolution of O(105) traces of whole-brain larval zebrafish imaging data on a laptop.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Calcium",
            "Calcium Signaling",
            "Data Interpretation, Statistical",
            "Humans",
            "Image Interpretation, Computer-Assisted",
            "Microscopy, Fluorescence",
            "Molecular Imaging",
            "Neurons",
            "Reproducibility of Results",
            "Sensitivity and Specificity",
            "Voltage-Sensitive Dye Imaging"
        ],
        "Authors": [
            {
                "First Name": "Johannes",
                "Last Name": "Friedrich",
                "Affiliation": "Department of Statistics, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Pengcheng",
                "Last Name": "Zhou",
                "Affiliation": "Department of Statistics, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Grossman Center for the Statistics of Mind, and Center for Theoretical Neuroscience, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2017"
    },
    {
        "PMID": "27208694",
        "Title": "Bayesian methods for event analysis of intracellular currents.",
        "Abstract": "Investigation of neural circuit functioning often requires statistical interpretation of events in subthreshold electrophysiological recordings. This problem is non-trivial because recordings may have moderate levels of structured noise and events may have distinct kinetics. In addition, novel experimental designs that combine optical and electrophysiological methods will depend upon statistical tools that combine multimodal data.",
        "Keywords": [
            "Bayesian methods",
            "Calcium imaging",
            "Connectivity mapping",
            "Event detection",
            "MCMC",
            "Postsynaptic current"
        ],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Automation, Laboratory",
            "Bayes Theorem",
            "Calcium",
            "Computer Simulation",
            "Intracellular Space",
            "Mice, Transgenic",
            "Optogenetics",
            "Patch-Clamp Techniques",
            "Pattern Recognition, Automated",
            "Synapses",
            "Synaptic Potentials",
            "Tissue Culture Techniques",
            "Voltage-Sensitive Dye Imaging"
        ],
        "Authors": [
            {
                "First Name": "Josh",
                "Last Name": "Merel",
                "Affiliation": "Neurobiology and Behavior Program, Columbia University, United States; Center for Theoretical Neuroscience, Columbia University, United States. Electronic address: jsmerel@gmail.com."
            },
            {
                "First Name": "Ben",
                "Last Name": "Shababo",
                "Affiliation": "Helen Wills Neuroscience Institute, University of California, Berkeley, United States."
            },
            {
                "First Name": "Alex",
                "Last Name": "Naka",
                "Affiliation": "Helen Wills Neuroscience Institute, University of California, Berkeley, United States."
            },
            {
                "First Name": "Hillel",
                "Last Name": "Adesnik",
                "Affiliation": "Helen Wills Neuroscience Institute, University of California, Berkeley, United States; Department of Molecular and Cellular Biology, University of California, Berkeley, United States."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Neurobiology and Behavior Program, Columbia University, United States; Center for Theoretical Neuroscience, Columbia University, United States; Department of Statistics, Columbia University, United States; Grossman Center for the Statistics of Mind, Columbia University, United States."
            }
        ],
        "Journal": "Journal of neuroscience methods",
        "PubDate": "2016"
    },
    {
        "PMID": "27191387",
        "Title": "Neuroprosthetic Decoder Training as Imitation Learning.",
        "Abstract": "Neuroprosthetic brain-computer interfaces function via an algorithm which decodes neural activity of the user into movements of an end effector, such as a cursor or robotic arm. In practice, the decoder is often learned by updating its parameters while the user performs a task. When the user's intention is not directly observable, recent methods have demonstrated value in training the decoder against a surrogate for the user's intended movement. Here we show that training a decoder in this way is a novel variant of an imitation learning problem, where an oracle or expert is employed for supervised training in lieu of direct observations, which are not available. Specifically, we describe how a generic imitation learning meta-algorithm, dataset aggregation (DAgger), can be adapted to train a generic brain-computer interface. By deriving existing learning algorithms for brain-computer interfaces in this framework, we provide a novel analysis of regret (an important metric of learning efficacy) for brain-computer interfaces. This analysis allows us to characterize the space of algorithmic variants and bounds on their regret rates. Existing approaches for decoder learning have been performed in the cursor control setting, but the available design principles for these decoders are such that it has been impossible to scale them to naturalistic settings. Leveraging our findings, we then offer an algorithm that combines imitation learning with optimal control, which should allow for training of arbitrary effectors for which optimal control can generate goal-oriented control. We demonstrate this novel and general BCI algorithm with simulated neuroprosthetic control of a 26 degree-of-freedom model of an arm, a sophisticated and realistic end effector.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Arm",
            "Brain-Computer Interfaces",
            "Computational Biology",
            "Computer Simulation",
            "Humans",
            "Learning",
            "Robotics",
            "Supervised Machine Learning",
            "Task Performance and Analysis"
        ],
        "Authors": [
            {
                "First Name": "Josh",
                "Last Name": "Merel",
                "Affiliation": "Neurobiology and Behavior program, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "David",
                "Last Name": "Carlson",
                "Affiliation": "Department of Statistics, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Neurobiology and Behavior program, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "John P",
                "Last Name": "Cunningham",
                "Affiliation": "Neurobiology and Behavior program, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2016"
    },
    {
        "PMID": "26949187",
        "Title": "Bayesian Sparse Regression Analysis Documents the Diversity of Spinal Inhibitory Interneurons.",
        "Abstract": "Documenting the extent of cellular diversity is a critical step in defining the functional organization of tissues and organs. To infer cell-type diversity from partial or incomplete transcription factor expression data, we devised a sparse Bayesian framework that is able to handle estimation uncertainty and can incorporate diverse cellular characteristics to optimize experimental design. Focusing on spinal V1 inhibitory interneurons, for which the spatial expression of 19 transcription factors has been mapped, we infer the existence of ~50 candidate V1 neuronal types, many of which localize in compact spatial domains in the ventral spinal cord. We have validated the existence of inferred cell types by direct experimental measurement, establishing this Bayesian framework as an effective platform for cell-type characterization in the nervous system and elsewhere.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Bayes Theorem",
            "Mice",
            "Renshaw Cells",
            "Spinal Cord",
            "Transcription Factors",
            "Transcriptome"
        ],
        "Authors": [
            {
                "First Name": "Mariano I",
                "Last Name": "Gabitto",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, NY 10032, USA; Department of Biochemistry and Molecular Biophysics, Howard Hughes Medical Institute, Kavli Institute for Brain Science, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA. Electronic address: mig2118@columbia.edu."
            },
            {
                "First Name": "Ari",
                "Last Name": "Pakman",
                "Affiliation": "Department of Statistics and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Jay B",
                "Last Name": "Bikoff",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, NY 10032, USA; Department of Biochemistry and Molecular Biophysics, Howard Hughes Medical Institute, Kavli Institute for Brain Science, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "L F",
                "Last Name": "Abbott",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, NY 10032, USA; Department of Physiology and Cellular Biophysics, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Thomas M",
                "Last Name": "Jessell",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, NY 10032, USA; Department of Biochemistry and Molecular Biophysics, Howard Hughes Medical Institute, Kavli Institute for Brain Science, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, NY 10032, USA; Department of Statistics and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA. Electronic address: liam@stat.columbia.edu."
            }
        ],
        "Journal": "Cell",
        "PubDate": "2016"
    },
    {
        "PMID": "26774160",
        "Title": "Simultaneous Denoising, Deconvolution, and Demixing of Calcium Imaging Data.",
        "Abstract": "We present a modular approach for analyzing calcium imaging recordings of large neuronal ensembles. Our goal is to simultaneously identify the locations of the neurons, demix spatially overlapping components, and denoise and deconvolve the spiking activity from the slow dynamics of the calcium indicator. Our approach relies on a constrained nonnegative matrix factorization that expresses the spatiotemporal fluorescence activity as the product of a spatial matrix that encodes the spatial footprint of each neuron in the optical field and a temporal matrix that characterizes the calcium concentration of each neuron over time. This framework is combined with a novel constrained deconvolution approach that extracts estimates of neural activity from fluorescence traces, to create a spatiotemporal processing algorithm that requires minimal parameter tuning. We demonstrate the general applicability of our method by applying it to inÂ vitro and inÂ vivo multi-neuronal imaging data, whole-brain light-sheet imaging data, and dendritic imaging data.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Calcium",
            "Dendrites",
            "Fluorescent Dyes",
            "Mice",
            "Mice, Inbred C57BL",
            "Microscopy, Fluorescence",
            "Neurons",
            "Statistics as Topic"
        ],
        "Authors": [
            {
                "First Name": "Eftychios A",
                "Last Name": "Pnevmatikakis",
                "Affiliation": "Center for Computational Biology, Simons Foundation, New York, NY 10010, USA; Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA. Electronic address: epnevmatikakis@simonsfoundation.org."
            },
            {
                "First Name": "Daniel",
                "Last Name": "Soudry",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Yuanjun",
                "Last Name": "Gao",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Timothy A",
                "Last Name": "Machado",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA; Department of Biochemistry and Molecular Biophysics and Howard Hughes Medical Institute, Columbia University, New York, NY 10032, USA; Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA; Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Josh",
                "Last Name": "Merel",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA; Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "David",
                "Last Name": "Pfau",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA; Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Thomas",
                "Last Name": "Reardon",
                "Affiliation": "Department of Biochemistry and Molecular Biophysics and Howard Hughes Medical Institute, Columbia University, New York, NY 10032, USA; Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA; Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Yu",
                "Last Name": "Mu",
                "Affiliation": "Howard Hughes Medical Institute, Janelia Research Campus, Ashburn, VA 20147, USA."
            },
            {
                "First Name": "Clay",
                "Last Name": "Lacefield",
                "Affiliation": "Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Weijian",
                "Last Name": "Yang",
                "Affiliation": "Neurotechnology Center, Department of Biological Sciences, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Misha",
                "Last Name": "Ahrens",
                "Affiliation": "Howard Hughes Medical Institute, Janelia Research Campus, Ashburn, VA 20147, USA."
            },
            {
                "First Name": "Randy",
                "Last Name": "Bruno",
                "Affiliation": "Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Thomas M",
                "Last Name": "Jessell",
                "Affiliation": "Department of Biochemistry and Molecular Biophysics and Howard Hughes Medical Institute, Columbia University, New York, NY 10032, USA; Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA; Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA."
            },
            {
                "First Name": "Darcy S",
                "Last Name": "Peterka",
                "Affiliation": "Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA; Neurotechnology Center, Department of Biological Sciences, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Rafael",
                "Last Name": "Yuste",
                "Affiliation": "Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA; Neurotechnology Center, Department of Biological Sciences, Columbia University, New York, NY 10027, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience, and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, USA; Department of Neuroscience and Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA; Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY 10032, USA; Neurotechnology Center, Department of Biological Sciences, Columbia University, New York, NY 10027, USA. Electronic address: liam@stat.columbia.edu."
            }
        ],
        "Journal": "Neuron",
        "PubDate": "2016"
    },
    {
        "PMID": "26465147",
        "Title": "Efficient \"Shotgun\" Inference of Neural Connectivity from Highly Sub-sampled Activity Data.",
        "Abstract": "Inferring connectivity in neuronal networks remains a key challenge in statistical neuroscience. The \"common input\" problem presents a major roadblock: it is difficult to reliably distinguish causal connections between pairs of observed neurons versus correlations induced by common input from unobserved neurons. Available techniques allow us to simultaneously record, with sufficient temporal resolution, only a small fraction of the network. Consequently, naive connectivity estimators that neglect these common input effects are highly biased. This work proposes a \"shotgun\" experimental design, in which we observe multiple sub-networks briefly, in a serial manner. Thus, while the full network cannot be observed simultaneously at any given time, we may be able to observe much larger subsets of the network over the course of the entire experiment, thus ameliorating the common input problem. Using a generalized linear model for a spiking recurrent neural network, we develop a scalable approximate expected loglikelihood-based Bayesian method to perform network inference given this type of data, in which only a small fraction of the network is observed in each time bin. We demonstrate in simulation that the shotgun experimental design can eliminate the biases induced by common input effects. Networks with thousands of neurons, in which only a small fraction of the neurons is observed in each time bin, can be quickly and accurately estimated, achieving orders of magnitude speed up over previous approaches.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Computer Simulation",
            "Connectome",
            "Data Interpretation, Statistical",
            "Humans",
            "Models, Neurological",
            "Models, Statistical",
            "Nerve Net",
            "Neurons",
            "Sample Size",
            "Synaptic Transmission"
        ],
        "Authors": [
            {
                "First Name": "Daniel",
                "Last Name": "Soudry",
                "Affiliation": "Department of Statistics, Department of Neuroscience, the Center for Theoretical Neuroscience, the Grossman Center for the Statistics of Mind, the Kavli Institute for Brain Science, and the NeuroTechnology Center, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Suraj",
                "Last Name": "Keshri",
                "Affiliation": "Department of Industrial Engineering and Operations Research, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Patrick",
                "Last Name": "Stinson",
                "Affiliation": "Department of Statistics, Department of Neuroscience, the Center for Theoretical Neuroscience, the Grossman Center for the Statistics of Mind, the Kavli Institute for Brain Science, and the NeuroTechnology Center, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Min-Hwan",
                "Last Name": "Oh",
                "Affiliation": "Department of Industrial Engineering and Operations Research, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Garud",
                "Last Name": "Iyengar",
                "Affiliation": "Department of Industrial Engineering and Operations Research, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Department of Neuroscience, the Center for Theoretical Neuroscience, the Grossman Center for the Statistics of Mind, the Kavli Institute for Brain Science, and the NeuroTechnology Center, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2015"
    },
    {
        "PMID": "26186188",
        "Title": "Primacy of Flexor Locomotor Pattern Revealed by Ancestral Reversion of Motor Neuron Identity.",
        "Abstract": "Spinal circuits can generate locomotor output in the absence of sensory or descending input, but the principles of locomotor circuit organization remain unclear. We sought insight into these principles by considering the elaboration of locomotor circuits across evolution. The identity of limb-innervating motor neurons was reverted to a state resembling that of motor neurons that direct undulatory swimming in primitive aquatic vertebrates, permitting assessment of the role of motor neuron identity in determining locomotor pattern. Two-photon imaging was coupled with spike inference to measure locomotor firing in hundreds of motor neurons in isolated mouse spinal cords. In wild-type preparations, we observed sequential recruitment of motor neurons innervating flexor muscles controlling progressively more distal joints. Strikingly, after reversion of motor neuron identity, virtually all firing patterns became distinctly flexor like. Our findings show that motor neuron identity directs locomotor circuit wiring and indicate the evolutionary primacy of flexor pattern generation.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Biological Evolution",
            "Extremities",
            "In Vitro Techniques",
            "Locomotion",
            "Mice",
            "Motor Neurons",
            "Muscle, Skeletal",
            "Spinal Cord"
        ],
        "Authors": [
            {
                "First Name": "Timothy A",
                "Last Name": "Machado",
                "Affiliation": "Departments of Neuroscience and Biochemistry and Molecular Biophysics, Howard Hughes Medical Institute, Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA; Department of Statistics, Center for Theoretical Neuroscience and Grossman Center for the Statistics of Mind, Columbia University, NewÂ York, NY 10027, USA. Electronic address: tam2138@columbia.edu."
            },
            {
                "First Name": "Eftychios",
                "Last Name": "Pnevmatikakis",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience and Grossman Center for the Statistics of Mind, Columbia University, NewÂ York, NY 10027, USA; Simons Center for Data Analysis, Simons Foundation, New York, NY 10010, USA."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Center for Theoretical Neuroscience and Grossman Center for the Statistics of Mind, Columbia University, NewÂ York, NY 10027, USA."
            },
            {
                "First Name": "Thomas M",
                "Last Name": "Jessell",
                "Affiliation": "Departments of Neuroscience and Biochemistry and Molecular Biophysics, Howard Hughes Medical Institute, Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA. Electronic address: tmj1@columbia.edu."
            },
            {
                "First Name": "Andrew",
                "Last Name": "Miri",
                "Affiliation": "Departments of Neuroscience and Biochemistry and Molecular Biophysics, Howard Hughes Medical Institute, Kavli Institute of Brain Science, Columbia University, New York, NY 10032, USA."
            }
        ],
        "Journal": "Cell",
        "PubDate": "2015"
    },
    {
        "PMID": "26029919",
        "Title": "Encoder-decoder optimization for brain-computer interfaces.",
        "Abstract": "Neuroprosthetic brain-computer interfaces are systems that decode neural activity into useful control signals for effectors, such as a cursor on a computer screen. It has long been recognized that both the user and decoding system can adapt to increase the accuracy of the end effector. Co-adaptation is the process whereby a user learns to control the system in conjunction with the decoder adapting to learn the user's neural patterns. We provide a mathematical framework for co-adaptation and relate co-adaptation to the joint optimization of the user's control scheme (\"encoding model\") and the decoding algorithm's parameters. When the assumptions of that framework are respected, co-adaptation cannot yield better performance than that obtainable by an optimal initial choice of fixed decoder, coupled with optimal user learning. For a specific case, we provide numerical methods to obtain such an optimized decoder. We demonstrate our approach in a model brain-computer interface system using an online prosthesis simulator, a simple human-in-the-loop pyschophysics setup which provides a non-invasive simulation of the BCI setting. These experiments support two claims: that users can learn encoders matched to fixed, optimal decoders and that, once learned, our approach yields expected performance advantages.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Brain-Computer Interfaces",
            "Computational Biology",
            "Computer Simulation",
            "Humans",
            "Models, Neurological",
            "Neural Prostheses",
            "Psychophysics",
            "Signal Processing, Computer-Assisted"
        ],
        "Authors": [
            {
                "First Name": "Josh",
                "Last Name": "Merel",
                "Affiliation": "Neurobiology and Behavior Program, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Donald M",
                "Last Name": "Pianto",
                "Affiliation": "Statistics Department, Columbia University, New York, New York, United States of America; Statistics Department, University of BrasÃ­lia, BrasÃ­lia, Distrito Federal, Brazil."
            },
            {
                "First Name": "John P",
                "Last Name": "Cunningham",
                "Affiliation": "Statistics Department, Columbia University, New York, New York, United States of America."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Statistics Department, Columbia University, New York, New York, United States of America."
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2015"
    },
    {
        "PMID": "25248082",
        "Title": "On quadrature methods for refractory point process likelihoods.",
        "Abstract": "Parametric models of the conditional intensity of a point process (e.g., generalized linear models) are popular in statistical neuroscience, as they allow us to characterize the variability in neural responses in terms of stimuli and spiking history. Parameter estimation in these models relies heavily on accurate evaluations of the log likelihood and its derivatives. Classical approaches use a discretized time version of the spiking process, and recent work has exploited the existence of a refractory period (during which the conditional intensity is zero following a spike) to obtain more accurate estimates of the likelihood. In this brief letter, we demonstrate that this method can be improved significantly by applying classical quadrature methods directly to the resulting continuous-time integral.",
        "Keywords": [],
        "MeSH terms": [],
        "Authors": [
            {
                "First Name": "Gonzalo",
                "Last Name": "Mena",
                "Affiliation": "Department of Statistics and Grossman Center for the Statistics of Mind, Columbia University, New York, NY 10027, U.S.A. gem2131@columbia.edu."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2014"
    },
    {
        "PMID": "24077932",
        "Title": "Fast state-space methods for inferring dendritic synaptic connectivity.",
        "Abstract": "We present fast methods for filtering voltage measurements and performing optimal inference of the location and strength of synaptic connections in large dendritic trees. Given noisy, subsampled voltage observations we develop fast l1-penalized regression methods for Kalman state-space models of the neuron voltage dynamics. The value of the l1-penalty parameter is chosen using cross-validation or, for low signal-to-noise ratio, a Mallows' Cp-like criterion. Using low-rank approximations, we reduce the inference runtime from cubic to linear in the number of dendritic compartments. We also present an alternative, fully Bayesian approach to the inference problem using a spike-and-slab prior. We illustrate our results with simulations on toy and real neuronal geometries. We consider observation schemes that either scan the dendritic geometry uniformly or measure linear combinations of voltages across several locations with random coefficients. For the latter, we show how to choose the coefficients to offset the correlation between successive measurements imposed by the neuron dynamics. This results in a \"compressed sensing\" observation scheme, with an important reduction in the number of measurements required to infer the synaptic weights.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Computer Simulation",
            "Dendrites",
            "Models, Neurological",
            "Neurons",
            "Synapses"
        ],
        "Authors": [
            {
                "First Name": "Ari",
                "Last Name": "Pakman",
                "Affiliation": ""
            },
            {
                "First Name": "Jonathan",
                "Last Name": "Huggins",
                "Affiliation": ""
            },
            {
                "First Name": "Carl",
                "Last Name": "Smith",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2014"
    },
    {
        "PMID": "23832289",
        "Title": "Fast inference in generalized linear models via expected log-likelihoods.",
        "Abstract": "Generalized linear models play an essential role in a wide variety of statistical applications. This paper discusses an approximation of the likelihood in these models that can greatly facilitate computation. The basic idea is to replace a sum that appears in the exact log-likelihood by an expectation over the model covariates; the resulting \"expected log-likelihood\" can in many cases be computed significantly faster than the exact log-likelihood. In many neuroscience experiments the distribution over model covariates is controlled by the experimenter and the expected log-likelihood approximation becomes particularly useful; for example, estimators based on maximizing this expected log-likelihood (or a penalized version thereof) can often be obtained with orders of magnitude computational savings compared to the exact maximum likelihood estimators. A risk analysis establishes that these maximum EL estimators often come with little cost in accuracy (and in some cases even improved accuracy) compared to standard maximum likelihood estimates. Finally, we find that these methods can significantly decrease the computation time of marginal likelihood calculations for model selection and of Markov chain Monte Carlo methods for sampling from the posterior parameter distribution. We illustrate our results by applying these methods to a computationally-challenging dataset of neural spike trains obtained via large-scale multi-electrode recordings in the primate retina.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Computer Simulation",
            "Humans",
            "Likelihood Functions",
            "Linear Models",
            "Models, Neurological",
            "Neurons"
        ],
        "Authors": [
            {
                "First Name": "Alexandro D",
                "Last Name": "Ramirez",
                "Affiliation": "Weill Cornell Medical College, New York, NY, USA, adr2110@gmail.com."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2014"
    },
    {
        "PMID": "23742213",
        "Title": "Computing loss of efficiency in optimal Bayesian decoders given noisy or incomplete spike trains.",
        "Abstract": "We investigate Bayesian methods for optimal decoding of noisy or incompletely-observed spike trains. Information about neural identity or temporal resolution may be lost during spike detection and sorting, or spike times measured near the soma may be corrupted with noise due to stochastic membrane channel effects in the axon. We focus on neural encoding models in which the (discrete) neural state evolves according to stimulus-dependent Markovian dynamics. Such models are sufficiently flexible that we may incorporate realistic stimulus encoding and spiking dynamics, but nonetheless permit exact computation via efficient hidden Markov model forward-backward methods. We analyze two types of signal degradation. First, we quantify the information lost due to jitter or downsampling in the spike-times. Second, we quantify the information lost when knowledge of the identities of different spiking neurons is corrupted. In each case the methods introduced here make it possible to quantify the dependence of the information loss on biophysical parameters such as firing rate, spike jitter amplitude, spike observation noise, etc. In particular, decoders that model the probability distribution of spike-neuron assignments significantly outperform decoders that use only the most likely spike assignments, and are ignorant of the posterior spike assignment uncertainty.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Bayes Theorem",
            "Computer Simulation",
            "Models, Neurological",
            "Nerve Net",
            "Neural Networks, Computer"
        ],
        "Authors": [
            {
                "First Name": "Carl",
                "Last Name": "Smith",
                "Affiliation": "Department of Chemistry, Columbia University, New York, NY 10027, USA. cas2207@columbia.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Network (Bristol, England)",
        "PubDate": "2013"
    },
    {
        "PMID": "22787437",
        "Title": "Fast spatiotemporal smoothing of calcium measurements in dendritic trees.",
        "Abstract": "We discuss methods for fast spatiotemporal smoothing of calcium signals in dendritic trees, given single-trial, spatially localized imaging data obtained via multi-photon microscopy. By analyzing the dynamics of calcium binding to probe molecules and the effects of the imaging procedure, we show that calcium concentration can be estimated up to an affine transformation, i.e., an additive and multiplicative constant. To obtain a full spatiotemporal estimate, we model calcium dynamics within the cell using a functional approach. The evolution of calcium concentration is represented through a smaller set of hidden variables that incorporate fast transients due to backpropagating action potentials (bAPs), or other forms of stimulation. Because of the resulting state space structure, inference can be done in linear time using forward-backward maximum-a-posteriori methods. Non-negativity constraints on the calcium concentration can also be incorporated using a log-barrier method that does not affect the computational scaling. Moreover, by exploiting the neuronal tree structure we show that the cost of the algorithm is also linear in the size of the dendritic tree, making the approach applicable to arbitrarily large trees. We apply this algorithm to data obtained from hippocampal CA1 pyramidal cells with experimentally evoked bAPs, some of which were paired with excitatory postsynaptic potentials (EPSPs). The algorithm recovers the timing of the bAPs and provides an estimate of the induced calcium transient throughout the tree. The proposed methods could be used to further understand the interplay between bAPs and EPSPs in synaptic strength modification. More generally, this approach allows us to infer the concentration on intracellular calcium across the dendritic tree from noisy observations at a discrete set of points in space.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Calcium",
            "Calcium Signaling",
            "Computational Biology",
            "Dendrites",
            "Hippocampus",
            "Microscopy, Fluorescence, Multiphoton",
            "Models, Neurological",
            "Rats",
            "Reproducibility of Results",
            "Signal Processing, Computer-Assisted"
        ],
        "Authors": [
            {
                "First Name": "Eftychios A",
                "Last Name": "Pnevmatikakis",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, New York, USA. eftychios@stat.columbia.edu"
            },
            {
                "First Name": "Keith",
                "Last Name": "Kelleher",
                "Affiliation": ""
            },
            {
                "First Name": "Rebecca",
                "Last Name": "Chen",
                "Affiliation": ""
            },
            {
                "First Name": "Petter",
                "Last Name": "Saggau",
                "Affiliation": ""
            },
            {
                "First Name": "KreÅ¡imir",
                "Last Name": "JosiÄ",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2012"
    },
    {
        "PMID": "22437567",
        "Title": "A Bayesian compressed-sensing approach for reconstructing neural connectivity from subsampled anatomical data.",
        "Abstract": "In recent years, the problem of reconstructing the connectivity in large neural circuits (\"connectomics\") has re-emerged as one of the main objectives of neuroscience. Classically, reconstructions of neural connectivity have been approached anatomically, using electron or light microscopy and histological tracing methods. This paper describes a statistical approach for connectivity reconstruction that relies on relatively easy-to-obtain measurements using fluorescent probes such as synaptic markers, cytoplasmic dyes, transsynaptic tracers, or activity-dependent dyes. We describe the possible design of these experiments and develop a Bayesian framework for extracting synaptic neural connectivity from such data. We show that the statistical reconstruction problem can be formulated naturally as a tractable Lâ-regularized quadratic optimization. As a concrete example, we consider a realistic hypothetical connectivity reconstruction experiment in C. elegans, a popular neuroscience model where a complete wiring diagram has been previously obtained based on long-term electron microscopy work. We show that the new statistical approach could lead to an orders of magnitude reduction in experimental effort in reconstructing the connectivity in this circuit. We further demonstrate that the spatial heterogeneity and biological variability in the connectivity matrix--not just the \"average\" connectivity--can also be estimated using the same method.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Animals, Genetically Modified",
            "Astrocytes",
            "Bayes Theorem",
            "Caenorhabditis elegans",
            "Connectome",
            "Green Fluorescent Proteins",
            "Image Processing, Computer-Assisted",
            "Luminescent Agents",
            "Microscopy, Fluorescence",
            "Models, Neurological",
            "Nerve Net",
            "Neurons",
            "Synapses"
        ],
        "Authors": [
            {
                "First Name": "Yuriy",
                "Last Name": "Mishchenko",
                "Affiliation": "Department of Engineering, Toros University, Bahcelievler Campus, 1857 St No 12, Yenisehir 33140, Mersin, Turkey. yuriy.mishchenko@gmail.com"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2012"
    },
    {
        "PMID": "22203465",
        "Title": "Modeling the impact of common noise inputs on the network activity of retinal ganglion cells.",
        "Abstract": "Synchronized spontaneous firing among retinal ganglion cells (RGCs), on timescales faster than visual responses, has been reported in many studies. Two candidate mechanisms of synchronized firing include direct coupling and shared noisy inputs. In neighboring parasol cells of primate retina, which exhibit rapid synchronized firing that has been studied extensively, recent experimental work indicates that direct electrical or synaptic coupling is weak, but shared synaptic input in the absence of modulated stimuli is strong. However, previous modeling efforts have not accounted for this aspect of firing in the parasol cell population. Here we develop a new model that incorporates the effects of common noise, and apply it to analyze the light responses and synchronized firing of a large, densely-sampled network of over 250 simultaneously recorded parasol cells. We use a generalized linear model in which the spike rate in each cell is determined by the linear combination of the spatio-temporally filtered visual input, the temporally filtered prior spikes of that cell, and unobserved sources representing common noise. The model accurately captures the statistical structure of the spike trains and the encoding of the visual stimulus, without the direct coupling assumption present in previous modeling work. Finally, we examined the problem of decoding the visual stimulus from the spike train given the estimated parameters. The common-noise model produces Bayesian decoding performance as accurate as that of a model with direct coupling, but with significantly more robustness to spike timing perturbations.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Computer Simulation",
            "In Vitro Techniques",
            "Macaca mulatta",
            "Models, Neurological",
            "Nerve Net",
            "Photic Stimulation",
            "Retina",
            "Retinal Ganglion Cells",
            "Visual Pathways"
        ],
        "Authors": [
            {
                "First Name": "Michael",
                "Last Name": "Vidne",
                "Affiliation": "Department of Applied Physics & Applied Mathematics, Center for Theoretical Neuroscience, Columbia University, New York, NY, USA. mv333@columbia.edu"
            },
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": ""
            },
            {
                "First Name": "Jonathon",
                "Last Name": "Shlens",
                "Affiliation": ""
            },
            {
                "First Name": "Jonathan W",
                "Last Name": "Pillow",
                "Affiliation": ""
            },
            {
                "First Name": "Jayant",
                "Last Name": "Kulkarni",
                "Affiliation": ""
            },
            {
                "First Name": "Alan M",
                "Last Name": "Litke",
                "Affiliation": ""
            },
            {
                "First Name": "E J",
                "Last Name": "Chichilnisky",
                "Affiliation": ""
            },
            {
                "First Name": "Eero",
                "Last Name": "Simoncelli",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2012"
    },
    {
        "PMID": "22089473",
        "Title": "Inferring synaptic inputs given a noisy voltage trace via sequential Monte Carlo methods.",
        "Abstract": "We discuss methods for optimally inferring the synaptic inputs to an electrotonically compact neuron, given intracellular voltage-clamp or current-clamp recordings from the postsynaptic cell. These methods are based on sequential Monte Carlo techniques (\"particle filtering\"). We demonstrate, on model data, that these methods can recover the time course of excitatory and inhibitory synaptic inputs accurately on a single trial. Depending on the observation noise level, no averaging over multiple trials may be required. However, excitatory inputs are consistently inferred more accurately than inhibitory inputs at physiological resting potentials, due to the stronger driving force associated with excitatory conductances. Once these synaptic input time courses are recovered, it becomes possible to fit (via tractable convex optimization techniques) models describing the relationship between the sensory stimulus and the observed synaptic input. We develop both parametric and nonparametric expectation-maximization (EM) algorithms that consist of alternating iterations between these synaptic recovery and model estimation steps. We employ a fast, robust convex optimization-based method to effectively initialize the filter; these fast methods may be of independent interest. The proposed methods could be applied to better understand the balance between excitation and inhibition in sensory processing in vivo.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Biophysics",
            "Electric Stimulation",
            "Membrane Potentials",
            "Models, Neurological",
            "Monte Carlo Method",
            "Neurons",
            "Patch-Clamp Techniques",
            "Stochastic Processes",
            "Synapses"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York City, NY, USA. liam@stat.columbia.edu"
            },
            {
                "First Name": "Michael",
                "Last Name": "Vidne",
                "Affiliation": ""
            },
            {
                "First Name": "Brian",
                "Last Name": "DePasquale",
                "Affiliation": ""
            },
            {
                "First Name": "Daniel Gil",
                "Last Name": "Ferreira",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2012"
    },
    {
        "PMID": "21861199",
        "Title": "Optimal experimental design for sampling voltage on dendritic trees in the low-SNR regime.",
        "Abstract": "Due to the limitations of current voltage sensing techniques, optimal filtering of noisy, undersampled voltage signals on dendritic trees is a key problem in computational cellular neuroscience. These limitations lead to voltage data that is incomplete (in the sense of only capturing a small portion of the full spatiotemporal signal) and often highly noisy. In this paper we use a Kalman filtering framework to develop optimal experimental design methods for voltage sampling. Our approach is to use a simple greedy algorithm with lazy evaluation to minimize the expected square error of the estimated spatiotemporal voltage signal. We take advantage of some particular features of the dendritic filtering problem to efficiently calculate the Kalman estimator's covariance. We test our framework with simulations of real dendritic branching structures and compare the quality of both time-invariant and time-varying sampling schemes. While the benefit of using the experimental design methods was modest in the time-invariant case, improvements of 25-100% over more naÃ¯ve methods were found when the observation locations were allowed to change with time. We also present a heuristic approximation to the greedy algorithm that is an order of magnitude faster while still providing comparable results.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Computer Simulation",
            "Dendrites",
            "Humans",
            "Models, Neurological",
            "Neurons",
            "Normal Distribution",
            "Research Design"
        ],
        "Authors": [
            {
                "First Name": "Jonathan Hunter",
                "Last Name": "Huggins",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, 1255 Amsterdam Ave., New York, NY 10027, USA. jhh2143@columbia.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2012"
    },
    {
        "PMID": "21813691",
        "Title": "Temporal precision in the visual pathway through the interplay of excitation and stimulus-driven suppression.",
        "Abstract": "Visual neurons can respond with extremely precise temporal patterning to visual stimuli that change on much slower time scales. Here, we investigate how the precise timing of cat thalamic spike trains-which can have timing as precise as 1 ms-is related to the stimulus, in the context of both artificial noise and natural visual stimuli. Using a nonlinear modeling framework applied to extracellular data, we demonstrate that the precise timing of thalamic spike trains can be explained by the interplay between an excitatory input and a delayed suppressive input that resembles inhibition, such that neuronal responses only occur in brief windows where excitation exceeds suppression. The resulting description of thalamic computation resembles earlier models of contrast adaptation, suggesting a more general role for mechanisms of contrast adaptation in visual processing. Thus, we describe a more complex computation underlying thalamic responses to artificial and natural stimuli that has implications for understanding how visual information is represented in the early stages of visual processing.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Adaptation, Physiological",
            "Animals",
            "Cats",
            "Female",
            "Geniculate Bodies",
            "Linear Models",
            "Male",
            "Models, Neurological",
            "Nerve Net",
            "Neurons",
            "Nonlinear Dynamics",
            "Paralysis",
            "Photic Stimulation",
            "Reproducibility of Results",
            "Time Perception",
            "Visual Cortex",
            "Visual Fields",
            "Visual Pathways"
        ],
        "Authors": [
            {
                "First Name": "Daniel A",
                "Last Name": "Butts",
                "Affiliation": "Department of Biology, University of Maryland, College Park, Maryland 20742, USA. dab@umd.edu"
            },
            {
                "First Name": "Chong",
                "Last Name": "Weng",
                "Affiliation": ""
            },
            {
                "First Name": "Jianzhong",
                "Last Name": "Jin",
                "Affiliation": ""
            },
            {
                "First Name": "Jose-Manuel",
                "Last Name": "Alonso",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience",
        "PubDate": "2011"
    },
    {
        "PMID": "21807854",
        "Title": "Imaging action potentials with calcium indicators.",
        "Abstract": null,
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Calcium",
            "Cytological Techniques",
            "Indicators and Reagents",
            "Neurons"
        ],
        "Authors": [
            {
                "First Name": "Rafael",
                "Last Name": "Yuste",
                "Affiliation": ""
            },
            {
                "First Name": "Jason",
                "Last Name": "MacLean",
                "Affiliation": ""
            },
            {
                "First Name": "Joshua",
                "Last Name": "Vogelstein",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Cold Spring Harbor protocols",
        "PubDate": "2011"
    },
    {
        "PMID": "21659018",
        "Title": "EMG prediction from motor cortical recordings via a nonnegative point-process filter.",
        "Abstract": "A constrained point-process filtering mechanism for prediction of electromyogram (EMG) signals from multichannel neural spike recordings is proposed here. Filters from the Kalman family are inherently suboptimal in dealing with non-Gaussian observations, or a state evolution that deviates from the Gaussianity assumption. To address these limitations, we modeled the non-Gaussian neural spike train observations by using a generalized linear model that encapsulates covariates of neural activity, including the neurons' own spiking history, concurrent ensemble activity, and extrinsic covariates (EMG signals). In order to predict the envelopes of EMGs, we reformulated the Kalman filter in an optimization framework and utilized a nonnegativity constraint. This structure characterizes the nonlinear correspondence between neural activity and EMG signals reasonably. The EMGs were recorded from 12 forearm and hand muscles of a behaving monkey during a grip-force task. In the case of limited training data, the constrained point-process filter improved the prediction accuracy when compared to a conventional Wiener cascade filter (a linear causal filter followed by a static nonlinearity) for different bin sizes and delays between input spikes and EMG output. For longer training datasets, results of the proposed filter and that of the Wiener cascade filter were comparable.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Databases, Factual",
            "Electrodes, Implanted",
            "Electromyography",
            "Forearm",
            "Hand",
            "Linear Models",
            "Macaca mulatta",
            "Man-Machine Systems",
            "Motor Cortex",
            "Signal Processing, Computer-Assisted"
        ],
        "Authors": [
            {
                "First Name": "Kianoush",
                "Last Name": "Nazarpour",
                "Affiliation": "University of Birmingham, Birmingham, B15 2TT, U.K. k.nazarpour@ncl.ac.uk"
            },
            {
                "First Name": "Christian",
                "Last Name": "Ethier",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "James M",
                "Last Name": "Rebesco",
                "Affiliation": ""
            },
            {
                "First Name": "R Chris",
                "Last Name": "Miall",
                "Affiliation": ""
            },
            {
                "First Name": "Lee E",
                "Last Name": "Miller",
                "Affiliation": ""
            }
        ],
        "Journal": "IEEE transactions on bio-medical engineering",
        "PubDate": "2012"
    },
    {
        "PMID": "21511704",
        "Title": "Designing optimal stimuli to control neuronal spike timing.",
        "Abstract": "Recent advances in experimental stimulation methods have raised the following important computational question: how can we choose a stimulus that will drive a neuron to output a target spike train with optimal precision, given physiological constraints? Here we adopt an approach based on models that describe how a stimulating agent (such as an injected electrical current or a laser light interacting with caged neurotransmitters or photosensitive ion channels) affects the spiking activity of neurons. Based on these models, we solve the reverse problem of finding the best time-dependent modulation of the input, subject to hardware limitations as well as physiologically inspired safety measures, that causes the neuron to emit a spike train that with highest probability will be close to a target spike train. We adopt fast convex constrained optimization methods to solve this problem. Our methods can potentially be implemented in real time and may also be generalized to the case of many cells, suitable for neural prosthesis applications. With the use of biologically sensible parameters and constraints, our method finds stimulation patterns that generate very precise spike trains in simulated experiments. We also tested the intracellular current injection method on pyramidal cells in mouse cortical slices, quantifying the dependence of spiking reliability and timing precision on constraints imposed on the applied currents.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Models, Neurological",
            "Neural Networks, Computer",
            "Neurons",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY 10032, USA. yashar@stat.columbia.edu"
            },
            {
                "First Name": "Adam M",
                "Last Name": "Packer",
                "Affiliation": ""
            },
            {
                "First Name": "Rafael",
                "Last Name": "Yuste",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neurophysiology",
        "PubDate": "2011"
    },
    {
        "PMID": "21389238",
        "Title": "Incorporating naturalistic correlation structure improves spectrogram reconstruction from neuronal activity in the songbird auditory midbrain.",
        "Abstract": "Birdsong is comprised of rich spectral and temporal organization, which might be used for vocal perception. To quantify how this structure could be used, we have reconstructed birdsong spectrograms by combining the spike trains of zebra finch auditory midbrain neurons with information about the correlations present in song. We calculated maximum a posteriori estimates of song spectrograms using a generalized linear model of neuronal responses and a series of prior distributions, each carrying different amounts of statistical information about zebra finch song. We found that spike trains from a population of mesencephalicus lateral dorsalis (MLd) neurons combined with an uncorrelated Gaussian prior can estimate the amplitude envelope of song spectrograms. The same set of responses can be combined with Gaussian priors that have correlations matched to those found across multiple zebra finch songs to yield song spectrograms similar to those presented to the animal. The fidelity of spectrogram reconstructions from MLd responses relies more heavily on prior knowledge of spectral correlations than temporal correlations. However, the best reconstructions combine MLd responses with both spectral and temporal correlations.",
        "Keywords": [],
        "MeSH terms": [
            "Acoustic Stimulation",
            "Action Potentials",
            "Animals",
            "Electrophysiology",
            "Finches",
            "Mesencephalon",
            "Neurons",
            "Signal Processing, Computer-Assisted",
            "Sound Spectrography",
            "Vocalization, Animal"
        ],
        "Authors": [
            {
                "First Name": "Alexandro D",
                "Last Name": "Ramirez",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, New York 10027, USA. adr2110@gmail.com"
            },
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": ""
            },
            {
                "First Name": "Joseph",
                "Last Name": "Schumacher",
                "Affiliation": ""
            },
            {
                "First Name": "David",
                "Last Name": "Schneider",
                "Affiliation": ""
            },
            {
                "First Name": "Sarah M N",
                "Last Name": "Woolley",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience",
        "PubDate": "2011"
    },
    {
        "PMID": "21299424",
        "Title": "Hidden Markov models for the stimulus-response relationships of multistate neural systems.",
        "Abstract": "Given recent experimental results suggesting that neural circuits may evolve through multiple firing states, we develop a framework for estimating state-dependent neural response properties from spike train data. We modify the traditional hidden Markov model (HMM) framework to incorporate stimulus-driven, non-Poisson point-process observations. For maximal flexibility, we allow external, time-varying stimuli and the neurons' own spike histories to drive both the spiking behavior in each state and the transitioning behavior between states. We employ an appropriately modified expectation-maximization algorithm to estimate the model parameters. The expectation step is solved by the standard forward-backward algorithm for HMMs. The maximization step reduces to a set of separable concave optimization problems if the model is restricted slightly. We first test our algorithm on simulated data and are able to fully recover the parameters used to generate the data and accurately recapitulate the sequence of hidden states. We then apply our algorithm to a recently published data set in which the observed neuronal ensembles displayed multistate behavior and show that inclusion of spike history information significantly improves the fit of the model. Additionally, we show that a simple reformulation of the state space of the underlying Markov chain allows us to implement a hybrid half-multistate, half-histogram model that may be more appropriate for capturing the complexity of certain data sets than either a simple HMM or a simple peristimulus time histogram model alone.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Artificial Intelligence",
            "Computer Simulation",
            "Humans",
            "Markov Chains",
            "Models, Theoretical",
            "Neural Networks, Computer",
            "Neurons"
        ],
        "Authors": [
            {
                "First Name": "Sean",
                "Last Name": "Escola",
                "Affiliation": "Center for Theoretical Neuroscience and Department of Psychiatry, Columbia University, New York, NY 10032, USA. sean@neurotheory.columbia.edu"
            },
            {
                "First Name": "Alfredo",
                "Last Name": "Fontanini",
                "Affiliation": ""
            },
            {
                "First Name": "Don",
                "Last Name": "Katz",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2011"
    },
    {
        "PMID": "21182868",
        "Title": "Kalman filter mixture model for spike sorting of non-stationary data.",
        "Abstract": "Nonstationarity in extracellular recordings can present a major problem during in vivo experiments. In this paper we present automatic methods for tracking time-varying spike shapes. Our algorithm is based on a computationally efficient Kalman filter model; the recursive nature of this model allows for on-line implementation of the method. The model parameters can be estimated using a standard expectation-maximization approach. In addition, refractory effects may be incorporated via closely related hidden Markov model techniques. We present an analysis of the algorithm's performance on both simulated and real data.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Computer Simulation",
            "Electronic Data Processing",
            "Markov Chains",
            "Models, Neurological",
            "Neurons",
            "Normal Distribution",
            "Online Systems",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Ana",
                "Last Name": "Calabrese",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, 1051 Riverside Dr., New York, NY 10032, United States. amc2257@columbia.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neuroscience methods",
        "PubDate": "2011"
    },
    {
        "PMID": "21138363",
        "Title": "Efficient, adaptive estimation of two-dimensional firing rate surfaces via Gaussian process methods.",
        "Abstract": "Estimating two-dimensional firing rate maps is a common problem, arising in a number of contexts: the estimation of place fields in hippocampus, the analysis of temporally nonstationary tuning curves in sensory and motor areas, the estimation of firing rates following spike-triggered covariance analyses, etc. Here we introduce methods based on Gaussian process nonparametric Bayesian techniques for estimating these two-dimensional rate maps. These techniques offer a number of advantages: the estimates may be computed efficiently, come equipped with natural errorbars, adapt their smoothness automatically to the local density and informativeness of the observed data, and permit direct fitting of the model hyperparameters (e.g., the prior smoothness of the rate map) via maximum marginal likelihood. We illustrate the method's flexibility and performance on a variety of simulated and real data.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Bayes Theorem",
            "Brain",
            "Computer Simulation",
            "Humans",
            "Models, Neurological",
            "Neurons",
            "Nonlinear Dynamics",
            "Normal Distribution"
        ],
        "Authors": [
            {
                "First Name": "Kamiar Rahnama",
                "Last Name": "Rad",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, USA. kamiar@stat.columbia.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Network (Bristol, England)",
        "PubDate": "2010"
    },
    {
        "PMID": "20964539",
        "Title": "Efficient Markov chain Monte Carlo methods for decoding neural spike trains.",
        "Abstract": "Stimulus reconstruction or decoding methods provide an important tool for understanding how sensory and motor information is represented in neural activity. We discuss Bayesian decoding methods based on an encoding generalized linear model (GLM) that accurately describes how stimuli are transformed into the spike trains of a group of neurons. The form of the GLM likelihood ensures that the posterior distribution over the stimuli that caused an observed set of spike trains is log concave so long as the prior is. This allows the maximum a posteriori (MAP) stimulus estimate to be obtained using efficient optimization algorithms. Unfortunately, the MAP estimate can have a relatively large average error when the posterior is highly nongaussian. Here we compare several Markov chain Monte Carlo (MCMC) algorithms that allow for the calculation of general Bayesian estimators involving posterior expectations (conditional on model parameters). An efficient version of the hybrid Monte Carlo (HMC) algorithm was significantly superior to other MCMC methods for gaussian priors. When the prior distribution has sharp edges and corners, on the other hand, the \"hit-and-run\" algorithm performed better than other MCMC methods. Using these algorithms, we show that for this latter class of priors, the posterior mean estimate can have a considerably lower average error than MAP, whereas for gaussian priors, the two estimators have roughly equal efficiency. We also address the application of MCMC methods for extracting nonmarginal properties of the posterior distribution. For example, by using MCMC to calculate the mutual information between the stimulus and response, we verify the validity of a computationally efficient Laplace approximation to this quantity for gaussian priors in a wide range of model parameters; this makes direct model-based computation of the mutual information tractable even in the case of large observed neural populations, where methods based on binning the spike train fail. Finally, we consider the effect of uncertainty in the GLM parameters on the posterior estimators.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Bayes Theorem",
            "Humans",
            "Markov Chains",
            "Monte Carlo Method",
            "Nerve Net",
            "Neural Networks, Computer",
            "Neurons",
            "Retinal Ganglion Cells",
            "Signal Processing, Computer-Assisted"
        ],
        "Authors": [
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, New York 10027, USA. yashar@stat.columbia.edu"
            },
            {
                "First Name": "Jonathan W",
                "Last Name": "Pillow",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2011"
    },
    {
        "PMID": "20964538",
        "Title": "Model-based decoding, information estimation, and change-point detection techniques for multineuron spike trains.",
        "Abstract": "One of the central problems in systems neuroscience is to understand how neural spike trains convey sensory information. Decoding methods, which provide an explicit means for reading out the information contained in neural spike responses, offer a powerful set of tools for studying the neural coding problem. Here we develop several decoding methods based on point-process neural encoding models, or forward models that predict spike responses to stimuli. These models have concave log-likelihood functions, which allow efficient maximum-likelihood model fitting and stimulus decoding. We present several applications of the encoding model framework to the problem of decoding stimulus information from population spike responses: (1) a tractable algorithm for computing the maximum a posteriori (MAP) estimate of the stimulus, the most probable stimulus to have generated an observed single- or multiple-neuron spike train response, given some prior distribution over the stimulus; (2) a gaussian approximation to the posterior stimulus distribution that can be used to quantify the fidelity with which various stimulus features are encoded; (3) an efficient method for estimating the mutual information between the stimulus and the spike trains emitted by a neural population; and (4) a framework for the detection of change-point times (the time at which the stimulus undergoes a change in mean or variance) by marginalizing over the posterior stimulus distribution. We provide several examples illustrating the performance of these estimators with simulated and real neural data.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Humans",
            "Models, Neurological",
            "Nerve Net",
            "Sensory Receptor Cells",
            "Signal Processing, Computer-Assisted"
        ],
        "Authors": [
            {
                "First Name": "Jonathan W",
                "Last Name": "Pillow",
                "Affiliation": "Center for Perceptual Systems, University of Texas at Austin, Austin, TX 78751, USA. pillow@mail.utexas.edu"
            },
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2011"
    },
    {
        "PMID": "20884487",
        "Title": "A generalized linear model of the impact of direct and indirect inputs to the lateral geniculate nucleus.",
        "Abstract": "Relay neurons in the lateral geniculate nucleus (LGN) receive direct visual input predominantly from a single retinal ganglion cell (RGC), in addition to indirect input from other sources including interneurons, thalamic reticular nucleus (TRN), and the visual cortex. To address the extent of influence of these indirect sources on the response properties of the LGN neurons, we fit a Generalized Linear Model (GLM) to the spike responses of cat LGN neurons driven by spatially homogeneous spots that were rapidly modulated by a pseudorandom luminance sequence. Several spot sizes were used to probe the spatial extent of the indirect visual effects. Our extracellular recordings captured both the LGN spikes and the incoming RGC input (S potentials), allowing us to divide the inputs to the GLM into two categories: the direct RGC input and the indirect input to which we have access through the luminance of the visual stimulus. For spots no larger than the receptive field center, the effect of the indirect input is negligible, while for larger spots its effect can, on average, account for 5% of the variance of the data and for as much as 25% in some cells. The polarity of the indirect visual influence is opposite to that of the linear receptive field of the neurons. We conclude that the indirect source of response modulation of the LGN relay neurons arises from inhibitory sources, compatible with thalamic interneurons or TRN.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Cats",
            "Geniculate Bodies",
            "Linear Models",
            "Photic Stimulation",
            "Retina",
            "Synaptic Transmission",
            "Visual Cortex",
            "Visual Pathways"
        ],
        "Authors": [
            {
                "First Name": "Baktash",
                "Last Name": "Babadi",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York, NY 10032, USA. bb2280@columbia.edu"
            },
            {
                "First Name": "Alexander",
                "Last Name": "Casti",
                "Affiliation": ""
            },
            {
                "First Name": "Youping",
                "Last Name": "Xiao",
                "Affiliation": ""
            },
            {
                "First Name": "Ehud",
                "Last Name": "Kaplan",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of vision",
        "PubDate": "2010"
    },
    {
        "PMID": "20556641",
        "Title": "Automating the design of informative sequences of sensory stimuli.",
        "Abstract": "Adaptive stimulus design methods can potentially improve the efficiency of sensory neurophysiology experiments significantly; however, designing optimal stimulus sequences in real time remains a serious technical challenge. Here we describe two approximate methods for generating informative stimulus sequences: the first approach provides a fast method for scoring the informativeness of a batch of specific potential stimulus sequences, while the second method attempts to compute an optimal stimulus distribution from which the experimenter may easily sample. We apply these methods to single-neuron spike train data recorded from the auditory midbrain of zebra finches, and demonstrate that the resulting stimulus sequences do in fact provide more information about neuronal tuning in a shorter amount of time than do more standard experimental designs.",
        "Keywords": [],
        "MeSH terms": [
            "Acoustic Stimulation",
            "Action Potentials",
            "Animals",
            "Automation",
            "Computer Simulation",
            "Finches",
            "Humans",
            "Information Theory",
            "Linear Models",
            "Models, Neurological",
            "Sensory Receptor Cells",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Jeremy",
                "Last Name": "Lewi",
                "Affiliation": "Georgia Institute of Technology, Atlanta, GA 30332, USA."
            },
            {
                "First Name": "David M",
                "Last Name": "Schneider",
                "Affiliation": ""
            },
            {
                "First Name": "Sarah M N",
                "Last Name": "Woolley",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2011"
    },
    {
        "PMID": "20554834",
        "Title": "Fast nonnegative deconvolution for spike train inference from population calcium imaging.",
        "Abstract": "Fluorescent calcium indicators are becoming increasingly popular as a means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from a raw fluorescence movie is a nontrivial problem. This work presents a fast nonnegative deconvolution filter to infer the approximately most likely spike train of each neuron, given the fluorescence observations. This algorithm outperforms optimal linear deconvolution (Wiener filtering) on both simulated and biological data. The performance gains come from restricting the inferred spike trains to be positive (using an interior-point method), unlike the Wiener filter. The algorithm runs in linear time, and is fast enough that even when simultaneously imaging >100 neurons, inference can be performed on the set of all observed traces faster than real time. Performing optimal spatial filtering on the images further refines the inferred spike train estimates. Importantly, all the parameters required to perform the inference can be estimated using only the fluorescence data, obviating the need to perform joint electrophysiological and imaging calibration experiments.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Calcium Signaling",
            "Computer Simulation",
            "Fluorescent Dyes",
            "Microscopy, Fluorescence",
            "Microscopy, Video",
            "Models, Neurological",
            "Neurons",
            "Normal Distribution",
            "Poisson Distribution",
            "Signal Processing, Computer-Assisted",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Joshua T",
                "Last Name": "Vogelstein",
                "Affiliation": "Johns Hopkins University, Department of Neuroscience, 3400 N. Charles St., Baltimore, MD 21205, USA. joshuav@jhu.edu"
            },
            {
                "First Name": "Adam M",
                "Last Name": "Packer",
                "Affiliation": ""
            },
            {
                "First Name": "Timothy A",
                "Last Name": "Machado",
                "Affiliation": ""
            },
            {
                "First Name": "Tanya",
                "Last Name": "Sippy",
                "Affiliation": ""
            },
            {
                "First Name": "Baktash",
                "Last Name": "Babadi",
                "Affiliation": ""
            },
            {
                "First Name": "Rafael",
                "Last Name": "Yuste",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neurophysiology",
        "PubDate": "2010"
    },
    {
        "PMID": "20359500",
        "Title": "Population decoding of motor cortical activity using a generalized linear model with hidden states.",
        "Abstract": "Generalized linear models (GLMs) have been developed for modeling and decoding population neuronal spiking activity in the motor cortex. These models provide reasonable characterizations between neural activity and motor behavior. However, they lack a description of movement-related terms which are not observed directly in these experiments, such as muscular activation, the subject's level of attention, and other internal or external states. Here we propose to include a multi-dimensional hidden state to address these states in a GLM framework where the spike count at each time is described as a function of the hand state (position, velocity, and acceleration), truncated spike history, and the hidden state. The model can be identified by an Expectation-Maximization algorithm. We tested this new method in two datasets where spikes were simultaneously recorded using a multi-electrode array in the primary motor cortex of two monkeys. It was found that this method significantly improves the model-fitting over the classical GLM, for hidden dimensions varying from 1 to 4. This method also provides more accurate decoding of hand state (reducing the mean square error by up to 29% in some cases), while retaining real-time computational efficiency. These improvements on representation and decoding over the classical GLM model suggest that this new approach could contribute as a useful tool to motor cortical decoding and prosthetic applications.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Biomechanical Phenomena",
            "Databases as Topic",
            "Hand",
            "Linear Models",
            "Macaca mulatta",
            "Male",
            "Microelectrodes",
            "Models, Neurological",
            "Motor Activity",
            "Motor Cortex",
            "Signal Processing, Computer-Assisted",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Vernon",
                "Last Name": "Lawhern",
                "Affiliation": "Department of Statistics, Florida State University, 117 N Woodward Ave, Tallahassee, FL 32306-4330, USA. vlawhern@stat.fsu.edu"
            },
            {
                "First Name": "Wei",
                "Last Name": "Wu",
                "Affiliation": ""
            },
            {
                "First Name": "Nicholas",
                "Last Name": "Hatsopoulos",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neuroscience methods",
        "PubDate": "2010"
    },
    {
        "PMID": "19943188",
        "Title": "Fast Kalman filtering on quasilinear dendritic trees.",
        "Abstract": "Optimal filtering of noisy voltage signals on dendritic trees is a key problem in computational cellular neuroscience. However, the state variable in this problem-the vector of voltages at every compartment-is very high-dimensional: realistic multicompartmental models often have on the order of N = 10(4) compartments. Standard implementations of the Kalman filter require O(N (3)) time and O(N (2)) space, and are therefore impractical. Here we take advantage of three special features of the dendritic filtering problem to construct an efficient filter: (1) dendritic dynamics are governed by a cable equation on a tree, which may be solved using sparse matrix methods in O(N) time; and current methods for observing dendritic voltage (2) provide low SNR observations and (3) only image a relatively small number of compartments at a time. The idea is to approximate the Kalman equations in terms of a low-rank perturbation of the steady-state (zero-SNR) solution, which may be obtained in O(N) time using methods that exploit the sparse tree structure of dendritic dynamics. The resulting methods give a very good approximation to the exact Kalman solution, but only require O(N) time and space. We illustrate the method with applications to real and simulated dendritic branching structures, and describe how to extend the techniques to incorporate spatially subsampled, temporally filtered, and nonlinearly transformed observations.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Calcium Channels",
            "Computer Simulation",
            "Dendrites",
            "Markov Chains",
            "Models, Neurological",
            "Neurons",
            "Pattern Recognition, Automated",
            "Voltage-Sensitive Dye Imaging"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA. liam@stat.columbia.edu"
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2010"
    },
    {
        "PMID": "19884914",
        "Title": "The relationship between optimal and biologically plausible decoding of stimulus velocity in the retina.",
        "Abstract": "A major open problem in systems neuroscience is to understand the relationship between behavior and the detailed spiking properties of neural populations. We assess how faithfully velocity information can be decoded from a population of spiking model retinal neurons whose spatiotemporal receptive fields and ensemble spike train dynamics are closely matched to real data. We describe how to compute the optimal Bayesian estimate of image velocity given the population spike train response and show that, in the case of global translation of an image with known intensity profile, on average the spike train ensemble signals speed with a fractional standard deviation of about 2% across a specific set of stimulus conditions. We further show how to compute the Bayesian velocity estimate in the case where we only have some a priori information about the (naturalistic) spatial correlation structure of the image but do not know the image explicitly. As expected, the performance of the Bayesian decoder is shown to be less accurate with decreasing prior image information. There turns out to be a close mathematical connection between a biologically plausible \"motion energy\" method for decoding the velocity and the Bayesian decoder in the case that the image is not known. Simulations using the motion energy method and the Bayesian decoder with unknown image reveal that they result in fractional standard deviations of 10% and 6%, respectively, across the same set of stimulus conditions. Estimation performance is rather insensitive to the details of the precise receptive field location, correlated activity between cells, and spike timing.",
        "Keywords": [],
        "MeSH terms": [
            "Bayes Theorem",
            "Computer Simulation",
            "Humans",
            "Models, Biological",
            "Models, Neurological",
            "Models, Statistical",
            "Motion Perception",
            "Psychophysics",
            "Retina",
            "Retinal Ganglion Cells",
            "Vision, Ocular"
        ],
        "Authors": [
            {
                "First Name": "Edmund C",
                "Last Name": "Lalor",
                "Affiliation": "Trinity Centre for Bioengineering and Institute of Neuroscience, Trinity College Dublin, College Green, Dublin 2, Ireland. edlalor@tcd.ie"
            },
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of the Optical Society of America. A, Optics, image science, and vision",
        "PubDate": "2009"
    },
    {
        "PMID": "19718814",
        "Title": "Mean-field approximations for coupled populations of generalized linear model spiking neurons with Markov refractoriness.",
        "Abstract": "There has recently been a great deal of interest in inferring network connectivity from the spike trains in populations of neurons. One class of useful models that can be fit easily to spiking data is based on generalized linear point process models from statistics. Once the parameters for these models are fit, the analyst is left with a nonlinear spiking network model with delays, which in general may be very difficult to understand analytically. Here we develop mean-field methods for approximating the stimulus-driven firing rates (in both the time-varying and steady-state cases), auto- and cross-correlations, and stimulus-dependent filtering properties of these networks. These approximations are valid when the contributions of individual network coupling terms are small and, hence, the total input to a neuron is approximately gaussian. These approximations lead to deterministic ordinary differential equations that are much easier to solve and analyze than direct Monte Carlo simulation of the network activity. These approximations also provide an analytical way to evaluate the linear input-output filter of neurons and how the filters are modulated by network interactions and some stimulus feature. Finally, in the case of strong refractory effects, the mean-field approximations in the generalized linear model become inaccurate; therefore, we introduce a model that captures strong refractoriness, retains all of the easy fitting properties of the standard generalized linear model, and leads to much more accurate approximations of mean firing rates and cross-correlations that retain fine temporal behaviors.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Computer Simulation",
            "Linear Models",
            "Markov Chains",
            "Models, Neurological",
            "Nerve Net",
            "Neurons",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Taro",
                "Last Name": "Toyoizumi",
                "Affiliation": "Department of Neuroscience, Columbia University, New York, NY 10032, USA. taro@neurotheory.columbia.edu"
            },
            {
                "First Name": "Kamiar Rahnama",
                "Last Name": "Rad",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2009"
    },
    {
        "PMID": "19649698",
        "Title": "A new look at state-space models for neural data.",
        "Abstract": "State space methods have proven indispensable in neural data analysis. However, common methods for performing inference in state-space models with non-Gaussian observations rely on certain approximations which are not always accurate. Here we review direct optimization methods that avoid these approximations, but that nonetheless retain the computational efficiency of the approximate methods. We discuss a variety of examples, applying these direct optimization techniques to problems in spike train smoothing, stimulus decoding, parameter estimation, and inference of synaptic properties. Along the way, we point out connections to some related standard statistical methods, including spline smoothing and isotonic regression. Finally, we note that the computational methods reviewed here do not in fact depend on the state-space setting at all; instead, the key property we are exploiting involves the bandedness of certain matrices. We close by discussing some applications of this more general point of view, including Markov chain Monte Carlo methods for neural decoding and efficient estimation of spatially-varying firing rates.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Computer Simulation",
            "Models, Neurological",
            "Models, Statistical",
            "Neurons",
            "Retinal Ganglion Cells",
            "Synapses"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA. liam@stat.columbia.edu."
            },
            {
                "First Name": "Yashar",
                "Last Name": "Ahmadian",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Daniel Gil",
                "Last Name": "Ferreira",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Shinsuke",
                "Last Name": "Koyama",
                "Affiliation": "Department of Statistics, Carnegie Mellon University, Pittsburgh, PA, USA."
            },
            {
                "First Name": "Kamiar",
                "Last Name": "Rahnama Rad",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Michael",
                "Last Name": "Vidne",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA."
            },
            {
                "First Name": "Joshua",
                "Last Name": "Vogelstein",
                "Affiliation": "Department of Neuroscience, Johns Hopkins University, Baltimore, MD, USA."
            },
            {
                "First Name": "Wei",
                "Last Name": "Wu",
                "Affiliation": "Department of Statistics, Florida State University, Tallahassee, FL, USA."
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2010"
    },
    {
        "PMID": "19619479",
        "Title": "Spike inference from calcium imaging using sequential Monte Carlo methods.",
        "Abstract": "As recent advances in calcium sensing technologies facilitate simultaneously imaging action potentials in neuronal populations, complementary analytical tools must also be developed to maximize the utility of this experimental paradigm. Although the observations here are fluorescence movies, the signals of interest--spike trains and/or time varying intracellular calcium concentrations--are hidden. Inferring these hidden signals is often problematic due to noise, nonlinearities, slow imaging rate, and unknown biophysical parameters. We overcome these difficulties by developing sequential Monte Carlo methods (particle filters) based on biophysical models of spiking, calcium dynamics, and fluorescence. We show that even in simple cases, the particle filters outperform the optimal linear (i.e., Wiener) filter, both by obtaining better estimates and by providing error bars. We then relax a number of our model assumptions to incorporate nonlinear saturation of the fluorescence signal, as well external stimulus and spike history dependence (e.g., refractoriness) of the spike trains. Using both simulations and in vitro fluorescence observations, we demonstrate temporal superresolution by inferring when within a frame each spike occurs. Furthermore, the model parameters may be estimated using expectation maximization with only a very limited amount of data (e.g., approximately 5-10 s or 5-40 spikes), without the requirement of any simultaneous electrophysiology or imaging experiments.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Calcium",
            "Fluorescence",
            "Intracellular Space",
            "Mice",
            "Mice, Inbred C57BL",
            "Models, Biological",
            "Monte Carlo Method",
            "Neurons",
            "Probability",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Joshua T",
                "Last Name": "Vogelstein",
                "Affiliation": "Department of Neuroscience, The Johns Hopkins School of Medicine, Baltimore, Maryland, USA. joshuav@jhu.edu"
            },
            {
                "First Name": "Brendon O",
                "Last Name": "Watson",
                "Affiliation": ""
            },
            {
                "First Name": "Adam M",
                "Last Name": "Packer",
                "Affiliation": ""
            },
            {
                "First Name": "Rafael",
                "Last Name": "Yuste",
                "Affiliation": ""
            },
            {
                "First Name": "Bruno",
                "Last Name": "Jedynak",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Biophysical journal",
        "PubDate": "2009"
    },
    {
        "PMID": "19497822",
        "Title": "Neural decoding of hand motion using a linear state-space model with hidden states.",
        "Abstract": "The Kalman filter has been proposed as a model to decode neural activity measured from the motor cortex in order to obtain real-time estimates of hand motion in behavioral neurophysiological experiments. However, currently used linear state-space models underlying the Kalman filter do not take into account other behavioral states such as muscular activity or the subject's level of attention, which are often unobservable during experiments but may play important roles in characterizing neural controlled hand movement. To address this issue, we depict these unknown states as one multidimensional hidden state in the linear state-space framework. This new model assumes that the observed neural firing rate is directly related to this hidden state. The dynamics of the hand state are also allowed to impact the dynamics of the hidden state, and vice versa. The parameters in the model can be identified by a conventional expectation-maximization algorithm. Since this model still uses the linear Gaussian framework, hand-state decoding can be performed by the efficient Kalman filter algorithm. Experimental results show that this new model provides a more appropriate representation of the neural data and generates more accurate decoding. Furthermore, we have used recently developed computationally efficient methods by incorporating a priori information of the targets of the reaching movement. Our results show that the hidden-state model with target-conditioning further improves decoding accuracy.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Computer Simulation",
            "Electroencephalography",
            "Evoked Potentials, Motor",
            "Hand",
            "Humans",
            "Linear Models",
            "Models, Neurological",
            "Motor Cortex",
            "Movement"
        ],
        "Authors": [
            {
                "First Name": "Wei",
                "Last Name": "Wu",
                "Affiliation": "Department of Statistics, Florida State University, Tallahassee, FL 32306, USA. wwu@stat.fsu.edu"
            },
            {
                "First Name": "Jayant E",
                "Last Name": "Kulkarni",
                "Affiliation": ""
            },
            {
                "First Name": "Nicholas G",
                "Last Name": "Hatsopoulos",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society",
        "PubDate": "2009"
    },
    {
        "PMID": "19424506",
        "Title": "Smoothing of, and parameter estimation from, noisy biophysical recordings.",
        "Abstract": "Biophysically detailed models of single cells are difficult to fit to real data. Recent advances in imaging techniques allow simultaneous access to various intracellular variables, and these data can be used to significantly facilitate the modelling task. These data, however, are noisy, and current approaches to building biophysically detailed models are not designed to deal with this. We extend previous techniques to take the noisy nature of the measurements into account. Sequential Monte Carlo (\"particle filtering\") methods, in combination with a detailed biophysical description of a cell, are used for principled, model-based smoothing of noisy recording data. We also provide an alternative formulation of smoothing where the neural nonlinearities are estimated in a non-parametric manner. Biophysically important parameters of detailed models (such as channel densities, intercompartmental conductances, input resistances, and observation noise) are inferred automatically from noisy data via expectation-maximization. Overall, we find that model-based smoothing is a powerful, robust technique for smoothing of noisy biophysical data and for inference of biophysical parameters in the face of recording noise.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Artificial Intelligence",
            "Biophysical Phenomena",
            "Cell Physiological Phenomena",
            "Computational Biology",
            "Computer Simulation",
            "Electrophysiological Phenomena",
            "Image Processing, Computer-Assisted",
            "Models, Biological",
            "Monte Carlo Method",
            "Statistics, Nonparametric"
        ],
        "Authors": [
            {
                "First Name": "Quentin J M",
                "Last Name": "Huys",
                "Affiliation": "Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom. qhuys@cantab.net"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "PLoS computational biology",
        "PubDate": "2009"
    },
    {
        "PMID": "19399603",
        "Title": "Efficient computation of the maximum a posteriori path and parameter estimation in integrate-and-fire and more general state-space models.",
        "Abstract": "A number of important data analysis problems in neuroscience can be solved using state-space models. In this article, we describe fast methods for computing the exact maximum a posteriori (MAP) path of the hidden state variable in these models, given spike train observations. If the state transition density is log-concave and the observation model satisfies certain standard assumptions, then the optimization problem is strictly concave and can be solved rapidly with Newton-Raphson methods, because the Hessian of the loglikelihood is block tridiagonal. We can further exploit this block-tridiagonal structure to develop efficient parameter estimation methods for these models. We describe applications of this approach to neural decoding problems, with a focus on the classic integrate-and-fire model as a key example.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Computer Simulation",
            "Models, Neurological",
            "Models, Statistical",
            "Neurons"
        ],
        "Authors": [
            {
                "First Name": "Shinsuke",
                "Last Name": "Koyama",
                "Affiliation": "Department of Statistics and Center for the Neural Basis of Cognition, Carnegie Mellon University, Pittsburgh, PA, USA. koyama@stat.cmu.edu."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA."
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2010"
    },
    {
        "PMID": "19292647",
        "Title": "Maximally reliable Markov chains under energy constraints.",
        "Abstract": "Signal-to-noise ratios in physical systems can be significantly degraded if the outputs of the systems are highly variable. Biological processes for which highly stereotyped signal generations are necessary features appear to have reduced their signal variabilities by employing multiple processing steps. To better understand why this multistep cascade structure might be desirable, we prove that the reliability of a signal generated by a multistate system with no memory (i.e., a Markov chain) is maximal if and only if the system topology is such that the process steps irreversibly through each state, with transition rates chosen such that an equal fraction of the total signal is generated in each state. Furthermore, our result indicates that by increasing the number of states, it is possible to arbitrarily increase the reliability of the system. In a physical system, however, an energy cost is associated with maintaining irreversible transitions, and this cost increases with the number of such transitions (i.e., the number of states). Thus, an infinite-length chain, which would be perfectly reliable, is infeasible. To model the effects of energy demands on the maximally reliable solution, we numerically optimize the topology under two distinct energy functions that penalize either irreversible transitions or incommunicability between states, respectively. In both cases, the solutions are essentially irreversible linear chains, but with upper bounds on the number of states set by the amount of available energy. We therefore conclude that a physical system for which signal reliability is important should employ a linear architecture, with the number of states (and thus the reliability) determined by the intrinsic energy constraints of the system.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Computer Simulation",
            "Energy Metabolism",
            "Markov Chains",
            "Models, Molecular",
            "Reproducibility of Results",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Sean",
                "Last Name": "Escola",
                "Affiliation": "Center for Theoretical Neuroscience and M.D./Ph.D. Program, Columbia University, New York, NY 10032, U.S.A. gse3@columbia.edu"
            },
            {
                "First Name": "Michael",
                "Last Name": "Eisele",
                "Affiliation": ""
            },
            {
                "First Name": "Kenneth",
                "Last Name": "Miller",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2009"
    },
    {
        "PMID": "19211329",
        "Title": "Bayesian image recovery for dendritic structures under low signal-to-noise conditions.",
        "Abstract": "Experimental research seeking to quantify neuronal structure constantly contends with restrictions on image resolution and variability. In particular, experimentalists often need to analyze images with very low signal-to-noise ratio (SNR). In many experiments, dye toxicity scales with the light intensity; this leads experimentalists to reduce image SNR in order to preserve the viability of the specimen. In this paper, we present a Bayesian approach for estimating the neuronal shape given low-SNR observations. This Bayesian framework has two major advantages. First, the method effectively incorporates known facts about 1) the image formation process, including blur and the Poisson nature of image noise at low intensities, and 2) dendritic shape, including the fact that dendrites are simply-connected geometric structures with smooth boundaries. Second, we may employ standard Markov chain Monte Carlo techniques for quantifying the posterior uncertainty in our estimate of the dendritic shape. We describe an efficient computational implementation of these methods and demonstrate the algorithm's performance on simulated noisy two-photon laser-scanning microscopy images.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Artificial Intelligence",
            "Bayes Theorem",
            "Dendrites",
            "Humans",
            "Image Enhancement",
            "Image Interpretation, Computer-Assisted",
            "Imaging, Three-Dimensional",
            "Reproducibility of Results",
            "Sensitivity and Specificity"
        ],
        "Authors": [
            {
                "First Name": "Geoffrey",
                "Last Name": "Fudenberg",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY 10027, USA. geoff.fudenberg@gmail.com"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "IEEE transactions on image processing : a publication of the IEEE Signal Processing Society",
        "PubDate": "2009"
    },
    {
        "PMID": "18928364",
        "Title": "Sequential optimal design of neurophysiology experiments.",
        "Abstract": "Adaptively optimizing experiments has the potential to significantly reduce the number of trials needed to build parametric statistical models of neural systems. However, application of adaptive methods to neurophysiology has been limited by severe computational challenges. Since most neurons are high-dimensional systems, optimizing neurophysiology experiments requires computing high-dimensional integrations and optimizations in real time. Here we present a fast algorithm for choosing the most informative stimulus by maximizing the mutual information between the data and the unknown parameters of a generalized linear model (GLM) that we want to fit to the neuron's activity. We rely on important log concavity and asymptotic normality properties of the posterior to facilitate the required computations. Our algorithm requires only low-rank matrix manipulations and a two-dimensional search to choose the optimal stimulus. The average running time of these operations scales quadratically with the dimensionality of the GLM, making real-time adaptive experimental design feasible even for high-dimensional stimulus and parameter spaces. For example, we require roughly 10 milliseconds on a desktop computer to optimize a 100-dimensional stimulus. Despite using some approximations to make the algorithm efficient, our algorithm asymptotically decreases the uncertainty about the model parameters at a rate equal to the maximum rate predicted by an asymptotic analysis. Simulation results show that picking stimuli by maximizing the mutual information can speed up convergence to the optimal values of the parameters by an order of magnitude compared to using random (nonadaptive) stimuli. Finally, applying our design procedure to real neurophysiology experiments requires addressing the nonstationarities that we would expect to see in neural responses; our algorithm can efficiently handle both fast adaptation due to spike history effects and slow, nonsystematic drifts in a neuron's activity.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Computer Simulation",
            "Humans",
            "Models, Neurological",
            "Neurons",
            "Neurophysiology",
            "Nonlinear Dynamics",
            "Research Design",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Jeremy",
                "Last Name": "Lewi",
                "Affiliation": "Bioengineering Graduate Program, Wallace H. Coulter Department of Biomedical Engineering, Laboratory for Neuroengineering, Georgia Institute of Technology, Atlanta, GA 30332, USA."
            },
            {
                "First Name": "Robert",
                "Last Name": "Butera",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2009"
    },
    {
        "PMID": "18650810",
        "Title": "Spatio-temporal correlations and visual signalling in a complete neuronal population.",
        "Abstract": "Statistical dependencies in the responses of sensory neurons govern both the amount of stimulus information conveyed and the means by which downstream neurons can extract it. Although a variety of measurements indicate the existence of such dependencies, their origin and importance for neural coding are poorly understood. Here we analyse the functional significance of correlated firing in a complete population of macaque parasol retinal ganglion cells using a model of multi-neuron spike responses. The model, with parameters fit directly to physiological data, simultaneously captures both the stimulus dependence and detailed spatio-temporal correlations in population responses, and provides two insights into the structure of the neural code. First, neural encoding at the population level is less noisy than one would expect from the variability of individual neurons: spike times are more precise, and can be predicted more accurately when the spiking of neighbouring neurons is taken into account. Second, correlations provide additional sensory information: optimal, model-based decoding that exploits the response correlation structure extracts 20% more information about the visual scene than decoding under the assumption of independence, and preserves 40% more visual information than optimal linear decoding. This model-based approach reveals the role of correlated activity in the retinal coding of visual stimuli, and provides a general framework for understanding the importance of correlated activity in populations of neurons.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Macaca mulatta",
            "Models, Neurological",
            "Photic Stimulation",
            "Retinal Ganglion Cells",
            "Time Factors",
            "Vision, Ocular"
        ],
        "Authors": [
            {
                "First Name": "Jonathan W",
                "Last Name": "Pillow",
                "Affiliation": "Gatsby Computational Neuroscience Unit, UCL, 17 Queen Square, London WC1N 3AR, UK. pillow@gatsby.ucl.ac.uk"
            },
            {
                "First Name": "Jonathon",
                "Last Name": "Shlens",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "Alexander",
                "Last Name": "Sher",
                "Affiliation": ""
            },
            {
                "First Name": "Alan M",
                "Last Name": "Litke",
                "Affiliation": ""
            },
            {
                "First Name": "E J",
                "Last Name": "Chichilnisky",
                "Affiliation": ""
            },
            {
                "First Name": "Eero P",
                "Last Name": "Simoncelli",
                "Affiliation": ""
            }
        ],
        "Journal": "Nature",
        "PubDate": "2008"
    },
    {
        "PMID": "18300178",
        "Title": "Inferring input nonlinearities in neural encoding models.",
        "Abstract": "We describe a class of models that predict how the instantaneous firing rate of a neuron depends on a dynamic stimulus. The models utilize a learnt pointwise nonlinear transform of the stimulus, followed by a linear filter that acts on the sequence of transformed inputs. In one case, the nonlinear transform is the same at all filter lag-times. Thus, this \"input nonlinearity\" converts the initial numerical representation of stimulus value to a new representation that provides optimal input to the subsequent linear model. We describe algorithms that estimate both the input nonlinearity and the linear weights simultaneously; and present techniques to regularise and quantify uncertainty in the estimates. In a second approach, the model is generalized to allow a different nonlinear transform of the stimulus value at each lag-time. Although more general, this model is algorithmically more straightforward to fit. However, it has many more degrees of freedom than the first approach, thus requiring more data for accurate estimation. We test the feasibility of these methods on synthetic data, and on responses from a neuron in rodent barrel cortex. The models are shown to predict responses to novel data accurately, and to recover several important neuronal response properties.",
        "Keywords": [],
        "MeSH terms": [
            "Models, Neurological",
            "Neural Networks, Computer",
            "Neurons",
            "Nonlinear Dynamics"
        ],
        "Authors": [
            {
                "First Name": "Misha B",
                "Last Name": "Ahrens",
                "Affiliation": "Gatsby Computational Neuroscience Unit, University College London, London, UK."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "Maneesh",
                "Last Name": "Sahani",
                "Affiliation": ""
            }
        ],
        "Journal": "Network (Bristol, England)",
        "PubDate": "2008"
    },
    {
        "PMID": "17945990",
        "Title": "Efficient model-based design of neurophysiological experiments.",
        "Abstract": "We apply an adaptive approach to optimal experimental design in the context of estimating the unknown parameters of a model of a neuron's response. We present an algorithm to choose the optimal (most informative) stimulus on each trial; this algorithm can be implemented efficiently even for high-dimensional stimulus and parameter spaces (in particular, no high-dimensional numerical optimizations or integrations are required). Our simulation results show that model parameters can be estimated much more efficiently using this adaptive algorithm rather than random sampling. We also show that this adaptive approach leads to superior performance in the case that the model parameters are nonstationary, as would be expected in real experiments.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Humans",
            "Models, Neurological",
            "Models, Statistical",
            "Nerve Net",
            "Neurons",
            "Neurophysiology",
            "Research Design",
            "Stochastic Processes",
            "Synaptic Transmission"
        ],
        "Authors": [
            {
                "First Name": "Jeremy",
                "Last Name": "Lewi",
                "Affiliation": "School of Bioengineering, Georgia Institute of Technology, USA. jlewi@gatech.edu"
            },
            {
                "First Name": "Robert",
                "Last Name": "Butera",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference",
        "PubDate": "2006"
    },
    {
        "PMID": "17943613",
        "Title": "Common-input models for multiple neural spike-train data.",
        "Abstract": "Recent developments in multi-electrode recordings enable the simultaneous measurement of the spiking activity of many neurons. Analysis of such multineuronal data is one of the key challenge in computational neuroscience today. In this work, we develop a multivariate point-process model in which the observed activity of a network of neurons depends on three terms: (1) the experimentally-controlled stimulus; (2) the spiking history of the observed neurons; and (3) a hidden term that corresponds, for example, to common input from an unobserved population of neurons that is presynaptic to two or more cells in the observed population. We consider two models for the network firing-rates, one of which is computationally and analytically tractable but can lead to unrealistically high firing-rates, while the other with reasonable firing-rates imposes a greater computational burden. We develop an expectation-maximization algorithm for fitting the parameters of both the models. For the analytically tractable model the expectation step is based on a continuous-time implementation of the extended Kalman smoother, and the maximization step involves two concave maximization problems which may be solved in parallel. The other model that we consider necessitates the use of Monte Carlo methods for the expectation as well as maximization step. We discuss the trade-off involved in choosing between the two models and the associated methods. The techniques developed allow us to solve a variety of inference problems in a straightforward, computationally efficient fashion; for example, we may use the model to predict network activity given an arbitrary stimulus, infer a neuron's ring rate given the stimulus and the activity of the other observed neurons, and perform optimal stimulus decoding and prediction. We present several detailed simulation studies which explore the strengths and limitations of our approach.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Computer Simulation",
            "Models, Neurological",
            "Neural Networks, Computer",
            "Neural Pathways",
            "Neurons",
            "Proportional Hazards Models"
        ],
        "Authors": [
            {
                "First Name": "Jayant E",
                "Last Name": "Kulkarni",
                "Affiliation": "Center for Theoretical Neuroscience, Columbia University, New York 10032, USA. jk2619@columbia.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Network (Bristol, England)",
        "PubDate": "2007"
    },
    {
        "PMID": "17925266",
        "Title": "Statistical models for neural encoding, decoding, and optimal stimulus design.",
        "Abstract": "There are two basic problems in the statistical analysis of neural data. The \"encoding\" problem concerns how information is encoded in neural spike trains: can we predict the spike trains of a neuron (or population of neurons), given an arbitrary stimulus or observed motor response? Conversely, the \"decoding\" problem concerns how much information is in a spike train, in particular, how well can we estimate the stimulus that gave rise to the spike train? This chapter describes statistical model-based techniques that in some cases provide a unified solution to these two coding problems. These models can capture stimulus dependencies as well as spike history and interneuronal interaction effects in population spike trains, and are intimately related to biophysically based models of integrate-and-fire type. We describe flexible, powerful likelihood-based methods for fitting these encoding models and then for using the models to perform optimal decoding. Each of these (apparently quite difficult) tasks turn out to be highly computationally tractable, due to a key concavity property of the model likelihood. Finally, we return to the encoding problem to describe how to use these models to adaptively optimize the stimuli presented to the cell on a trial-by-trial basis, in order that we may infer the optimal model parameters as efficiently as possible.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Computer Simulation",
            "Models, Neurological",
            "Models, Statistical",
            "Neurons"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics and Center for Theoretical Neuroscience, Columbia University, New York, NY, USA. liam@stat.columbia.edu"
            },
            {
                "First Name": "Jonathan",
                "Last Name": "Pillow",
                "Affiliation": ""
            },
            {
                "First Name": "Jeremy",
                "Last Name": "Lewi",
                "Affiliation": ""
            }
        ],
        "Journal": "Progress in brain research",
        "PubDate": "2007"
    },
    {
        "PMID": "17492371",
        "Title": "Integral equation methods for computing likelihoods and their derivatives in the stochastic integrate-and-fire model.",
        "Abstract": "We recently introduced likelihood-based methods for fitting stochastic integrate-and-fire models to spike train data. The key component of this method involves the likelihood that the model will emit a spike at a given time t. Computing this likelihood is equivalent to computing a Markov first passage time density (the probability that the model voltage crosses threshold for the first time at time t). Here we detail an improved method for computing this likelihood, based on solving a certain integral equation. This integral equation method has several advantages over the techniques discussed in our previous work: in particular, the new method has fewer free parameters and is easily differentiable (for gradient computations). The new method is also easily adaptable for the case in which the model conductance, not just the input current, is time-varying. Finally, we describe how to incorporate large deviations approximations to very small likelihoods.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Algorithms",
            "Animals",
            "Brain",
            "Computer Simulation",
            "Humans",
            "Membrane Potentials",
            "Nerve Net",
            "Neural Networks, Computer",
            "Neural Pathways",
            "Neurons",
            "Probability",
            "Reaction Time",
            "Stochastic Processes",
            "Synapses",
            "Synaptic Transmission",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Columbia University, New York, NY, USA. liam@stat.columbia.edu"
            },
            {
                "First Name": "Adrian",
                "Last Name": "Haith",
                "Affiliation": ""
            },
            {
                "First Name": "Gabor",
                "Last Name": "Szirtes",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2008"
    },
    {
        "PMID": "16999572",
        "Title": "The spike-triggered average of the integrate-and-fire cell driven by gaussian white noise.",
        "Abstract": "We compute the exact spike-triggered average (STA) of the voltage for the nonleaky integrate-and-fire (IF) cell in continuous time, driven by gaussian white noise. The computation is based on techniques from the theory of renewal processes and continuous-time hidden Markov processes (e.g., the backward and forward Fokker-Planck partial differential equations associated with first-passage time densities). From the STA voltage, it is straightforward to derive the STA input current. The theory also gives an explicit asymptotic approximation for the STA of the leaky IF cell, valid in the low-noise regime sigma --> 0. We consider both the STA and the conditional average voltage given an observed spike \"doublet\" event, that is, two spikes separated by some fixed period of silence. In each case, we find that the STA as a function of time-preceding-spike, tau, has a square root singularity as tau approaches zero from below and scales linearly with the scale of injected noise current. We close by briefly examining the discrete-time case, where similar phenomena are observed.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Electrophysiology",
            "Models, Neurological",
            "Neurons",
            "Noise",
            "Normal Distribution",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, New York University, New York, NY 10027, USA. liam@stat.columbia.edu"
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2006"
    },
    {
        "PMID": "16790591",
        "Title": "Linear encoding of muscle activity in primary motor cortex and cerebellum.",
        "Abstract": "The activity of neurons in primary motor cortex (M1) and cerebellum is known to correlate with extrinsic movement parameters, including hand position and velocity. Relatively few studies have addressed the encoding of intrinsic parameters, such as muscle activity. Here we applied a generalized regression analysis to describe the relationship of neurons in M1 and cerebellar dentate nucleus to electromyographic (EMG) activity from hand and forearm muscles, during performance of precision grip by macaque monkeys. We showed that cells in both M1 and dentate encode muscle activity in a linear fashion, and that EMG signals provide predictions of neural discharge that are equally accurate to those from kinematic information under these task conditions. Neural activity in M1 was significantly more correlated with both EMG and kinematic signals than was activity in dentate nucleus. Furthermore, the analysis enabled us to look at the temporal properties of muscle encoding. Cells were broadly tuned to muscle activity as a function of the lag between spiking and EMG and there was considerable heterogeneity in the optimal delay among individual neurons. However, a single lag (40 ms) was generally sufficient to provide good fits. Finally, incorporating spike history effects in our model offered no advantage in predicting novel spike trains, reinforcing the simple nature of the muscle encoding that we observed here.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Biomechanical Phenomena",
            "Cerebellum",
            "Electromyography",
            "Electrophysiology",
            "Female",
            "Forearm",
            "Hand",
            "Linear Models",
            "Macaca mulatta",
            "Male",
            "Models, Neurological",
            "Motor Cortex",
            "Motor Neurons",
            "Muscle, Skeletal"
        ],
        "Authors": [
            {
                "First Name": "Benjamin R",
                "Last Name": "Townsend",
                "Affiliation": "Sobell Department of Motor Neuroscience and Movement Disorders, Institute of Neurology, University College London, London UK WC1N 3BG."
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "Roger N",
                "Last Name": "Lemon",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neurophysiology",
        "PubDate": "2006"
    },
    {
        "PMID": "16633936",
        "Title": "The most likely voltage path and large deviations approximations for integrate-and-fire neurons.",
        "Abstract": "We develop theory and numerical methods for computing the most likely subthreshold voltage path of a noisy integrate-and-fire (IF) neuron, given observations of the neuron's superthreshold spiking activity. This optimal voltage path satisfies a second-order ordinary differential (Euler-Lagrange) equation which may be solved analytically in a number of special cases, and which may be solved numerically in general via a simple \"shooting\" algorithm. Our results are applicable for both linear and nonlinear subthreshold dynamics, and in certain cases may be extended to correlated subthreshold noise sources. We also show how this optimal voltage may be used to obtain approximations to (1) the likelihood that an IF cell with a given set of parameters was responsible for the observed spike train; and (2) the instantaneous firing rate and interspike interval distribution of a given noisy IF cell. The latter probability approximations are based on the classical Freidlin-Wentzell theory of large deviations principles for stochastic differential equations. We close by comparing this most likely voltage path to the true observed subthreshold voltage trace in a case when intracellular voltage recordings are available in vitro.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Differential Threshold",
            "Electric Stimulation",
            "Models, Neurological",
            "Nerve Net",
            "Neural Conduction",
            "Neural Inhibition",
            "Neural Networks, Computer",
            "Neurons",
            "Nonlinear Dynamics",
            "Reaction Time",
            "Software",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Department of Statistics, Columbia University, Columbia. liam@stat.columbia.edu"
            }
        ],
        "Journal": "Journal of computational neuroscience",
        "PubDate": "2006"
    },
    {
        "PMID": "16624998",
        "Title": "Efficient estimation of detailed single-neuron models.",
        "Abstract": "Biophysically accurate multicompartmental models of individual neurons have significantly advanced our understanding of the input-output function of single cells. These models depend on a large number of parameters that are difficult to estimate. In practice, they are often hand-tuned to match measured physiological behaviors, thus raising questions of identifiability and interpretability. We propose a statistical approach to the automatic estimation of various biologically relevant parameters, including 1) the distribution of channel densities, 2) the spatiotemporal pattern of synaptic input, and 3) axial resistances across extended dendrites. Recent experimental advances, notably in voltage-sensitive imaging, motivate us to assume access to: i) the spatiotemporal voltage signal in the dendrite and ii) an approximate description of the channel kinetics of interest. We show here that, given i and ii, parameters 1-3 can be inferred simultaneously by nonnegative linear regression; that this optimization problem possesses a unique solution and is guaranteed to converge despite the large number of parameters and their complex nonlinear interaction; and that standard optimization algorithms efficiently reach this optimum with modest computational and data requirements. We demonstrate that the method leads to accurate estimations on a wide variety of challenging model data sets that include up to about 10(4) parameters (roughly two orders of magnitude more than previously feasible) and describe how the method gives insights into the functional interaction of groups of channels.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Biophysical Phenomena",
            "Biophysics",
            "Cell Membrane",
            "Data Interpretation, Statistical",
            "Dendrites",
            "Electrophysiology",
            "Ion Channel Gating",
            "Ion Channels",
            "Kinetics",
            "Ligands",
            "Likelihood Functions",
            "Models, Neurological",
            "Monte Carlo Method",
            "Neurons",
            "Patch-Clamp Techniques",
            "Receptors, N-Methyl-D-Aspartate",
            "Synapses"
        ],
        "Authors": [
            {
                "First Name": "Quentin J M",
                "Last Name": "Huys",
                "Affiliation": "Gatsby Computational Neuroscience Unit, University College London, UK. qhuys.ahrens@gatsby.ucl.ac.uk"
            },
            {
                "First Name": "Misha B",
                "Last Name": "Ahrens",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neurophysiology",
        "PubDate": "2006"
    },
    {
        "PMID": "16306413",
        "Title": "Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model.",
        "Abstract": "Sensory encoding in spiking neurons depends on both the integration of sensory inputs and the intrinsic dynamics and variability of spike generation. We show that the stimulus selectivity, reliability, and timing precision of primate retinal ganglion cell (RGC) light responses can be reproduced accurately with a simple model consisting of a leaky integrate-and-fire spike generator driven by a linearly filtered stimulus, a postspike current, and a Gaussian noise current. We fit model parameters for individual RGCs by maximizing the likelihood of observed spike responses to a stochastic visual stimulus. Although compact, the fitted model predicts the detailed time structure of responses to novel stimuli, accurately capturing the interaction between the spiking history and sensory stimulus selectivity. The model also accounts for the variability in responses to repeated stimuli, even when fit to data from a single (nonrepeating) stimulus sequence. Finally, the model can be used to derive an explicit, maximum-likelihood decoding rule for neural spike trains, thus providing a tool for assessing the limitations that spiking variability imposes on sensory performance.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Macaca",
            "Models, Neurological",
            "Models, Statistical",
            "Photic Stimulation",
            "Retinal Ganglion Cells"
        ],
        "Authors": [
            {
                "First Name": "Jonathan W",
                "Last Name": "Pillow",
                "Affiliation": "Howard Hughes Medical Institute, Center for Neural Science, Courant Institute of Mathematical Sciences, New York University, New York, New York 10003, USA. pillow@cns.nyu.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "Valerie J",
                "Last Name": "Uzzell",
                "Affiliation": ""
            },
            {
                "First Name": "Eero P",
                "Last Name": "Simoncelli",
                "Affiliation": ""
            },
            {
                "First Name": "E J",
                "Last Name": "Chichilnisky",
                "Affiliation": ""
            }
        ],
        "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience",
        "PubDate": "2005"
    },
    {
        "PMID": "16041995",
        "Title": "Statistical encoding model for a primary motor cortical brain-machine interface.",
        "Abstract": "A number of studies of the motor system suggest that the majority of primary motor cortical neurons represent simple movement-related kinematic and dynamic quantities in their time-varying activity patterns. An example of such an encoding relationship is the cosine tuning of firing rate with respect to the direction of hand motion. We present a systematic development of statistical encoding models for movement-related motor neurons using multielectrode array recordings during a two-dimensional (2-D) continuous pursuit-tracking task. Our approach avoids massive averaging of responses by utilizing 2-D normalized occupancy plots, cascaded linear-nonlinear (LN) system models and a method for describing variability in discrete random systems. We found that the expected firing rate of most movement-related motor neurons is related to the kinematic values by a linear transformation, with a significant nonlinear distortion in about 1/3 of the neurons. The measured variability of the neural responses is markedly non-Poisson in many neurons and is well captured by a \"normalized-Gaussian\" statistical model that is defined and introduced here. The statistical model is seamlessly integrated into a nearly-optimal recursive method for decoding movement from neural responses based on a Sequential Monte Carlo filter.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Brain Mapping",
            "Cognition",
            "Electroencephalography",
            "Evoked Potentials, Motor",
            "Macaca",
            "Models, Neurological",
            "Models, Statistical",
            "Monte Carlo Method",
            "Motor Cortex",
            "Signal Processing, Computer-Assisted",
            "User-Computer Interface"
        ],
        "Authors": [
            {
                "First Name": "Shy",
                "Last Name": "Shoham",
                "Affiliation": "Faculty of Biomedical Engineering, the Technion, Israel Institute of Technology, Haifa 32000, Israel. sshoham@bm.technion.ac.il"
            },
            {
                "First Name": "Liam M",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "Matthew R",
                "Last Name": "Fellows",
                "Affiliation": ""
            },
            {
                "First Name": "Nicholas G",
                "Last Name": "Hatsopoulos",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Donoghue",
                "Affiliation": ""
            },
            {
                "First Name": "Richard A",
                "Last Name": "Normann",
                "Affiliation": ""
            }
        ],
        "Journal": "IEEE transactions on bio-medical engineering",
        "PubDate": "2005"
    },
    {
        "PMID": "15901405",
        "Title": "Asymptotic theory of information-theoretic experimental design.",
        "Abstract": "We discuss an idea for collecting data in a relatively efficient manner. Our point of view is Bayesian and information-theoretic: on any given trial, we want to adaptively choose the input in such a way that the mutual information between the (unknown) state of the system and the (stochastic) output is maximal, given any prior information (including data collected on any previous trials). We prove a theorem that quantifies the effectiveness of this strategy and give a few illustrative examples comparing the performance of this adaptive technique to that of the more usual nonadaptive experimental design. In particular, we calculate the asymptotic efficiency of the information-maximization strategy and demonstrate that this method is in a well-defined sense never less efficient--and is generically more efficient--than the nonadaptive strategy. For example, we are able to explicitly calculate the asymptotic relative efficiency of the staircase method widely employed in psychophysics research and to demonstrate the dependence of this efficiency on the form of the psychometric function underlying the output responses.",
        "Keywords": [],
        "MeSH terms": [],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Gatsby Computational Neuroscience Unit, University College London, London, WC1N 3AR, UK. liam@gatsby.ucl.ac.uk"
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2005"
    },
    {
        "PMID": "15600233",
        "Title": "Maximum likelihood estimation of cascade point-process neural encoding models.",
        "Abstract": "Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage. We analyze the estimation of one such model for which this nonlinear step is implemented by a known parametric function; the assumption that this function is known speeds the estimation process considerably. We investigate the shape of the likelihood function for this type of model, give a simple condition on the nonlinearity ensuring that no non-global local maxima exist in the likelihood-leading, in turn, to efficient algorithms for the computation of the maximum likelihood estimator-and discuss the implications for the form of the allowed nonlinearities. Finally, we note some interesting connections between the likelihood-based estimators and the classical spike-triggered average estimator, discuss some useful extensions of the basic model structure, and provide two novel applications to physiological data.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Likelihood Functions",
            "Models, Neurological",
            "Motor Cortex",
            "Neural Networks, Computer",
            "Neurons"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Gatsby Computational Neuroscience Unit, University College London, UK. liam@gatsby.ucl.ac.uk"
            }
        ],
        "Journal": "Network (Bristol, England)",
        "PubDate": "2004"
    },
    {
        "PMID": "15516273",
        "Title": "Maximum likelihood estimation of a stochastic integrate-and-fire neural encoding model.",
        "Abstract": "We examine a cascade encoding model for neural response in which a linear filtering stage is followed by a noisy, leaky, integrate-and-fire spike generation mechanism. This model provides a biophysically more realistic alternative to models based on Poisson (memoryless) spike generation, and can effectively reproduce a variety of spiking behaviors seen in vivo. We describe the maximum likelihood estimator for the model parameters, given only extracellular spike train responses (not intracellular voltage data). Specifically, we prove that the log-likelihood function is concave and thus has an essentially unique global maximum that can be found using gradient ascent techniques. We develop an efficient algorithm for computing the maximum likelihood solution, demonstrate the effectiveness of the resulting estimator with numerical simulations, and discuss a method of testing the model's validity using time-rescaling and density evolution techniques.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Biophysical Phenomena",
            "Biophysics",
            "Electrophysiology",
            "Likelihood Functions",
            "Membrane Potentials",
            "Models, Neurological",
            "Models, Statistical",
            "Neurons",
            "Nonlinear Dynamics",
            "Poisson Distribution"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Howard Hughes Medical Institute, Center for Neural Science, New York University, New York, NY 10003, USA. liam@cns.nyu.edu"
            },
            {
                "First Name": "Jonathan W",
                "Last Name": "Pillow",
                "Affiliation": ""
            },
            {
                "First Name": "Eero P",
                "Last Name": "Simoncelli",
                "Affiliation": ""
            }
        ],
        "Journal": "Neural computation",
        "PubDate": "2004"
    },
    {
        "PMID": "15456829",
        "Title": "Superlinear population encoding of dynamic hand trajectory in primary motor cortex.",
        "Abstract": "Neural activity in primary motor cortex (MI) is known to correlate with hand position and velocity. Previous descriptions of this tuning have (1) been linear in position or velocity, (2) depended only instantaneously on these signals, and/or (3) not incorporated the effects of interneuronal dependencies on firing rate. We show here that many MI cells encode a superlinear function of the full time-varying hand trajectory. Approximately 20% of MI cells carry information in the hand trajectory beyond just the position, velocity, and acceleration at a single time lag. Moreover, approximately one-third of MI cells encode the trajectory in a significantly superlinear manner; as one consequence, even small position changes can dramatically modulate the gain of the velocity tuning of MI cells, in agreement with recent psychophysical evidence. We introduce a compact nonlinear \"preferred trajectory\" model that predicts the complex structure of the spatiotemporal tuning functions described in previous work. Finally, observing the activity of neighboring cells in the MI network significantly increases the predictability of the firing rate of a single MI cell; however, we find interneuronal dependencies in MI to be much more locked to external kinematic parameters than those described recently in the hippocampus. Nevertheless, this neighbor activity is approximately as informative as the hand velocity, supporting the view that neural encoding in MI is best understood at a population level.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Biomechanical Phenomena",
            "Cell Communication",
            "Conditioning, Operant",
            "Hand",
            "Macaca fascicularis",
            "Macaca mulatta",
            "Models, Neurological",
            "Models, Statistical",
            "Motor Cortex",
            "Movement",
            "Neurons",
            "Probability"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom WC1N 3AR. liam@gatsby.ucl.ac.uk"
            },
            {
                "First Name": "Shy",
                "Last Name": "Shoham",
                "Affiliation": ""
            },
            {
                "First Name": "Matthew R",
                "Last Name": "Fellows",
                "Affiliation": ""
            },
            {
                "First Name": "Nicholas G",
                "Last Name": "Hatsopoulos",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Donoghue",
                "Affiliation": ""
            }
        ],
        "Journal": "The Journal of neuroscience : the official journal of the Society for Neuroscience",
        "PubDate": "2004"
    },
    {
        "PMID": "13679402",
        "Title": "Spatiotemporal tuning of motor cortical neurons for hand position and velocity.",
        "Abstract": "A pursuit-tracking task (PTT) and multielectrode recordings were used to investigate the spatiotemporal encoding of hand position and velocity in primate primary motor cortex (MI). Continuous tracking of a randomly moving visual stimulus provided a broad sample of velocity and position space, reduced statistical dependencies between kinematic variables, and minimized the nonstationarities that are found in typical \"step-tracking\" tasks. These statistical features permitted the application of signal-processing and information-theoretic tools for the analysis of neural encoding. The multielectrode method allowed for the comparison of tuning functions among simultaneously recorded cells. During tracking, MI neurons showed heterogeneity of position and velocity coding, with markedly different temporal dynamics for each. Velocity-tuned neurons were approximately sinusoidally tuned for direction, with linear speed scaling; other cells showed sinusoidal tuning for position, with linear scaling by distance. Velocity encoding led behavior by about 100 ms for most cells, whereas position tuning was more broadly distributed, with leads and lags suggestive of both feedforward and feedback coding. Individual cells encoded velocity and position weakly, with comparable amounts of information about each. Linear regression methods confirmed that random, 2-D hand trajectories can be reconstructed from the firing of small ensembles of randomly selected neurons (3-19 cells) within the MI arm area. These findings demonstrate that MI carries information about evolving hand trajectory during visually guided pursuit tracking, including information about arm position both during and after its specification. However, the reconstruction methods used here capture only the low-frequency components of movement during the PTT. Hand motion signals appear to be represented as a distributed code in which diverse information about position and velocity is available within small regions of MI.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Behavior, Animal",
            "Electrophysiology",
            "Hand",
            "Kinesthesis",
            "Macaca",
            "Models, Neurological",
            "Motion Perception",
            "Motor Cortex",
            "Motor Neurons",
            "Motor Skills",
            "Movement",
            "Psychomotor Performance",
            "Pursuit, Smooth",
            "Reaction Time",
            "Space Perception",
            "Time Factors"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Center for Neural Science, New York University, New York, New York 10003, USA."
            },
            {
                "First Name": "Matthew R",
                "Last Name": "Fellows",
                "Affiliation": ""
            },
            {
                "First Name": "Nicholas G",
                "Last Name": "Hatsopoulos",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Donoghue",
                "Affiliation": ""
            }
        ],
        "Journal": "Journal of neurophysiology",
        "PubDate": "2004"
    },
    {
        "PMID": "12938766",
        "Title": "Convergence properties of three spike-triggered analysis techniques.",
        "Abstract": "We analyse the convergence properties of three spike-triggered data analysis techniques. Our results are obtained in the setting of a probabilistic linear-nonlinear (LN) cascade neural encoding model; this model has recently become popular in the study of the neural coding of natural signals. We start by giving exact rate-of-convergence results for the common spike-triggered average technique. Next, we analyse a spike-triggered covariance method, variants of which have been recently exploited successfully by Bialek, Simoncelli and colleagues. Unfortunately, the conditions that guarantee that these two estimators will converge to the correct parameters are typically not satisfied by natural signal data. Therefore, we introduce an estimator for the LN model parameters which is designed to converge under general conditions to the correct model. We derive the rate of convergence of this estimator, provide an algorithm for its computation and demonstrate its application to simulated data as well as physiological data from the primary motor cortex of awake behaving monkeys. We also give lower bounds on the convergence rate of any possible LN estimator. Our results should prove useful in the study of the neural coding of high-dimensional natural signals.",
        "Keywords": [],
        "MeSH terms": [
            "Action Potentials",
            "Animals",
            "Haplorhini",
            "Linear Models",
            "Motor Cortex",
            "Neural Networks, Computer"
        ],
        "Authors": [
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": "Center for Neural Science, New York University, 4 Washington Place, New York, NY 10003, USA. liam@cns.nyu.edu"
            }
        ],
        "Journal": "Network (Bristol, England)",
        "PubDate": "2003"
    },
    {
        "PMID": "12677328",
        "Title": "Sequential movement representations based on correlated neuronal activity.",
        "Abstract": "We tested the hypothesis that sequential movements are represented in the correlated activity of motor cortical neurons. We simultaneously recorded multiple single neurons in the motor cortex while monkeys performed a two-segment movement sequence. Before any movement began the correlated spike firing between pairs of neurons differed when these sequences were planned as whole (planned) as compared to when they were planned one segment at a time (unplanned) even when the firing rates of these neurons did not distinguish between the two conditions. Moreover, the correlation strength was significantly larger when the directional preferences of the neurons matched the direction of the final segment of the sequence. Our results suggest that spatially distributed groups of MI neurons form dynamic correlation structures that distinguish different forms of sequential action.",
        "Keywords": [],
        "MeSH terms": [
            "Animals",
            "Electrophysiology",
            "Macaca fascicularis",
            "Macaca mulatta",
            "Motor Cortex",
            "Movement",
            "Neurons",
            "Orientation"
        ],
        "Authors": [
            {
                "First Name": "Nicholas G",
                "Last Name": "Hatsopoulos",
                "Affiliation": "Department of Organismal Biology and Anatomy, University of Chicago, 1027 East 57th Street, Chicago, IL 60637, USA. nicho@uchicago.edu"
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Donoghue",
                "Affiliation": ""
            }
        ],
        "Journal": "Experimental brain research",
        "PubDate": "2003"
    },
    {
        "PMID": "11894084",
        "Title": "Instant neural control of a movement signal.",
        "Abstract": "The activity of motor cortex (MI) neurons conveys movement intent sufficiently well to be used as a control signal to operate artificial devices, but until now this has called for extensive training or has been confined to a limited movement repertoire. Here we show how activity from a few (7-30) MI neurons can be decoded into a signal that a monkey is able to use immediately to move a computer cursor to any new position in its workspace (14 degrees x 14 degrees visual angle). Our results, which are based on recordings made by an electrode array that is suitable for human use, indicate that neurally based control of movement may eventually be feasible in paralysed humans.",
        "Keywords": [],
        "MeSH terms": [
            "Algorithms",
            "Animals",
            "Computers",
            "Electrodes",
            "Feedback",
            "Hand",
            "Humans",
            "Macaca mulatta",
            "Motor Cortex",
            "Movement",
            "Neurons",
            "Paralysis"
        ],
        "Authors": [
            {
                "First Name": "Mijail D",
                "Last Name": "Serruya",
                "Affiliation": "Department of Neuroscience, Box 1953, Brown University, Providence, Rhode Island 02912, USA. mijail_serruya@brown.edu"
            },
            {
                "First Name": "Nicholas G",
                "Last Name": "Hatsopoulos",
                "Affiliation": ""
            },
            {
                "First Name": "Liam",
                "Last Name": "Paninski",
                "Affiliation": ""
            },
            {
                "First Name": "Matthew R",
                "Last Name": "Fellows",
                "Affiliation": ""
            },
            {
                "First Name": "John P",
                "Last Name": "Donoghue",
                "Affiliation": ""
            }
        ],
        "Journal": "Nature",
        "PubDate": "2002"
    }
]